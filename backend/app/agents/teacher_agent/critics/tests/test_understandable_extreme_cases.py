import asyncio
import json
from dotenv import load_dotenv
from backend.app.agents.teacher_agent.critics.quality_critic import QualityCritic
from backend.app.agents.teacher_agent.skills.exam_generator.exam_nodes import get_llm

load_dotenv()

async def test_extreme_cases():
    """
    Test with truly terrible (1 point) and excellent (4-5 point) cases.
    """
    print("=== Testing Extreme Cases for Understandable Rubric ===\n")
    
    llm = get_llm()
    critic = QualityCritic(llm, threshold=4.0)
    
    # Test Case 1: TRULY TERRIBLE - Should definitely get 1 point
    print("Test Case 1: çœŸæ­£ç³Ÿç³•çš„æ¡ˆä¾‹ï¼ˆæ‡‰å¾— 1 åˆ†ï¼‰")
    print("=" * 70)
    
    terrible_case = {
        "type": "multiple_choice",
        "questions": [{
            "question_number": 1,
            "question_text": "æ¢¯åº¦çˆ†ç‚¸èˆ‡ vanishing gradient åœ¨ LSTM çš„ cell state æ›´æ–°ä¸­ï¼Œé€é forget gate èˆ‡ input gate çš„èª¿ç¯€æ©Ÿåˆ¶ï¼Œå¦‚ä½•å½±éŸ¿ backpropagation through time çš„ç©©å®šæ€§ï¼Ÿ",
            "options": {
                "A": "é€é gating mechanism å¯¦ç¾ gradient clipping",
                "B": "ä½¿ç”¨ orthogonal initialization é¿å… exploding gradients",
                "C": "Cell state çš„ additive update ç·©è§£ vanishing gradients", 
                "D": "Bidirectional RNN å¯å®Œå…¨è§£æ±ºæ­¤å•é¡Œ"
            },
            "correct_answer": "C",
            "source": {
                "page_number": "1",
                "evidence": "LSTM æ¶æ§‹è¨­è¨ˆç”¨æ–¼è™•ç†é•·åºåˆ—ã€‚"
            }
        }]
    }
    
    result1 = await critic.evaluate(terrible_case, criteria=["Understandable"])
    
    print(f"\nè©•ä¼°çµæœ:")
    if "evaluations" in result1:
        for eval_item in result1["evaluations"]:
            print(f"\nğŸ“Š {eval_item['criteria']}")
            print(f"   è©•åˆ†: {eval_item['rating']}/5")
            print(f"   åˆ†æ: {eval_item['analysis'][:200]}...")
            if eval_item.get('suggestions'):
                print(f"   å»ºè­°æ•¸é‡: {len(eval_item['suggestions'])}")
                for i, sug in enumerate(eval_item['suggestions'][:2], 1):
                    print(f"      {i}. {sug}")
    
    # Test Case 2: Moderately bad - Should get 2 points
    print("\n\n" + "=" * 70)
    print("Test Case 2: ä¸­ç­‰ç³Ÿç³•ï¼ˆæ‡‰å¾— 2 åˆ†ï¼‰")
    print("=" * 70)
    
    moderate_case = {
        "type": "multiple_choice",
        "questions": [{
            "question_number": 1,
            "question_text": "åœ¨ Transformer æ¶æ§‹ä¸­ï¼ŒMulti-Head Attention çš„ä¸»è¦å„ªå‹¢ç‚ºä½•ï¼Ÿ",
            "options": {
                "A": "å¢åŠ æ¨¡å‹åƒæ•¸é‡",
                "B": "å…è¨±æ¨¡å‹é—œæ³¨ä¸åŒä½ç½®çš„è³‡è¨Š",
                "C": "æ¸›å°‘è¨“ç·´æ™‚é–“",
                "D": "è‡ªå‹•é€²è¡Œç‰¹å¾µå·¥ç¨‹"
            },
            "correct_answer": "B",
            "source": {
                "page_number": "1",
                "evidence": "Multi-Head Attention æ˜¯ Transformer çš„æ ¸å¿ƒæ©Ÿåˆ¶ã€‚"
            }
        }]
    }
    
    result2 = await critic.evaluate(moderate_case, criteria=["Understandable"])
    
    print(f"\nè©•ä¼°çµæœ:")
    if "evaluations" in result2:
        for eval_item in result2["evaluations"]:
            print(f"\nğŸ“Š {eval_item['criteria']}")
            print(f"   è©•åˆ†: {eval_item['rating']}/5")
            print(f"   åˆ†æ: {eval_item['analysis'][:200]}...")
    
    # Test Case 3: EXCELLENT (4 points standard) - Clear context + proper terminology
    print("\n\n" + "=" * 70)
    print("Test Case 3: å„ªç§€æ¨™æº–ï¼ˆæ‡‰å¾— 4 åˆ†ï¼‰")
    print("=" * 70)
    
    excellent_case = {
        "type": "multiple_choice",
        "questions": [{
            "question_number": 1,
            "question_text": """åœ¨æ©Ÿå™¨å­¸ç¿’ä¸­ï¼Œæˆ‘å€‘å¸¸éœ€è¦å°‡è³‡æ–™åˆ†æˆã€Œè¨“ç·´é›†ã€å’Œã€Œæ¸¬è©¦é›†ã€ã€‚
è¨“ç·´é›†ç”¨ä¾†è®“æ¨¡å‹å­¸ç¿’è¦å¾‹ï¼Œæ¸¬è©¦é›†å‰‡ç”¨ä¾†æª¢é©—æ¨¡å‹å°ã€Œå¾æœªè¦‹éçš„è³‡æ–™ã€çš„é æ¸¬èƒ½åŠ›ã€‚

ä¾‹å¦‚ï¼šè‹¥è¦å»ºç«‹æˆ¿åƒ¹é æ¸¬æ¨¡å‹ï¼Œæˆ‘å€‘å¯èƒ½ç”¨ 80% çš„æˆ¿å±‹è³‡æ–™ä¾†è¨“ç·´ï¼Œå‰©ä¸‹ 20% ç”¨ä¾†æ¸¬è©¦æ¨¡å‹æ˜¯å¦èƒ½æº–ç¢ºé æ¸¬æ–°æˆ¿å±‹çš„åƒ¹æ ¼ã€‚

é—œæ–¼æ¸¬è©¦é›†çš„ä½¿ç”¨ï¼Œä¸‹åˆ—ä½•è€…æ­£ç¢ºï¼Ÿ""",
            "options": {
                "A": "æ¸¬è©¦é›†çš„è³‡æ–™å¯ä»¥åŒæ™‚ç”¨æ–¼è¨“ç·´ï¼Œä»¥æå‡æº–ç¢ºåº¦",
                "B": "æ¸¬è©¦é›†ç”¨æ–¼è©•ä¼°æ¨¡å‹å°æœªè¦‹éè³‡æ–™çš„é æ¸¬èƒ½åŠ›",
                "C": "æ¸¬è©¦é›†å¿…é ˆèˆ‡è¨“ç·´é›†å®Œå…¨ç›¸åŒ",
                "D": "æ¸¬è©¦é›†åªåœ¨æ¨¡å‹è¨“ç·´éç¨‹ä¸­ä½¿ç”¨ä¸€æ¬¡"
            },
            "correct_answer": "B",
            "source": {
                "page_number": "1",
                "evidence": "è³‡æ–™åˆ†å‰²æ˜¯ç›£ç£å¼å­¸ç¿’çš„åŸºæœ¬æ­¥é©Ÿã€‚"
            }
        }]
    }
    
    result3 = await critic.evaluate(excellent_case, criteria=["Understandable"])
    
    print(f"\nè©•ä¼°çµæœ:")
    if "evaluations" in result3:
        for eval_item in result3["evaluations"]:
            print(f"\nğŸ“Š {eval_item['criteria']}")
            print(f"   è©•åˆ†: {eval_item['rating']}/5")
            print(f"   åˆ†æ: {eval_item['analysis'][:200]}...")
    
    print("\n" + "=" * 70)
    print("ğŸ“Š è©•åˆ†ç¸½çµ")
    print("=" * 70)
    if "evaluations" in result1:
        print(f"  Case 1 (çœŸæ­£ç³Ÿç³•): {result1['evaluations'][0]['rating']}/5 - é æœŸ 1 åˆ†")
    if "evaluations" in result2:
        print(f"  Case 2 (ä¸­ç­‰ç³Ÿç³•): {result2['evaluations'][0]['rating']}/5 - é æœŸ 2 åˆ†")
    if "evaluations" in result3:
        print(f"  Case 3 (å„ªç§€æ¨™æº–): {result3['evaluations'][0]['rating']}/5 - é æœŸ 4 åˆ†")

if __name__ == "__main__":
    asyncio.run(test_extreme_cases())
