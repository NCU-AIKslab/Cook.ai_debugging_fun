import asyncio
import json
from dotenv import load_dotenv
from backend.app.agents.teacher_agent.critics.quality_critic import QualityCritic
from backend.app.agents.teacher_agent.skills.exam_generator.exam_nodes import get_llm

load_dotenv()

async def test_evidence_based_evaluation():
    """
    Test Quality Critic with real generated examples focusing on evidence-based terminology check.
    """
    print("=== Testing Evidence-Based Terminology Evaluation ===\n")
    
    llm = get_llm()
    critic = QualityCritic(llm, threshold=4.0)
    
    # Test Case 1: Good - All terms in evidence
    print("Test Case 1: å„ªç§€ - æ‰€æœ‰è¡“èªéƒ½åœ¨ evidence ä¸­")
    print("=" * 70)
    
    good_case = {
        "type": "multiple_choice",
        "questions": [{
            "question_number": 3,
            "question_text": "ä½¿ç”¨ KNN ç®—æ³•çš„ç›®çš„ç‚ºä½•ï¼Ÿ",
            "options": {
                "A": "åˆªé™¤é‡è¤‡è³‡æ–™",
                "B": "é æ¸¬ç¼ºå¤±å€¼",
                "C": "é™ç¶­",
                "D": "æ•¸æ“šé›†æˆ"
            },
            "correct_answer": "B",
            "source": {
                "page_number": "10",
                "evidence": "ä½¿ç”¨å¦‚ KNNï¼ˆK-Nearest Neighborsï¼‰ç­‰ç®—æ³•,æ ¹æ“šç›¸ä¼¼è¨˜éŒ„ä¾†é æ¸¬ç¼ºå¤±å€¼ã€‚"
            }
        }]
    }
    
    result1 = await critic.evaluate(good_case, criteria=["Understandable"])
    
    print(f"é¡Œç›®: {good_case['questions'][0]['question_text']}")
    print(f"Evidence: {good_case['questions'][0]['source']['evidence']}\n")
    
    if "evaluations" in result1:
        for eval_item in result1["evaluations"]:
            print(f"ğŸ“Š è©•åˆ†: {eval_item['rating']}/5 (é æœŸ 4-5 åˆ†)")
            print(f"åˆ†æ: {eval_item['analysis'][:150]}...")
            if eval_item.get('suggestions'):
                print(f"å»ºè­°: {len(eval_item['suggestions'])} é …")
    
    # Test Case 2: Bad -Answer contradicts evidence
    print("\n\n" + "=" * 70)
    print("Test Case 2: åš´é‡å•é¡Œ - ç­”æ¡ˆèˆ‡ evidence çŸ›ç›¾")
    print("=" * 70)
    
    bad_case = {
        "type": "multiple_choice",
        "questions": [{
            "question_number": 5,
            "question_text": "å¡«è£œç¼ºå¤±å€¼çš„æ–¹å¼ä¹‹ä¸€æ˜¯ä½¿ç”¨ä»€éº¼ä¾†å¡«è£œå¹´é½¡ï¼Ÿ",
            "options": {
                "A": "ä¸­ä½æ•¸",
                "B": "å¹³å‡æ•¸",
                "C": "çœ¾æ•¸",
                "D": "å›ºå®šå€¼"
            },
            "correct_answer": "A",
            "source": {
                "page_number": "13",
                "evidence": "å¹´é½¡ (Age) åˆ—å¡«è£œç‚ºå¹³å‡å€¼ã€‚"
            }
        }]
    }
    
    result2 = await critic.evaluate(bad_case, criteria=["Understandable"])
    
    print(f"é¡Œç›®: {bad_case['questions'][0]['question_text']}")
    print(f"æ­£ç¢ºç­”æ¡ˆ: A (ä¸­ä½æ•¸)")
    print(f"Evidence: {bad_case['questions'][0]['source']['evidence']}")
    print(f"â†’ çŸ›ç›¾ï¼Evidence èªªçš„æ˜¯ã€Œå¹³å‡å€¼ã€\n")
    
    if "evaluations" in result2:
        for eval_item in result2["evaluations"]:
            print(f"ğŸ“Š è©•åˆ†: {eval_item['rating']}/5 (é æœŸ 1-2 åˆ†)")
            print(f"åˆ†æ: {eval_item['analysis'][:200]}...")
            if eval_item.get('suggestions'):
                print(f"\nå»ºè­°:")
                for sug in eval_item['suggestions'][:3]:
                    print(f"  - {sug}")
    
    # Test Case 3: Medium - Some terms not in evidence
    print("\n\n" + "=" * 70)
    print("Test Case 3: ä¸­ç­‰ - éƒ¨åˆ†è¡“èªæœªåœ¨ evidence ä¸­")
    print("=" * 70)
    
    medium_case = {
        "type": "multiple_choice",
        "questions": [{
            "question_number": 1,
            "question_text": "åœ¨è³‡æ–™æ¸…æ´—ä¸­ï¼Œå¦‚ä½•è™•ç†ç¼ºå¤±å€¼ï¼Ÿ",
            "options": {
                "A": "ä½¿ç”¨ä¸­ä½æ•¸å¡«è£œæ‰€æœ‰ç¼ºå¤±å€¼",
                "B": "åˆªé™¤æ‰€æœ‰åŒ…å«ç¼ºå¤±å€¼çš„è¡Œ",
                "C": "ä½¿ç”¨å›æ­¸æ¨¡å‹é æ¸¬å¡«è£œç¼ºå¤±å€¼",
                "D": "å¿½è¦–ç¼ºå¤±å€¼ä¸åšè™•ç†"
            },
            "correct_answer": "C",
            "source": {
                "page_number": "10",
                "evidence": "åˆ©ç”¨å…¶ä»–ç‰¹å¾µå»ºç«‹å›æ­¸æ¨¡å‹ä¾†é æ¸¬ç¼ºå¤±å€¼ã€‚"
            }
        }]
    }
    
    result3 = await critic.evaluate(medium_case, criteria=["Understandable"])
    
    print(f"é¡Œç›®: {medium_case['questions'][0]['question_text']}")
    print(f"Evidence: {medium_case['questions'][0]['source']['evidence']}")
    print(f"â†’ é¸é … A æåˆ°ã€Œä¸­ä½æ•¸ã€ä½† evidence æœªæåŠ\n")
    
    if "evaluations" in result3:
        for eval_item in result3["evaluations"]:
            print(f"ğŸ“Š è©•åˆ†: {eval_item['rating']}/5 (é æœŸ 3-4 åˆ†)")
            print(f"åˆ†æ: {eval_item['analysis'][:200]}...")
    
    print("\n" + "=" * 70)
    print("âœ… æ¸¬è©¦å®Œæˆ")
    print("=" * 70)

if __name__ == "__main__":
    asyncio.run(test_evidence_based_evaluation())
