import asyncio
import json
from dotenv import load_dotenv
from backend.app.agents.teacher_agent.critics.quality_critic import QualityCritic
from backend.app.agents.teacher_agent.skills.exam_generator.exam_nodes import get_llm

load_dotenv()

async def test_understandable_rubric():
    """
    Test the enhanced Understandable rubric with edge cases:
    1. Lack of context
    2. Inappropriate use of advanced terminology
    """
    print("=== Testing Enhanced 'Understandable' Rubric ===\n")
    
    llm = get_llm()
    critic = QualityCritic(llm, threshold=4.0)
    
    # Test Case 1: No context + Many undefined terms (æ‡‰è©²å¾— 1-2 åˆ†)
    print("Test Case 1: ç¼ºä¹æƒ…å¢ƒ + å¤§é‡æœªå®šç¾©è¡“èª")
    print("=" * 70)
    
    case1 = {
        "type": "multiple_choice",
        "questions": [{
            "question_number": 1,
            "question_text": "ä¸‹åˆ—é—œæ–¼ Backpropagation ä¸­çš„æ¢¯åº¦æ¶ˆå¤±å•é¡Œï¼Œä½•è€…æ­£ç¢ºï¼Ÿ",
            "options": {
                "A": "ä½¿ç”¨ ReLU å¯å®Œå…¨é¿å…",
                "B": "èˆ‡ Sigmoid çš„å°æ•¸ç¯„åœæœ‰é—œ",
                "C": "åªå‡ºç¾åœ¨ RNN ä¸­",
                "D": "å¯é€é Batch Normalization è§£æ±º"
            },
            "correct_answer": "B",
            "source": {
                "page_number": "1",
                "evidence": "æ¢¯åº¦æ¶ˆå¤±æ˜¯æ·±åº¦å­¸ç¿’è¨“ç·´ä¸­çš„å¸¸è¦‹å•é¡Œã€‚"
            }
        }]
    }
    
    result1 = await critic.evaluate(case1, criteria=["Understandable"])
   
    print(f"\nè©•ä¼°çµæœ:")
    print(json.dumps(result1, ensure_ascii=False, indent=2))
    
    if "evaluations" in result1:
        for eval_item in result1["evaluations"]:
            print(f"\nğŸ“Š {eval_item['criteria']}")
            print(f"   è©•åˆ†: {eval_item['rating']}/5")
            print(f"   åˆ†æ: {eval_item['analysis']}")
            if eval_item.get('suggestions'):
                print(f"   å»ºè­°:")
                for sug in eval_item['suggestions']:
                    print(f"      - {sug}")
    
    # Test Case 2: Minimal context + Some undefined terms (æ‡‰è©²å¾— 2-3 åˆ†)
    print("\n\n" + "=" * 70)
    print("Test Case 2: æƒ…å¢ƒä¸è¶³ + éƒ¨åˆ†æœªå®šç¾©è¡“èª")
    print("=" * 70)
    
    case2 = {
        "type": "multiple_choice",
        "questions": [{
            "question_number": 1,
            "question_text": "åœ¨æ•¸æ“šé è™•ç†éšæ®µï¼Œæ¨™æº–åŒ–ï¼ˆStandardizationï¼‰çš„ç›®çš„ç‚ºä½•ï¼Ÿ",
            "options": {
                "A": "å°‡æ•¸æ“šè½‰æ›ç‚º 0-1 ç¯„åœ",
                "B": "ç§»é™¤ç•°å¸¸å€¼",
                "C": "ä½¿æ•¸æ“šå‡å€¼ç‚º 0ï¼Œæ¨™æº–å·®ç‚º 1",
                "D": "å¢åŠ æ•¸æ“šç¶­åº¦"
            },
            "correct_answer": "C",
            "source": {
                "page_number": "1",
                "evidence": "æ¨™æº–åŒ–æ˜¯å¸¸ç”¨çš„æ•¸æ“šé è™•ç†æŠ€è¡“ã€‚"
            }
        }]
    }
    
    result2 = await critic.evaluate(case2, criteria=["Understandable"])
    
    print(f"\nè©•ä¼°çµæœ:")
    print(json.dumps(result2, ensure_ascii=False, indent=2))
    
    if "evaluations" in result2:
        for eval_item in result2["evaluations"]:
            print(f"\nğŸ“Š {eval_item['criteria']}")
            print(f"   è©•åˆ†: {eval_item['rating']}/5")
            print(f"   åˆ†æ: {eval_item['analysis']}")
            if eval_item.get('suggestions'):
                print(f"   å»ºè­°:")
                for sug in eval_item['suggestions']:
                    print(f"      - {sug}")
    
    # Test Case 3: Good context + Clear definitions (æ‡‰è©²å¾— 4-5 åˆ†)
    print("\n\n" + "=" * 70)
    print("Test Case 3: å……è¶³æƒ…å¢ƒ + æ¸…æ¥šå®šç¾©")
    print("=" * 70)
    
    case3 = {
        "type": "multiple_choice",
        "questions": [{
            "question_number": 1,
            "question_text": """å°æ˜æƒ³è¦å»ºç«‹ä¸€å€‹é æ¸¬æˆ¿åƒ¹çš„æ¨¡å‹ã€‚ä»–æ”¶é›†äº† 500 ç­†æˆ¿å±‹è³‡æ–™ï¼Œæ¯ç­†è³‡æ–™åŒ…å« 20 å€‹ç‰¹å¾µï¼ˆå¦‚ï¼šåªæ•¸ã€æˆ¿é–“æ•¸ã€å±‹é½¡ç­‰ï¼‰ã€‚
ä½†ä»–ç™¼ç¾ï¼Œè¨±å¤šç‰¹å¾µä¹‹é–“é«˜åº¦ç›¸é—œï¼ˆä¾‹å¦‚ï¼šåªæ•¸èˆ‡æˆ¿é–“æ•¸ï¼‰ï¼Œé€™æœƒè®“æ¨¡å‹è®Šå¾—è¤‡é›œä¸”é›£ä»¥è§£é‡‹ã€‚

ç‚ºäº†ç°¡åŒ–æ¨¡å‹ï¼Œå°æ˜æ±ºå®šä½¿ç”¨ã€Œä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ã€ã€‚PCA çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ï¼š
æ‰¾å‡ºè³‡æ–™ä¸­æœ€é‡è¦çš„ã€Œæ–¹å‘ã€ï¼ˆç¨±ç‚ºä¸»æˆåˆ†ï¼‰ï¼Œå°‡åŸæœ¬ 20 å€‹ç‰¹å¾µå£“ç¸®æˆ 3-5 å€‹ä¸»æˆåˆ†ï¼Œ
åŒæ™‚ä¿ç•™ 95% ä»¥ä¸Šçš„è³‡è¨Šé‡ã€‚

æ ¹æ“šä»¥ä¸Šæƒ…å¢ƒï¼ŒPCA åœ¨å°æ˜çš„æ¡ˆä¾‹ä¸­ä¸»è¦è§£æ±ºäº†ä»€éº¼å•é¡Œï¼Ÿ""",
            "options": {
                "A": "å¢åŠ æˆ¿å±‹ç‰¹å¾µçš„æ•¸é‡ï¼Œè®“æ¨¡å‹æ›´æº–ç¢º",
                "B": "æ¸›å°‘ç‰¹å¾µç¶­åº¦ï¼Œé™ä½æ¨¡å‹è¤‡é›œåº¦",
                "C": "è‡ªå‹•æ‰¾å‡ºæˆ¿åƒ¹æœ€é«˜çš„æˆ¿å±‹",
                "D": "å°‡æˆ¿é–“æ•¸è½‰æ›ç‚ºåªæ•¸"
            },
            "correct_answer": "B",
            "source": {
                "page_number": "1",
                "evidence": "PCA æ˜¯é™ç¶­æŠ€è¡“ï¼Œç”¨æ–¼ç°¡åŒ–é«˜ç¶­æ•¸æ“šã€‚"
            }
        }]
    }
    
    result3 = await critic.evaluate(case3, criteria=["Understandable"])
    
    print(f"\nè©•ä¼°çµæœ:")
    print(json.dumps(result3, ensure_ascii=False, indent=2))
    
    if "evaluations" in result3:
        for eval_item in result3["evaluations"]:
            print(f"\nğŸ“Š {eval_item['criteria']}")
            print(f"   è©•åˆ†: {eval_item['rating']}/5")
            print(f"   åˆ†æ: {eval_item['analysis']}")
            if eval_item.get('suggestions'):
                print(f"   å»ºè­°:")
                for sug in eval_item['suggestions']:
                    print(f"      - {sug}")
    
    print("\n" + "=" * 70)
    print("âœ… Rubric æ¸¬è©¦å®Œæˆ!")
    print("=" * 70)
    
    # Summary
    print("\nğŸ“Š è©•åˆ†ç¸½çµ:")
    if "evaluations" in result1:
        print(f"  Case 1 (ç„¡æƒ…å¢ƒ+å¤šè¡“èª): {result1['evaluations'][0]['rating']}/5 - é æœŸ 1-2 åˆ†")
    if "evaluations" in result2:
        print(f"  Case 2 (æƒ…å¢ƒä¸è¶³+éƒ¨åˆ†è¡“èª): {result2['evaluations'][0]['rating']}/5 - é æœŸ 2-3 åˆ†")
    if "evaluations" in result3:
        print(f"  Case 3 (å……è¶³æƒ…å¢ƒ+æ¸…æ¥šå®šç¾©): {result3['evaluations'][0]['rating']}/5 - é æœŸ 4-5 åˆ†")

if __name__ == "__main__":
    asyncio.run(test_understandable_rubric())
