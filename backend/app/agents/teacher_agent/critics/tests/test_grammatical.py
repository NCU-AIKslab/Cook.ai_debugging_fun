import asyncio
import json
from dotenv import load_dotenv
from backend.app.agents.teacher_agent.critics.quality_critic import QualityCritic
from backend.app.agents.teacher_agent.skills.exam_generator.exam_nodes import get_llm

load_dotenv()

async def test_grammatical_rubric():
    """
    Test Grammatical rubric with spelling errors and punctuation issues.
    """
    print("=== Testing Grammatical Rubric ===\n")
    
    llm = get_llm()
    critic = QualityCritic(llm, threshold=4.0)
    
    # Test Case 1: Severe spelling error (1 point expected)
    print("Test Case 1: åš´é‡æ‹¼å¯«éŒ¯èª¤ï¼ˆå°ˆæ¥­è¡“èªæ‹¼éŒ¯ï¼‰")
    print("=" * 70)
    
    spelling_error_case = {
        "type": "multiple_choice",
        "questions": [{
            "question_number": 8,
            "question_text": "ä½¿ç”¨ Pæ–½ä¾†æ›¿ä»£ç¶­åº¦ï¼Œé€™å«ä»€éº¼ï¼Ÿ",
            "options": {
                "A": "æ•¸æ“šæ¸…æ´—",
                "B": "æ•¸æ“šé›†æˆ",
                "C": "é™ç¶­",
                "D": "æ•¸æ“šè½‰æ›"
            },
            "correct_answer": "C",
            "source": {
                "page_number": "25",
                "evidence": "PCA é™ç¶­åˆ° 2 å€‹ä¸»è¦æˆåˆ†ã€‚"
            }
        }]
    }
    
    result1 = await critic.evaluate(spelling_error_case, criteria=["Grammatical"])
    
    print(f"é¡Œç›®: {spelling_error_case['questions'][0]['question_text']}")
    print(f"å•é¡Œ: ã€ŒPæ–½ã€æ‡‰ç‚ºã€ŒPCAã€")
    print(f"Evidence: {spelling_error_case['questions'][0]['source']['evidence']}\n")
    
    if "evaluations" in result1:
        for eval_item in result1["evaluations"]:
            print(f"ğŸ“Š è©•åˆ†: {eval_item['rating']}/5 (é æœŸ 1-2 åˆ†)")
            print(f"åˆ†æ: {eval_item['analysis'][:200]}...")
            if eval_item.get('suggestions'):
                print(f"\nå»ºè­°:")
                for sug in eval_item['suggestions'][:3]:
                    print(f"  - {sug}")
    
    # Test Case 2: Multiple spelling errors + missing punctuation (2 points expected)
    print("\n\n" + "=" * 70)
    print("Test Case 2: å¤šè™•éŒ¯åˆ¥å­— + æ¨™é»ç¼ºå¤±")
    print("=" * 70)
    
    multiple_errors_case = {
        "type": "multiple_choice",
        "questions": [{
            "question_number": 1,
            "question_text": "æ©Ÿå™¨å­¸ç¿’ä¸­ç‰¹å¾µå·¥ç¨‹çš„ç›®åœ°æ˜¯ä»€éº¼å®ƒå¯ä»¥å¹«åŠ©æ¨¡å½¢æé«˜æº–ç¢ºç‡",
            "options": {
                "A": "å¢åŠ æ•¸æ“šé‡",
                "B": "é¸æ“‡å’Œå»ºæ§‹æœ‰æ•ˆç‰¹å¾µ",
                "C": "æ¸›å°‘è¨“ç·´æ™‚é–“",
                "D": "é¿å…éæ“¬åˆ"
            },
            "correct_answer": "B",
            "source": {
                "page_number": "15",
                "evidence": "ç‰¹å¾µå·¥ç¨‹æ˜¯é¸æ“‡å’Œå»ºæ§‹æœ‰æ•ˆç‰¹å¾µçš„éç¨‹ã€‚"
            }
        }]
    }
    
    result2 = await critic.evaluate(multiple_errors_case, criteria=["Grammatical"])
    
    print(f"é¡Œç›®: {multiple_errors_case['questions'][0]['question_text']}")
    print("å•é¡Œ:")
    print("  - ã€Œç›®åœ°ã€æ‡‰ç‚ºã€Œç›®çš„ã€")
    print("  - ã€Œæ¨¡å½¢ã€æ‡‰ç‚ºã€Œæ¨¡å‹ã€")
    print("  - ç¼ºå°‘å•è™Ÿå’Œé€—è™Ÿ\n")
    
    if "evaluations" in result2:
        for eval_item in result2["evaluations"]:
            print(f"ğŸ“Š è©•åˆ†: {eval_item['rating']}/5 (é æœŸ 1-2 åˆ†)")
            print(f"åˆ†æ: {eval_item['analysis'][:200]}...")
            if eval_item.get('suggestions'):
                print(f"\nå»ºè­°æ•¸é‡: {len(eval_item['suggestions'])}")
    
    # Test Case 3: Minor punctuation issue (3 points expected)
    print("\n\n" + "=" * 70)
    print("Test Case 3: è¼•å¾®æ¨™é»å•é¡Œ")
    print("=" * 70)
    
    minor_issue_case = {
        "type": "multiple_choice",
        "questions": [{
            "question_number": 1,
            "question_text": "åœ¨è³‡æ–™æ¸…æ´—ä¸­,è™•ç†ç¼ºå¤±å€¼çš„å¸¸è¦‹æ–¹æ³•åŒ…æ‹¬å“ªäº›ã€‚",
            "options": {
                "A": "åˆªé™¤å«ç¼ºå¤±å€¼çš„è¡Œ",
                "B": "ä½¿ç”¨å¹³å‡å€¼å¡«è£œ",
                "C": "ä½¿ç”¨å›æ­¸æ¨¡å‹é æ¸¬",
                "D": "ä»¥ä¸Šçš†æ˜¯"
            },
            "correct_answer": "D",
            "source": {
                "page_number": "10",
                "evidence": "è™•ç†ç¼ºå¤±å€¼çš„æ–¹æ³•åŒ…æ‹¬åˆªé™¤ã€å¡«è£œå’Œé æ¸¬ã€‚"
            }
        }]
    }
    
    result3 = await critic.evaluate(minor_issue_case, criteria=["Grammatical"])
    
    print(f"é¡Œç›®: {minor_issue_case['questions'][0]['question_text']}")
    print("å•é¡Œ:")
    print("  - é€—è™Ÿæ‡‰ç‚ºå…¨å½¢ã€Œï¼Œã€")
    print("  - å¥å°¾æ‡‰ç‚ºå•è™Ÿã€Œï¼Ÿã€è€Œéå¥è™Ÿ\n")
    
    if "evaluations" in result3:
        for eval_item in result3["evaluations"]:
            print(f"ğŸ“Š è©•åˆ†: {eval_item['rating']}/5 (é æœŸ 3-4 åˆ†)")
            print(f"åˆ†æ: {eval_item['analysis'][:150]}...")
    
    # Test Case 4: Perfect grammar (4-5 points expected)
    print("\n\n" + "=" * 70)
    print("Test Case 4: èªæ³•å®Œç¾")
    print("=" * 70)
    
    perfect_case = {
        "type": "multiple_choice",
        "questions": [{
            "question_number": 1,
            "question_text": "PCAï¼ˆä¸»æˆåˆ†åˆ†æï¼‰çš„ä¸»è¦ç›®çš„ç‚ºä½•ï¼Ÿ",
            "options": {
                "A": "å¢åŠ è³‡æ–™ç¶­åº¦",
                "B": "é™ç¶­ä»¥ç°¡åŒ–åˆ†æ",
                "C": "è™•ç†ç¼ºå¤±å€¼",
                "D": "ç§»é™¤ç•°å¸¸å€¼"
            },
            "correct_answer": "B",
            "source": {
                "page_number": "25",
                "evidence": "PCA ä¸»è¦ç”¨æ–¼é™ç¶­ã€‚"
            }
        }]
    }
    
    result4 = await critic.evaluate(perfect_case, criteria=["Grammatical"])
    
    print(f"é¡Œç›®: {perfect_case['questions'][0]['question_text']}")
    print("ç‹€æ³: ç„¡æ‹¼å¯«éŒ¯èª¤ã€æ¨™é»æ­£ç¢º\n")
    
    if "evaluations" in result4:
        for eval_item in result4["evaluations"]:
            print(f"ğŸ“Š è©•åˆ†: {eval_item['rating']}/5 (é æœŸ 4-5 åˆ†)")
            print(f"åˆ†æ: {eval_item['analysis'][:150]}...")
    
    print("\n" + "=" * 70)
    print("âœ… Grammatical Rubric æ¸¬è©¦å®Œæˆ")
    print("=" * 70)

if __name__ == "__main__":
    asyncio.run(test_grammatical_rubric())
