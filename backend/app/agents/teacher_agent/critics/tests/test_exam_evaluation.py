import asyncio
import json
from dotenv import load_dotenv
from backend.app.agents.teacher_agent.critics.quality_critic import QualityCritic
from backend.app.agents.teacher_agent.skills.exam_generator.exam_nodes import get_llm

load_dotenv()

# Mock exam with 3 questions
MOCK_EXAM = {
    "type": "multiple_choice",
    "title": "æ©Ÿå™¨å­¸ç¿’åŸºç¤è€ƒè©¦",
    "questions": [
        {
            "question_number": 1,
            "question_text": "PCAï¼ˆä¸»æˆåˆ†åˆ†æï¼‰çš„ä¸»è¦ç›®çš„ç‚ºä½•ï¼Ÿ",
            "options": {
                "A": "å¢åŠ è³‡æ–™çš„ç¶­åº¦",
                "B": "é™ç¶­ä»¥ç°¡åŒ–è³‡æ–™åˆ†æ",
                "C": "åŒæ™‚è™•ç†å¤šå€‹è³‡æ–™ä¾†æº",
                "D": "æé«˜è³‡æ–™çš„æº–ç¢ºæ€§"
            },
            "correct_answer": "B",
            "source": {
                "page_number": "25",
                "evidence": "PCAçš„ä¸»è¦ç”¨é€”æ˜¯å°‡è³‡æ–™é™ç¶­åˆ°è¼ƒå°‘çš„ä¸»è¦æˆåˆ†ï¼Œä»¥ç°¡åŒ–è³‡æ–™çš„åˆ†æã€‚"
            }
        },
        {
            "question_number": 2,
            "question_text": "æœºå™¨å­¦ä¹ ä¸­çš„æ•°æ®é¢„å¤„ç†åŒ…æ‹¬å“ªäº›æ­¥é©Ÿï¼Ÿ",  # ç°¡é«”å­—å•é¡Œ
            "options": {
                "A": "æ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ–",
                "B": "æ•°æ®å¯è§†åŒ–",
                "C": "æ¨¡å‹è¨“ç·´",
                "D": "ä»¥ä¸Šçš†æ˜¯"
            },
            "correct_answer": "A",
            "source": {
                "page_number": "15",
                "evidence": "èµ„æ–™é¢„å¤„ç†æ˜¯æœºå™¨å­¦ä¹ æµç¨‹ä¸­çš„é‡è¦æ­¥éª¤ã€‚"
            }
        },
        {
            "question_number": 3,
            "question_text": "å¡«è£œç¼ºå¤±å€¼çš„æ–¹å¼ä¹‹ä¸€æ˜¯ä½¿ç”¨ä»€éº¼ä¾†å¡«è£œå¹´é½¡ï¼Ÿ",
            "options": {
                "A": "ä¸­ä½æ•¸",
                "B": "å¹³å‡æ•¸",
                "C": "çœ¾æ•¸",
                "D": "å›æ­¸æ¨¡å‹"
            },
            "correct_answer": "A",
            "source": {
                "page_number": "10",
                "evidence": "å¹´é½¡ (Age) åˆ—å¡«è£œç‚ºå¹³å‡å€¼ã€‚"  # ç­”æ¡ˆçŸ›ç›¾å•é¡Œ
            }
        }
    ]
}

# Mock RAG content
MOCK_RAG_CONTENT = """
PCAï¼ˆä¸»æˆåˆ†åˆ†æï¼‰æ˜¯ä¸€ç¨®é™ç¶­æŠ€è¡“ï¼Œç”¨æ–¼ç°¡åŒ–é«˜ç¶­åº¦è³‡æ–™ã€‚
åœ¨æ©Ÿå™¨å­¸ç¿’ä¸­ï¼Œè³‡æ–™é è™•ç†åŒ…æ‹¬æ¸…æ´—ã€æ¨™æº–åŒ–ç­‰æ­¥é©Ÿã€‚
è™•ç†ç¼ºå¤±å€¼æ™‚ï¼Œå¯ä»¥ä½¿ç”¨å¹³å‡å€¼ã€ä¸­ä½æ•¸æˆ–çœ¾æ•¸é€²è¡Œå¡«è£œã€‚
"""


async def test_evaluate_exam():
    """
    Test evaluate_exam - æ•´å·è©•ä¼° + é€é¡Œè©•ä¼°
    """
    print("=" * 80)
    print("Test: Evaluate Exam (Overall + Per-Question)")
    print("=" * 80)
    
    llm = get_llm()
    critic = QualityCritic(llm, threshold=4.0)
    
    result = await critic.evaluate_exam(MOCK_EXAM, rag_content=MOCK_RAG_CONTENT)
    
    print(f"\nçµæœçµæ§‹: {list(result.keys())}")
    
    # Overall assessment
    if "overall" in result and "evaluations" in result["overall"]:
        print(f"\nğŸ“„ æ•´å·è©•ä¼°:")
        for eval_item in result["overall"]["evaluations"]:
            rating = eval_item['rating']
            emoji = "âš ï¸" if rating < 4 else "âœ…"
            print(f"  {emoji} {eval_item['criteria']}: {rating}/5")
    
    # Per-question assessment
    if "per_question" in result:
        print(f"\nğŸ“ é€é¡Œè©•ä¼°:")
        for q_result in result["per_question"]:
            q_num = q_result['question_number']
            print(f"\n  ç¬¬ {q_num} é¡Œ:")
            if "evaluations" in q_result:
                for eval_item in q_result["evaluations"]:
                    rating = eval_item['rating']
                    emoji = "âš ï¸" if rating < 4 else "âœ…"
                    print(f"    {emoji} {eval_item['criteria']}: {rating}/5")
    
    # Statistics
    if "statistics" in result:
        stats = result["statistics"]
        print(f"\nğŸ“Š çµ±è¨ˆè³‡è¨Š:")
        print(f"  ç¸½é¡Œæ•¸: {stats.get('total_questions', 0)}")
        if stats.get("questions_below_threshold"):
            print(f"  âš ï¸ éœ€è¦æ”¹é€²çš„é¡Œç›®: {stats['questions_below_threshold']}")
        
        print(f"\n  å¹³å‡åˆ†æ•¸:")
        for criteria, avg in stats.get("avg_scores_by_criteria", {}).items():
            print(f"    â€¢ {criteria}: {avg}/5")
    
    return result


async def test_evaluate_single_question():
    """
    Test evaluate_single_question - å–®é¡Œè©•ä¼°
    """
    print("\n\n" + "=" * 80)
    print("Test: Evaluate Single Question")
    print("=" * 80)
    
    llm = get_llm()
    critic = QualityCritic(llm, threshold=4.0)
    
    # æ¸¬è©¦ç¬¬3é¡Œï¼ˆæœ‰çŸ›ç›¾å•é¡Œï¼‰
    question = MOCK_EXAM["questions"][2]
    
    result = await critic.evaluate_single_question(question, rag_content=MOCK_RAG_CONTENT)
    
    print(f"\nè©•ä¼°ç¬¬ {question['question_number']} é¡Œ:")
    
    if "evaluations" in result:
        for eval_item in result["evaluations"]:
            rating = eval_item['rating']
            emoji = "âš ï¸" if rating < 4 else "âœ…"
            print(f"\n  {emoji} {eval_item['criteria']}: {rating}/5")
            print(f"     åˆ†æ: {eval_item['analysis'][:100]}...")
            
            if eval_item.get('suggestions') and rating < 4:
                print(f"     å»ºè­°: {eval_item['suggestions'][0][:80]}...")
    
    return result


async def main():
    """
    Run all tests
    """
    print("\nğŸ§ª æ¸¬è©¦ QualityCritic - ç°¡åŒ–ç‰ˆ\n")
    
    # Test 1: Evaluate entire exam
    await test_evaluate_exam()
    
    # Test 2: Evaluate single question
    await test_evaluate_single_question()
    
    print("\n\n" + "=" * 80)
    print("âœ… æ¸¬è©¦å®Œæˆ")
    print("=" * 80)
    print("\nğŸ“Œ API ä½¿ç”¨æ–¹å¼:")
    print("  1. evaluate_exam(exam, rag_content) - æ•´å· + é€é¡Œ + çµ±è¨ˆ")
    print("  2. evaluate_single_question(question, rag_content) - å–®é¡Œè©•ä¼°")
    print("\nğŸ“¡ API Server Endpoints:")
    print("  â€¢ POST /api/v1/testing/critic/evaluate_exam")
    print("  â€¢ POST /api/v1/testing/critic/evaluate_single_question")


if __name__ == "__main__":
    asyncio.run(main())
