import logging
from typing import Dict, List, Any, Optional
from ragas.metrics import Faithfulness
from langchain_openai import ChatOpenAI
import os

logger = logging.getLogger(__name__)

class CustomFaithfulness(Faithfulness):
    """
    Custom Faithfulness metric that generates specific Traditional Chinese feedback.
    Checks if the answer is supported by the retrieved contexts.
    """
    
    async def _ascore(self, row: Dict, callbacks: Any = None) -> float:
        """
        Override _ascore to add custom feedback generation.
        
        Args:
            row: Dict with keys 'user_input', 'response', 'retrieved_contexts'
        """
        # Call parent's _ascore to get the faithfulness score
        score = await super()._ascore(row, callbacks)
        
        # Store score for feedback generation
        self._last_score = score
        
        return score
    
    async def _generate_feedback_with_llm(self, score: float, row: Dict, threshold: float = 0.625) -> Dict[str, Any]:
        """
        ä½¿ç”¨ LLM ç”Ÿæˆåˆ†æèˆ‡æ”¹é€²å»ºè­°ï¼ˆåƒè€ƒ QualityCritic çš„è¨­è¨ˆï¼‰
        
        Args:
            score: Ragas Faithfulness çš„åŸå§‹åˆ†æ•¸ (0-1)
            row: è©•ä¼°è³‡æ–™
            threshold: åŠæ ¼é–¾å€¼ï¼ˆå°æ‡‰æ¨™æº–åŒ–åˆ†æ•¸ 4ï¼‰
        
        Returns:
            {"analysis": str, "suggestions": List[str]}
        """
        from langchain_core.messages import HumanMessage
        import json
        
        # æº–å‚™è©•ä¼°å…§å®¹
        user_input = row.get('user_input', '')
        response = row.get('response', '')
        contexts = row.get('retrieved_contexts', [])
        
        # æ¨™æº–åŒ–åˆ†æ•¸
        normalized_score = normalize_ragas_score(score)
        raw_linear_score = 1.0 + (score * 4.0)
        
        # Debug logging
        logger.info(f"ğŸ” Faithfulness LLM Feedback - Data Check:")
        logger.info(f"  - Ragas score: {score:.3f}")
        logger.info(f"  - Contexts count: {len(contexts)}")
        if contexts:
            logger.info(f"  - First context preview: {contexts[0][:100]}...")
        else:
            logger.warning(f"  âš ï¸  NO CONTEXTS provided to LLM feedback generation!")
        
        # æ§‹å»º promptï¼ˆåƒè€ƒ quality_critic çš„ Analysis-Rate-Suggest ç­–ç•¥ï¼‰
        prompt = f"""ä½ æ˜¯å°ˆæ¥­çš„äº‹å¯¦æ€§è©•ä¼°å°ˆå®¶ï¼Œè² è²¬æª¢æŸ¥ç”Ÿæˆçš„æ•™è‚²å…§å®¹æ˜¯å¦èˆ‡åƒè€ƒè³‡æ–™ä¸€è‡´ã€‚

**è©•ä¼°æŒ‡æ¨™ï¼šFaithfulness (å¿ å¯¦åº¦)**
- **å®šç¾©**ï¼šç­”æ¡ˆä¸­çš„é™³è¿°æ˜¯å¦éƒ½æœ‰åƒè€ƒè³‡æ–™çš„æ”¯æŒï¼Œä¸åŒ…å«è‡†æ¸¬æˆ–ç„¡æ³•é©—è­‰çš„è³‡è¨Š
- **Ragas è‡ªå‹•è©•åˆ†**ï¼š{score:.3f} (0-1 scale)
- **æ¨™æº–åŒ–åˆ†æ•¸**ï¼š{normalized_score}/5 (ç·šæ€§æ˜ å°„å¾Œå››æ¨äº”å…¥)
- **åŠæ ¼æ¨™æº–**ï¼šâ‰¥ 4 åˆ†ï¼ˆå°æ‡‰ Ragas â‰¥ {threshold}ï¼‰

**å¾…è©•ä¼°å…§å®¹ï¼š**
```
å•é¡Œï¼š{user_input}
å›ç­”ï¼š{response[:500]}{'...' if len(response) > 500 else ''}
```

**åƒè€ƒè³‡æ–™ï¼ˆæª¢ç´¢åˆ°çš„æ•™æåŸæ–‡ï¼‰ï¼š**
```
{chr(10).join(f"[Context {i+1}] {ctx[:300]}" for i, ctx in enumerate(contexts[:3])) if contexts else 'âš ï¸ ç„¡åƒè€ƒè³‡æ–™ï¼ˆé€™æ˜¯ç•°å¸¸æƒ…æ³ï¼ŒRagas è©•åˆ†å¯èƒ½ä¸æº–ç¢ºï¼‰'}
```

**è©•ä¼°è¦æ±‚ï¼š**
è«‹åŸºæ–¼ Ragas åˆ†æ•¸ï¼ˆ{score:.3f}ï¼‰å’Œä¸Šè¿°å…§å®¹ï¼Œæä¾›ï¼š
1. **analysis**ï¼šè©³ç´°åˆ†æç‚ºä½• Ragas çµ¦å‡ºæ­¤åˆ†æ•¸
   - å¼•ç”¨å…·é«”ä¾‹å­èªªæ˜ç­”æ¡ˆä¸­å“ªäº›é™³è¿°æœ‰/æ²’æœ‰åƒè€ƒè³‡æ–™æ”¯æŒ
   - åˆ†æç­”æ¡ˆæ˜¯å¦åŒ…å«è‡†æ¸¬ã€æ¨è«–æˆ–ç„¡æ³•é©—è­‰çš„è³‡è¨Š
   - å¿…é ˆä½¿ç”¨ç¹é«”ä¸­æ–‡

2. **suggestions**ï¼šå…·é«”çš„æ”¹é€²å»ºè­°ï¼ˆè‹¥åˆ†æ•¸ < {threshold}ï¼‰
   - è‹¥åˆ†æ•¸ â‰¥ {threshold}ï¼šå¯æä¾›å¯é¸çš„å„ªåŒ–å»ºè­°ï¼ˆå¯ç‚ºç©ºï¼‰
   - è‹¥åˆ†æ•¸ < {threshold}ï¼šå¿…é ˆæä¾›å…·é«”ã€å¯æ“ä½œçš„æ”¹é€²æ–¹å‘
   - æ¯æ¢å»ºè­°å¿…é ˆå…·é«”ä¸”å¯åŸ·è¡Œ
   - å¿…é ˆä½¿ç”¨ç¹é«”ä¸­æ–‡

**è¼¸å‡ºæ ¼å¼ï¼ˆåš´æ ¼ JSONï¼‰ï¼š**
```json
{{
  "analysis": "è©³ç´°åˆ†æ...",
  "suggestions": ["å»ºè­°1", "å»ºè­°2"]
}}
```

è«‹ç¢ºä¿è¼¸å‡ºçš„ JSON å¯è¢« Python json.loads() ç›´æ¥è§£æã€‚
"""
        
        try:
            llm = get_fact_critic_llm()
            messages = [HumanMessage(content=prompt)]
            response = await llm.ainvoke(messages)
            output = response.content.strip()
            
            # Parse JSON from response
            if "```json" in output:
                output = output.split("```json")[1].split("```")[0].strip()
            elif "```" in output:
                parts = output.split("```")
                if len(parts) >= 3:
                    output = parts[1].strip()
            
            parsed = json.loads(output)
            
            return {
                "analysis": parsed.get("analysis", ""),
                "suggestions": parsed.get("suggestions", [])
            }
            
        except Exception as e:
            logger.error(f"LLM feedback generation failed: {e}")
            # Fallback to simple feedback
            if score < 0.5:
                return {
                    "analysis": f"Ragas Faithfulness åˆ†æ•¸: {score:.2f}ã€‚ç­”æ¡ˆä¸­æœ‰å¤šè™•é™³è¿°æœªå¾—åˆ°ä¸Šä¸‹æ–‡çš„æ”¯æŒã€‚",
                    "suggestions": ["è«‹é€å¥æª¢æŸ¥ç­”æ¡ˆï¼Œç¢ºä¿æ¯å€‹é™³è¿°éƒ½æœ‰æ˜ç¢ºçš„è­‰æ“šä¾†æºã€‚"]
                }
            elif score < threshold:
                return {
                    "analysis": f"Ragas Faithfulness åˆ†æ•¸: {score:.2f}ã€‚ç­”æ¡ˆä¸­éƒ¨åˆ†é™³è¿°ç¼ºä¹ä¸Šä¸‹æ–‡æ”¯æŒã€‚",
                    "suggestions": ["å¼·åŒ–ç­”æ¡ˆèˆ‡åŸæ–‡çš„å°æ‡‰é—œä¿‚ï¼Œç§»é™¤ç„¡æ³•é©—è­‰çš„æ¨è«–ã€‚"]
                }
            else:
                return {
                    "analysis": f"Ragas Faithfulness åˆ†æ•¸: {score:.2f}ã€‚ç­”æ¡ˆå¤§è‡´æ­£ç¢ºã€‚",
                    "suggestions": []
                }
    
    async def score_with_feedback(self, row: Dict, callbacks: Any = None) -> Dict[str, Any]:
        """
        Computes score and returns detailed LLM-generated feedback in Traditional Chinese.
        
        Args:
            row: Dict with keys:
                - 'user_input': The question
                - 'response': The answer to evaluate
                - 'retrieved_contexts': List of context strings
        
        Returns:
            Dict with:
                - 'score': float (0-1, åŸå§‹ Ragas åˆ†æ•¸)
                - 'normalized_score': int (1-5, æ¨™æº–åŒ–åˆ†æ•¸)
                - 'raw_linear_score': float (1-5, ç·šæ€§æ˜ å°„æœªå››æ¨äº”å…¥)
                - 'analysis': str (LLM ç”Ÿæˆçš„åˆ†æ)
                - 'suggestions': List[str] (LLM ç”Ÿæˆçš„å»ºè­°)
        """
        score = None
        ragas_error = None
        
        # Try to get Ragas score
        try:
            score = await self._ascore(row, callbacks)
        except Exception as e:
            ragas_error = str(e)
            logger.warning(f"Ragas Faithfulness scoring failed (will use fallback): {ragas_error}")
            # Use a fallback heuristic score based on context overlap
            contexts = row.get('retrieved_contexts', [])
            response = row.get('response', '')
            if contexts and response:
                # Simple heuristic: check how many context keywords appear in response
                context_words = set(' '.join(contexts).lower().split())
                response_words = set(response.lower().split())
                overlap = len(context_words & response_words)
                total = len(context_words) if context_words else 1
                score = min(1.0, overlap / max(total * 0.3, 1))  # Normalize
            else:
                score = 0.5  # Neutral fallback
        
        try:
            # Generate LLM feedback (this uses our own code, not Ragas)
            llm_feedback = await self._generate_feedback_with_llm(score, row)
            
            # Calculate normalized score
            normalized_score = normalize_ragas_score(score)
            raw_linear_score = 1.0 + (score * 4.0)
            
            result = {
                "score": score,
                "normalized_score": normalized_score,
                "raw_linear_score": raw_linear_score,
                "analysis": llm_feedback["analysis"],
                "suggestions": llm_feedback["suggestions"]
            }
            
            if ragas_error:
                result["warning"] = f"Ragas scoring failed, used heuristic fallback. Error: {ragas_error[:100]}"
            
            return result
            
        except Exception as e:
            logger.error(f"Error in CustomFaithfulness evaluation: {e}")
            import traceback
            traceback.print_exc()
            return {
                "score": score if score is not None else 0.0,
                "normalized_score": normalize_ragas_score(score) if score else 1,
                "raw_linear_score": 1.0 + ((score or 0) * 4.0),
                "analysis": f"è©•ä¼°éç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}",
                "suggestions": [],
                "error": str(e)
            }


class TaskSatisfaction:
    """
    è©•ä¼°ç”Ÿæˆçµæœæ˜¯å¦æ»¿è¶³ä½¿ç”¨è€…çš„åŸºæœ¬ä»»å‹™è¦æ±‚ï¼ˆæ ¼å¼ã€æ•¸é‡ç­‰ï¼‰ã€‚
    ä½¿ç”¨åŠ æ¬Šæª¢æŸ¥é …ç›®è¨ˆç®— 1-5 åˆ†ã€‚
    
    ä¸­æ–‡åç¨±ï¼šä»»å‹™ç¬¦åˆåº¦
    """
    
    # æª¢æŸ¥é …ç›®å®šç¾©ï¼ˆexam_generationï¼‰
    EXAM_CHECKS = [
        {"name": "question_count", "weight": 2, "description": "é¡Œç›®æ•¸é‡æ˜¯å¦ç¬¦åˆè¦æ±‚"},
        {"name": "question_type", "weight": 2, "description": "é¡Œå‹æ˜¯å¦ç¬¦åˆï¼ˆé¸æ“‡é¡Œ/æ˜¯éé¡Œ/å•ç­”é¡Œï¼‰"},
        {"name": "has_options", "weight": 1, "description": "é¸æ“‡é¡Œæ˜¯å¦æœ‰ ABCD é¸é …"},
        {"name": "has_correct_answer", "weight": 2, "description": "æ˜¯å¦æœ‰æ­£ç¢ºç­”æ¡ˆ"},
        {"name": "has_source", "weight": 1, "description": "æ˜¯å¦æœ‰ä¾†æºå¼•ç”¨"},
    ]
    
    def __init__(self):
        self.llm = None
    
    def detect_task_type(self, user_query: str) -> str:
        """
        æ ¹æ“šä½¿ç”¨è€… query è‡ªå‹•åµæ¸¬ä»»å‹™é¡å‹ã€‚
        
        Returns:
            "exam_generation" | "summary" | "generic"
        """
        query_lower = user_query.lower()
        
        # Exam generation keywords
        exam_keywords = [
            "é¡Œ", "å‡ºé¡Œ", "é¸æ“‡é¡Œ", "æ˜¯éé¡Œ", "å•ç­”é¡Œ", "æ¸¬é©—", "è€ƒè©¦",
            "quiz", "question", "exam", "test", "ç·´ç¿’é¡Œ", "ç¿’é¡Œ"
        ]
        
        # Summary keywords
        summary_keywords = [
            "ç¸½çµ", "æ‘˜è¦", "æ•´ç†", "æ­¸ç´", "æ¦‚è¿°", "é‡é»",
            "summarize", "summary", "overview", "outline"
        ]
        
        # Check for exam keywords
        for kw in exam_keywords:
            if kw in query_lower:
                return "exam_generation"
        
        # Check for summary keywords
        for kw in summary_keywords:
            if kw in query_lower:
                return "summary"
        
        # Default to generic
        return "generic"
    
    async def evaluate(
        self,
        user_query: str,
        generated_content: Any,
        task_type: str = "auto"
    ) -> Dict[str, Any]:
        """
        è©•ä¼°ç”Ÿæˆçµæœæ˜¯å¦æ»¿è¶³ä»»å‹™è¦æ±‚ã€‚
        
        Args:
            user_query: ä½¿ç”¨è€…åŸå§‹è«‹æ±‚
            generated_content: ç”Ÿæˆçš„å…§å®¹ï¼ˆçµæ§‹åŒ–æˆ–å­—ä¸²ï¼‰
            task_type: ä»»å‹™é¡å‹ ("auto" | "exam_generation" | "summary" | "generic")
        
        Returns:
            {
                "score": float (0-1),
                "normalized_score": int (1-5),
                "task_type": str,  # åµæ¸¬åˆ°çš„ä»»å‹™é¡å‹
                "checks": List[Dict],
                "weighted_score": int,
                "total_weight": int,
                "analysis": str,
                "suggestions": List[str]
            }
        """
        # Auto-detect task type if not specified
        if task_type == "auto":
            task_type = self.detect_task_type(user_query)
        
        if task_type == "exam_generation":
            result = await self._evaluate_exam(user_query, generated_content)
        elif task_type == "summary":
            result = await self._evaluate_summary(user_query, generated_content)
        else:
            # Generic evaluation
            result = await self._evaluate_generic(user_query, generated_content)
        
        # Add detected task_type to result
        result["task_type"] = task_type
        return result
    
    async def _evaluate_exam(self, user_query: str, generated_content: Any) -> Dict[str, Any]:
        """è©•ä¼°é¡Œç›®ç”Ÿæˆçµæœ"""
        checks = []
        
        # è§£æç”Ÿæˆå…§å®¹
        questions = self._extract_questions(generated_content)
        
        # è§£æä½¿ç”¨è€…è¦æ±‚
        requirements = await self._parse_requirements(user_query)
        
        # 1. æª¢æŸ¥é¡Œç›®æ•¸é‡
        expected_count = requirements.get("count", 0)
        actual_count = len(questions)
        count_passed = (expected_count == 0) or (actual_count == expected_count)
        checks.append({
            "name": "question_count",
            "weight": 2,
            "passed": count_passed,
            "expected": expected_count if expected_count > 0 else "æœªæŒ‡å®š",
            "actual": actual_count
        })
        
        # 2. æª¢æŸ¥é¡Œå‹
        expected_type = requirements.get("type", "")
        actual_types = self._detect_question_types(questions)
        type_passed = (not expected_type) or (expected_type in actual_types)
        checks.append({
            "name": "question_type",
            "weight": 2,
            "passed": type_passed,
            "expected": expected_type if expected_type else "æœªæŒ‡å®š",
            "actual": ", ".join(actual_types) if actual_types else "æœªçŸ¥"
        })
        
        # 3. æª¢æŸ¥é¸é …ï¼ˆåƒ…é¸æ“‡é¡Œï¼‰
        has_options = all(
            self._has_options(q) for q in questions 
            if self._is_multiple_choice(q)
        )
        # å¦‚æœæ²’æœ‰é¸æ“‡é¡Œï¼Œè¦–ç‚ºé€šé
        options_applicable = any(self._is_multiple_choice(q) for q in questions)
        checks.append({
            "name": "has_options",
            "weight": 1,
            "passed": has_options if options_applicable else True,
            "note": "N/A (éé¸æ“‡é¡Œ)" if not options_applicable else None
        })
        
        # 4. æª¢æŸ¥æ­£ç¢ºç­”æ¡ˆ
        has_answers = all(self._has_answer(q) for q in questions)
        checks.append({
            "name": "has_correct_answer",
            "weight": 2,
            "passed": has_answers
        })
        
        # 5. æª¢æŸ¥ä¾†æºå¼•ç”¨
        has_sources = all(self._has_source(q) for q in questions)
        checks.append({
            "name": "has_source",
            "weight": 1,
            "passed": has_sources
        })
        
        # è¨ˆç®—åŠ æ¬Šåˆ†æ•¸
        weighted_score = sum(c["weight"] for c in checks if c["passed"])
        total_weight = sum(c["weight"] for c in checks)
        
        # è½‰æ›ç‚º 1-5 åˆ†
        ratio = weighted_score / total_weight if total_weight > 0 else 0
        raw_score = 1 + (ratio * 4)
        normalized_score = int(round(raw_score))
        
        # ç”Ÿæˆåˆ†æå’Œå»ºè­°
        analysis, suggestions = self._generate_feedback(checks, ratio)
        
        return {
            "score": ratio,
            "normalized_score": normalized_score,
            "checks": checks,
            "weighted_score": weighted_score,
            "total_weight": total_weight,
            "analysis": analysis,
            "suggestions": suggestions,
            "llm_usage": self.get_llm_usage()  # Include LLM cost for database logging
        }
    
    async def _evaluate_summary(self, user_query: str, generated_content: Any) -> Dict[str, Any]:
        """è©•ä¼°æ‘˜è¦ç”Ÿæˆçµæœï¼ˆç°¡åŒ–ç‰ˆï¼Œå¯å¾ŒçºŒæ“´å……ï¼‰"""
        checks = []
        
        content_str = str(generated_content) if generated_content else ""
        
        # 1. æª¢æŸ¥æ˜¯å¦æœ‰å…§å®¹
        has_content = len(content_str.strip()) > 50
        checks.append({
            "name": "has_content",
            "weight": 3,
            "passed": has_content
        })
        
        # 2. æª¢æŸ¥é•·åº¦åˆç†æ€§
        reasonable_length = 50 < len(content_str) < 5000
        checks.append({
            "name": "reasonable_length",
            "weight": 2,
            "passed": reasonable_length
        })
        
        # è¨ˆç®—åˆ†æ•¸
        weighted_score = sum(c["weight"] for c in checks if c["passed"])
        total_weight = sum(c["weight"] for c in checks)
        ratio = weighted_score / total_weight if total_weight > 0 else 0
        raw_score = 1 + (ratio * 4)
        normalized_score = int(round(raw_score))
        
        analysis = "æ‘˜è¦ç”Ÿæˆçµæœè©•ä¼°å®Œæˆã€‚" if ratio >= 0.8 else "æ‘˜è¦ç”Ÿæˆçµæœæœ‰å¾…æ”¹é€²ã€‚"
        suggestions = [] if ratio >= 0.8 else ["è«‹ç¢ºä¿æ‘˜è¦å…§å®¹å®Œæ•´ä¸”é•·åº¦é©ä¸­ã€‚"]
        
        return {
            "score": ratio,
            "normalized_score": normalized_score,
            "checks": checks,
            "weighted_score": weighted_score,
            "total_weight": total_weight,
            "analysis": analysis,
            "suggestions": suggestions
        }
    
    async def _evaluate_generic(self, user_query: str, generated_content: Any) -> Dict[str, Any]:
        """é€šç”¨è©•ä¼°ï¼ˆfallbackï¼‰"""
        has_content = bool(generated_content)
        ratio = 1.0 if has_content else 0.0
        
        return {
            "score": ratio,
            "normalized_score": 5 if has_content else 1,
            "checks": [{"name": "has_content", "weight": 1, "passed": has_content}],
            "weighted_score": 1 if has_content else 0,
            "total_weight": 1,
            "analysis": "å·²ç”Ÿæˆå…§å®¹ã€‚" if has_content else "æœªç”Ÿæˆä»»ä½•å…§å®¹ã€‚",
            "suggestions": [] if has_content else ["è«‹é‡æ–°å˜—è©¦ç”Ÿæˆã€‚"]
        }
    
    def _extract_questions(self, content: Any) -> List[Dict]:
        """å¾ç”Ÿæˆå…§å®¹ä¸­æå–é¡Œç›®åˆ—è¡¨"""
        if isinstance(content, list):
            # å¯èƒ½æ˜¯ç›´æ¥çš„é¡Œç›®åˆ—è¡¨
            questions = []
            for item in content:
                if isinstance(item, dict):
                    if "questions" in item:
                        # Copy block-level type to individual questions
                        block_type = item.get("type", "")
                        for q in item["questions"]:
                            if isinstance(q, dict) and not q.get("question_type"):
                                q["question_type"] = block_type
                        questions.extend(item["questions"])
                    elif "question" in item or "stem" in item or "statement_text" in item:
                        questions.append(item)
            return questions if questions else content
        elif isinstance(content, dict):
            if "questions" in content:
                # Copy type to individual questions
                block_type = content.get("type", "")
                for q in content["questions"]:
                    if isinstance(q, dict) and not q.get("question_type"):
                        q["question_type"] = block_type
                return content["questions"]
            elif "content" in content:
                return self._extract_questions(content["content"])
        elif isinstance(content, str):
            # å˜—è©¦å¾å­—ä¸²ä¸­è¨ˆç®—é¡Œç›®æ•¸é‡ï¼ˆç°¡åŒ–ï¼‰
            import re
            matches = re.findall(r'(?:é¡Œç›®|å•é¡Œ|Question)\s*\d+', content, re.IGNORECASE)
            return [{"raw": m} for m in matches] if matches else []
        return []
    
    async def _parse_requirements(self, user_query: str) -> Dict[str, Any]:
        """
        è§£æä½¿ç”¨è€…è¦æ±‚çš„é¡Œç›®æ•¸é‡å’Œé¡Œå‹
        
        ä½¿ç”¨æ··åˆå¼æ–¹æ³•ï¼š
        1. å…ˆå˜—è©¦è¦å‰‡å¼åŒ¹é…ï¼ˆå¿«é€Ÿã€é›¶æˆæœ¬ï¼‰
        2. å¦‚æœé¡Œå‹æœªè­˜åˆ¥ï¼Œç”¨ LLM fallbackï¼ˆç¢ºä¿æº–ç¢ºæ€§ï¼‰
        """
        import re
        
        requirements = {"count": 0, "type": "", "parse_method": "rules"}
        
        # Step 1: è¦å‰‡å¼è§£ææ•¸é‡
        count_patterns = [
            r'(\d+)\s*é¡Œ',
            r'(\d+)\s*é“',
            r'å‡º\s*(\d+)',
            r'ç”Ÿæˆ\s*(\d+)',
        ]
        for pattern in count_patterns:
            match = re.search(pattern, user_query)
            if match:
                requirements["count"] = int(match.group(1))
                break
        
        # Step 2: è¦å‰‡å¼è§£æé¡Œå‹ (æŒ‰å„ªå…ˆåº)
        type_mapping = [
            (["é¸æ“‡", "multiple choice", "mc"], "multiple_choice"),
            (["æ˜¯é", "åˆ¤æ–·", "true false", "å°éŒ¯"], "true_false"),
            (["å¡«ç©º", "fill in", "blank"], "fill_in_blank"),
            (["ç°¡ç­”", "å•ç­”", "short answer"], "short_answer"),
        ]
        
        query_lower = user_query.lower()
        for keywords, q_type in type_mapping:
            if any(kw in query_lower for kw in keywords):
                requirements["type"] = q_type
                break
        
        # Step 3: å¦‚æœé¡Œå‹æœªè­˜åˆ¥ï¼Œç”¨ LLM fallback
        if not requirements["type"]:
            llm_result = await self._parse_requirements_by_llm(user_query)
            if llm_result:
                requirements["type"] = llm_result.get("type", "")
                requirements["count"] = llm_result.get("count", requirements["count"])
                requirements["parse_method"] = "llm"
        
        return requirements
    
    async def _parse_requirements_by_llm(self, user_query: str) -> Optional[Dict[str, Any]]:
        """
        ä½¿ç”¨ LLM è§£æä½¿ç”¨è€…è¦æ±‚ï¼ˆè¦å‰‡å¼å¤±æ•—æ™‚çš„ fallbackï¼‰
        """
        import json
        import logging
        
        if not self.llm:
            from backend.app.agents.teacher_agent.critics.fact_critic import get_fact_critic_llm
            self.llm = get_fact_critic_llm()
        
        prompt = f"""åˆ†æä»¥ä¸‹ä½¿ç”¨è€…è«‹æ±‚ï¼Œæå–é¡Œç›®è¦æ±‚ã€‚

ä½¿ç”¨è€…è«‹æ±‚: "{user_query}"

è«‹ä»¥ JSON æ ¼å¼å›ç­”ï¼š
{{
    "count": <é¡Œç›®æ•¸é‡, æ•´æ•¸, å¦‚æœæœªæŒ‡å®šå‰‡ç‚º 0>,
    "type": "<é¡Œå‹, å¦‚: multiple_choice, true_false, fill_in_blank, short_answer, matching, ordering, æˆ–å…¶ä»–. å¦‚æœæœªæŒ‡å®šå‰‡ç‚ºç©ºå­—ä¸²>"
}}

åªå›ç­” JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚"""

        try:
            response = await self.llm.ainvoke(prompt)
            content = response.content.strip()
            
            # å˜—è©¦è§£æ JSON
            if content.startswith("```"):
                content = content.split("```")[1]
                if content.startswith("json"):
                    content = content[4:]
            
            result = json.loads(content)
            
            # è¨˜éŒ„ LLM æˆæœ¬
            token_usage = response.response_metadata.get("token_usage", {})
            prompt_tokens = token_usage.get("prompt_tokens", 0)
            completion_tokens = token_usage.get("completion_tokens", 0)
            
            # ç´¯åŠ åˆ° TaskSatisfaction çš„æˆæœ¬è¿½è¹¤
            if not hasattr(self, '_llm_usage'):
                self._llm_usage = {"prompt_tokens": 0, "completion_tokens": 0}
            self._llm_usage["prompt_tokens"] += prompt_tokens
            self._llm_usage["completion_tokens"] += completion_tokens
            
            logging.info(f"ğŸ“Š TaskSatisfaction LLM fallback used: {prompt_tokens}+{completion_tokens} tokens")
            
            return result
            
        except Exception as e:
            import logging
            logging.warning(f"LLM parse requirements failed: {e}")
            return None
    
    def get_llm_usage(self) -> Dict[str, int]:
        """å–å¾—æ­¤æ¬¡è©•ä¼°çš„ LLM ä½¿ç”¨é‡"""
        return getattr(self, '_llm_usage', {"prompt_tokens": 0, "completion_tokens": 0})
    
    def _detect_question_types(self, questions: List[Dict]) -> List[str]:
        """æª¢æ¸¬é¡Œç›®é¡å‹"""
        types = set()
        for q in questions:
            if isinstance(q, dict):
                q_type = q.get("type", q.get("question_type", ""))
                if q_type:
                    types.add(q_type)
                elif q.get("options") or q.get("choices"):
                    types.add("multiple_choice")
        return list(types)
    
    def _is_multiple_choice(self, question: Dict) -> bool:
        """åˆ¤æ–·æ˜¯å¦ç‚ºé¸æ“‡é¡Œ"""
        if isinstance(question, dict):
            q_type = question.get("type", question.get("question_type", ""))
            return q_type == "multiple_choice" or bool(question.get("options") or question.get("choices"))
        return False
    
    def _has_options(self, question: Dict) -> bool:
        """æª¢æŸ¥æ˜¯å¦æœ‰é¸é …"""
        if isinstance(question, dict):
            options = question.get("options") or question.get("choices") or []
            return len(options) >= 2
        return False
    
    def _has_answer(self, question: Dict) -> bool:
        """æª¢æŸ¥æ˜¯å¦æœ‰ç­”æ¡ˆ"""
        if isinstance(question, dict):
            return bool(
                question.get("correct_answer") or 
                question.get("answer") or 
                question.get("correct_option")
            )
        return False
    
    def _has_source(self, question: Dict) -> bool:
        """æª¢æŸ¥æ˜¯å¦æœ‰ä¾†æºå¼•ç”¨"""
        if isinstance(question, dict):
            return bool(
                question.get("source") or 
                question.get("source_page") or 
                question.get("evidence") or
                question.get("sources")
            )
        return False
    
    def _generate_feedback(self, checks: List[Dict], ratio: float) -> tuple:
        """ç”Ÿæˆåˆ†æå’Œå»ºè­°"""
        failed_checks = [c for c in checks if not c["passed"]]
        
        if ratio >= 0.875:
            analysis = "ç”Ÿæˆçµæœå®Œå…¨ç¬¦åˆè¦æ±‚ï¼Œæ‰€æœ‰æª¢æŸ¥é …ç›®å‡é€šéã€‚"
            suggestions = []
        elif ratio >= 0.625:
            analysis = f"ç”Ÿæˆçµæœå¤§è‡´ç¬¦åˆè¦æ±‚ï¼Œä½†æœ‰ {len(failed_checks)} é …æœªé€šéã€‚"
            suggestions = [f"æ”¹é€² {c['name']}" for c in failed_checks]
        else:
            analysis = f"ç”Ÿæˆçµæœä¸ç¬¦åˆè¦æ±‚ï¼Œæœ‰ {len(failed_checks)} é …é‡è¦æª¢æŸ¥æœªé€šéã€‚"
            suggestions = [f"å¿…é ˆæ”¹é€² {c['name']}" for c in failed_checks]
        
        return analysis, suggestions


def normalize_ragas_score(ragas_score: float) -> int:
    """
    å°‡ Ragas çš„ 0-1 åˆ†æ•¸æ¨™æº–åŒ–ç‚º 1-5 åˆ†ï¼ˆæ•´æ•¸ï¼‰
    
    ä½¿ç”¨ç·šæ€§æ˜ å°„ + å››æ¨äº”å…¥æ–¹æ³•ï¼š
    raw_score = 1 + (ragas_score Ã— 4)
    normalized_score = round(raw_score)
    
    å¯¦éš›æ˜ å°„ï¼ˆå››æ¨äº”å…¥å¾Œï¼‰ï¼š
    [0.0, 0.125) â†’ 1
    [0.125, 0.375) â†’ 2
    [0.375, 0.625) â†’ 3
    [0.625, 0.875) â†’ 4
    [0.875, 1.0] â†’ 5
    
    Args:
        ragas_score: Ragas åŸå§‹åˆ†æ•¸ (0.0-1.0)
    
    Returns:
        æ•´æ•¸åˆ†æ•¸ 1-5ï¼Œèˆ‡ G-Eval å°é½Š
        
    Examples:
        >>> normalize_ragas_score(0.1)   # â†’ 1
        >>> normalize_ragas_score(0.3)   # â†’ 2
        >>> normalize_ragas_score(0.5)   # â†’ 3
        >>> normalize_ragas_score(0.7)   # â†’ 4 (åŸæœ¬çš„é–¾å€¼)
        >>> normalize_ragas_score(0.9)   # â†’ 5
    """
    # Clamp to [0, 1]
    ragas_score = max(0.0, min(1.0, ragas_score))
    
    # Linear mapping
    raw_score = 1.0 + (ragas_score * 4.0)
    
    # Round to nearest integer
    return int(round(raw_score))


def get_fact_critic_llm() -> ChatOpenAI:
    """
    Get LLM for fact critic using Cook.ai project settings.
    """
    model_name = os.getenv("GENERATOR_MODEL", "gpt-4o-mini")
    return ChatOpenAI(model=model_name, temperature=0)

