import asyncio
import json
import logging
from typing import Dict, List, Any, Optional
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.language_models import BaseChatModel
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

logger = logging.getLogger(__name__)

RUBRICS = {
    # æƒ…å¢ƒä¸å®Œæ•´ã€å°ˆæœ‰åè©è¶…å‡ºå­¸ç”Ÿå­¸ç¿’ç¯„åœï¼ˆéœ€åƒè€ƒ RAG contextï¼Œè€Œéåƒ… source.evidenceï¼‰
    "Understandable": {
        "description": "å¯ç†è§£æ€§ (Understandable)",
        "1": "ç¼ºä¹å¿…è¦æƒ…å¢ƒèªªæ˜ï¼Œå­¸ç”Ÿç„¡æ³•ç†è§£ã€Œç‚ºä»€éº¼è¦å•é€™å€‹å•é¡Œã€ã€‚æˆ–ä½¿ç”¨å¤§é‡æ•™æï¼ˆRAG contextï¼‰ä¸­å®Œå…¨æœªå‡ºç¾çš„å°ˆæ¥­è¡“èªï¼ˆ4å€‹ä»¥ä¸Šï¼‰ï¼Œè¶…å‡ºå­¸ç”Ÿå­¸ç¿’ç¯„åœã€‚",
        "2": "æƒ…å¢ƒèªªæ˜åš´é‡ä¸è¶³ï¼Œåƒ…æä¾›ç‰‡æ®µè¨Šæ¯ã€‚æˆ–ä½¿ç”¨ 3 å€‹ä»¥ä¸Šè¶…å‡ºæ•™æç¯„åœçš„å°ˆæ¥­è¡“èªï¼Œå­¸ç”Ÿéœ€è¦é¡å¤–èƒŒæ™¯çŸ¥è­˜æ‰èƒ½ç†è§£ã€‚",
        "3": "æä¾›åŸºæœ¬æƒ…å¢ƒï¼Œä½†ä¸å¤ å®Œæ•´ã€‚æˆ–æœ‰ 1-2 å€‹è¡“èªè¶…å‡ºæ•™æç¯„åœï¼Œå­¸ç”Ÿç¶“éæ¨æ•²å¯ç†è§£é¡Œæ„ã€‚",
        "4": "æƒ…å¢ƒèªªæ˜å……è¶³ï¼Œå­¸ç”Ÿèƒ½ç†è§£å•é¡ŒèƒŒæ™¯å’Œç›®çš„ã€‚æ‰€æœ‰è¡“èªéƒ½åœ¨æ•™æï¼ˆRAG contextï¼‰ç¯„åœå…§ï¼Œç¬¦åˆå­¸ç”Ÿç¨‹åº¦ã€‚",
        "5": "æä¾›å®Œæ•´æƒ…å¢ƒå’ŒèƒŒæ™¯èªªæ˜ï¼Œå­¸ç”Ÿèƒ½æ¸…æ¥šç†è§£å•é¡Œçš„ä¾†é¾å»è„ˆã€‚è¡“èªä½¿ç”¨ç²¾æº–ä¸”å®Œå…¨ç¬¦åˆæ•™æå…§å®¹å’Œå­¸ç”Ÿç¨‹åº¦ã€‚"
    },
    
    # æ‹¼å¯«éŒ¯èª¤ã€æ¨™é»ç¬¦è™ŸéŒ¯èª¤
    "Grammatical": {
        "description": "èªæ³•æ­£ç¢ºæ€§ (Grammatical)",
        "1": "å­˜åœ¨å¤šå€‹åš´é‡æ‹¼å¯«éŒ¯èª¤ï¼ˆ3å€‹ä»¥ä¸Šï¼‰ï¼Œå¦‚å°ˆæ¥­è¡“èªæ‹¼éŒ¯ï¼ˆã€ŒPæ–½ã€æ‡‰ç‚ºã€ŒPCAã€ï¼‰ã€é—œéµå­—éŒ¯åˆ¥å­—ã€‚æˆ–ç¼ºå°‘å¿…è¦æ¨™é»ç¬¦è™Ÿå°è‡´èªæ„ä¸æ¸…ï¼Œå¥å­çµæ§‹æ··äº‚ç„¡æ³•ç†è§£ã€‚",
        "2": "å­˜åœ¨ 2-3 å€‹æ˜é¡¯çš„æ‹¼å¯«éŒ¯èª¤æˆ–éŒ¯åˆ¥å­—ï¼Œæˆ–æ¨™é»ä½¿ç”¨ä¸ç•¶å½±éŸ¿é–±è®€æµæš¢åº¦ï¼ˆå¦‚ç¼ºå°‘é€—è™Ÿã€å•è™Ÿä½ç½®éŒ¯èª¤ï¼‰ã€‚å¥å­çµæ§‹åŸºæœ¬æ­£ç¢ºä½†ç•¥é¡¯ç”Ÿç¡¬ã€‚",
        "3": "æœ‰ 1 å€‹è¼•å¾®çš„æ‹¼å¯«éŒ¯èª¤æˆ–æ¨™é»ç‘•ç–µï¼Œä½†ä¸å½±éŸ¿æ•´é«”ç†è§£ã€‚å¥å­çµæ§‹é€šé †ï¼Œæ–‡ç­†å°šå¯ã€‚",
        "4": "ç„¡æ˜é¡¯æ‹¼å¯«æˆ–æ¨™é»éŒ¯èª¤ï¼Œå¥å­çµæ§‹æµæš¢ï¼Œç”¨è©æ°ç•¶ã€‚èªæ³•èˆ‡æ ¼å¼ç¬¦åˆå­¸è¡“æ¨™æº–ã€‚",
        "5": "èªæ³•èˆ‡æ ¼å¼å“è¶Šï¼Œæ–‡ç­†æµæš¢ä¸”å®Œå…¨æ­£ç¢ºã€‚æ¨™é»ä½¿ç”¨ç²¾æº–ï¼Œå°ˆæ¥­è¡“èªæ‹¼å¯«å®Œå…¨æ­£ç¢ºï¼Œå¥å­çµæ§‹å„ªç¾æ˜“è®€ã€‚"
    },
    
    # é‚è¼¯çŸ›ç›¾ã€ç­”æ¡ˆèˆ‡åƒè€ƒè³‡æ–™ä¸ä¸€è‡´
    "Logical_Consistency": {
        "description": "é‚è¼¯ä¸€è‡´æ€§ (Logical Consistency)",
        "1": "æ­£ç¢ºç­”æ¡ˆèˆ‡åƒè€ƒè³‡æ–™ï¼ˆsource.evidence æˆ– RAG contextï¼‰åš´é‡çŸ›ç›¾ï¼Œç­”æ¡ˆæ˜ç¢ºéŒ¯èª¤ã€‚æˆ–é¡Œç›®é‚è¼¯æ··äº‚ï¼Œé¸é …ä¹‹é–“äº’ç›¸çŸ›ç›¾ã€‚",
        "2": "ç­”æ¡ˆèˆ‡åƒè€ƒè³‡æ–™éƒ¨åˆ†çŸ›ç›¾ï¼Œæˆ–é¡Œç›®é‚è¼¯æœ‰æ˜é¡¯æ¼æ´ã€‚é¸é …è¨­è¨ˆä¸ç•¶ï¼Œå¯èƒ½æœ‰å¤šå€‹åˆç†ç­”æ¡ˆæˆ–ç„¡æ­£ç¢ºç­”æ¡ˆã€‚",
        "3": "ç­”æ¡ˆèˆ‡åƒè€ƒè³‡æ–™åŸºæœ¬ä¸€è‡´ï¼Œä½†å­˜åœ¨è¼•å¾®çš„é‚è¼¯ç‘•ç–µæˆ–ä¸å¤ ç²¾ç¢ºçš„è¡¨è¿°ã€‚é¸é …è¨­è¨ˆå°šå¯ã€‚",
        "4": "ç­”æ¡ˆèˆ‡åƒè€ƒè³‡æ–™å®Œå…¨ä¸€è‡´ï¼Œé‚è¼¯æ¸…æ™°æ­£ç¢ºã€‚é¸é …è¨­è¨ˆåˆç†ï¼Œå¹²æ“¾é …æœ‰è¾¨è­˜åº¦ã€‚",
        "5": "ç­”æ¡ˆèˆ‡åƒè€ƒè³‡æ–™å®Œç¾å°æ‡‰ï¼Œé‚è¼¯åš´è¬¹ç„¡èª¤ã€‚é¸é …è¨­è¨ˆå„ªç§€ï¼Œæ¯å€‹é¸é …éƒ½æœ‰æ˜ç¢ºçš„é‚è¼¯ä¾æ“šã€‚"
    },
    # é€£æ¥è©ä½¿ç”¨é‡è¤‡æ€§å¤ªé«˜ã€ä¸é€šé †ã€å¤§é™¸ç”¨èª(å¯èƒ½è¼ƒé›£åˆ¤æ–·ï¼Œæœ‰å¯èƒ½åƒè€ƒè³‡æ–™æœ¬èº«å°±åŒ…å«ï¼Œå¦‚æœæœ‰æŠ“åˆ°çš„è©±å»ºè­°ä¸€ä¸‹å°±å¥½)
    "Phrasing": {
        "description": "æªè¾­æ­£ç•¶æ€§ (Phrasing)",
        "1": "ç”¨è©æ˜é¡¯ä¸ç¬¦åˆç¹é«”ä¸­æ–‡è¦ç¯„ï¼Œå«æœ‰å¤šå€‹ç°¡é«”ä¸­æ–‡è©å½™ï¼ˆä¾‹å¦‚ï¼šæœºå™¨å­¦ä¹ ã€æ•°æ®ã€è´¨é‡ï¼‰ã€‚æˆ–é€£æ¥è©ä½¿ç”¨æ¥µåº¦é‡è¤‡ï¼Œå¥å­åš´é‡ä¸é€šé †ï¼Œæªè¾­è–„å¼±å½±éŸ¿å°ˆæ¥­æ€§ã€‚",
        "2": "å«æœ‰å°‘é‡å¤§é™¸ç”¨èªæˆ–ç°¡é«”è©å½™ã€‚æˆ–é€£æ¥è©ä½¿ç”¨é‡è¤‡æ€§é«˜ï¼ˆåŒä¸€é€£æ¥è©å‡ºç¾3æ¬¡ä»¥ä¸Šï¼‰ï¼Œå¥å­çµæ§‹ç•¥é¡¯ç”Ÿç¡¬ä¸æµæš¢ã€‚",
        "3": "ç”¨è©åŸºæœ¬ç¬¦åˆç¹é«”ä¸­æ–‡è¦ç¯„ï¼Œä½†å¯èƒ½æœ‰ 1-2 è™•å¤§é™¸ç”¨èªï¼ˆè‹¥ä¾†æºæ–¼åƒè€ƒè³‡æ–™å‰‡å¯æ¥å—ï¼‰ã€‚é€£æ¥è©ä½¿ç”¨å°šå¯ï¼Œå¥å­é€šé †ä½†ç¼ºä¹è®ŠåŒ–ã€‚",
        "4": "ç”¨è©æ¸…æ™°æ°ç•¶ï¼Œç¬¦åˆå°ç£å­¸è¡“ç”¨èªç¿’æ…£ã€‚é€£æ¥è©ä½¿ç”¨å¾—ç•¶ï¼Œå¥å­æµæš¢æœ‰è®ŠåŒ–ã€‚å³ä½¿åƒè€ƒè³‡æ–™å«å¤§é™¸ç”¨èªï¼Œä¹Ÿå·²é©ç•¶è½‰æ›ã€‚",
        "5": "ç”¨è©ç²¾æº–å„ªç¾ï¼Œå®Œå…¨ç¬¦åˆå°ç£æ•™è‚²å…§å®¹è¦ç¯„ã€‚é€£æ¥è©ä½¿ç”¨éˆæ´»å¤šè®Šï¼Œå¥å­çµæ§‹è±å¯Œæµæš¢ï¼Œæ–‡ç­†å„ªç§€ã€‚"
    },
    "Core Concept Focus": {
        "description": "æ ¸å¿ƒæ¦‚å¿µèšç„¦æ€§ (Core Concept Focus)",
        "1": "å…§å®¹å®Œå…¨åé›¢æ ¸å¿ƒæ¦‚å¿µï¼Œéƒ½åœ¨è¨è«–æ¬¡è¦æˆ–ç„¡é—œçš„ç´°ç¯€ã€‚",
        "2": "å…§å®¹æœ‰æåˆ°æ ¸å¿ƒæ¦‚å¿µï¼Œä½†èŠ±è²»éå¤šç¯‡å¹…åœ¨æ¬¡è¦ç´°ç¯€ä¸Šã€‚",
        "3": "å…§å®¹æœ‰æ¸…æ¥šåœ°å‘ˆç¾æ ¸å¿ƒæ¦‚å¿µã€‚",
        "4": "å…§å®¹æ¸…æ¥šåœ°å‘ˆç¾æ ¸å¿ƒæ¦‚å¿µï¼Œä¸”èƒ½å€åˆ†ä¸»æ¬¡ï¼Œèˆ‡å­¸ç¿’ç›®æ¨™ç›¸é—œã€‚",
        "5": "å…§å®¹å®Œå…¨èšç„¦æ–¼æ ¸å¿ƒæ¦‚å¿µï¼Œä¸¦åœç¹å…¶å»ºæ§‹å‡ºæ·±åˆ»çš„è«–è¿°ï¼Œèˆ‡å­¸ç¿’ç›®æ¨™é«˜åº¦å°é½Šã€‚"
    },
    "Would You Use It": {
        "description": "æ¡ç”¨æ„é¡˜ (Would You Use It)",
        "1": "å®Œå…¨ä¸æœƒï¼Œé€™ä»½æ•™ææ¯«ç„¡ç”¨è™•æˆ–å……æ»¿éŒ¯èª¤ï¼Œä½¿ç”¨å®ƒå¯èƒ½å¸¶ä¾†èª¤å°å­¸ç”Ÿçš„é¢¨éšªã€‚",
        "2": "ä¸æœƒï¼Œé™¤éé€²è¡Œå¤§å¹…åº¦çš„ä¿®æ”¹ã€‚",
        "3": "æœƒï¼Œä½†åœ¨ä½¿ç”¨å‰éœ€è¦é€²è¡Œä¸€äº›é‡è¦çš„ä¿®æ”¹ã€‚æ•´é«”å“è³ªå‹‰å¼·åŠæ ¼ï¼Œç‘•ç–µé»å¯ä¿®å¾©ã€‚",
        "4": "æœƒï¼Œåªéœ€è¦é€²è¡Œä¸€äº›å¾®å°çš„æ½¤é£¾å³å¯ä½¿ç”¨ã€‚æ•´é«”å“è³ªåŠæ ¼ï¼Œç‘•ç–µé»å¯ä¿®å¾©ã€‚",
        "5": "çµ•å°æœƒï¼Œé€™ä»½æ•™æå¯ä»¥ç›´æ¥æ¡ç”¨ï¼Œå“è³ªå ªæ¯”äººé¡å°ˆå®¶ã€‚æ²’æœ‰ç™¼ç¾ä»»ä½•å“è³ªç‘•ç–µæˆ–æ½›åœ¨é¢¨éšªã€‚"
    }
}

class QualityCritic:
    """
    Evaluates educational content quality using Analyze-rate strategy (based on G-Eval research).
    
    Strategy: Rationale-Based LLM Evaluation Framework
    1. Analyze: LLM analyzes content against rubrics with detailed reasoning
    2. Rate: LLM assigns 1-5 score based on analysis
    3. Suggest: LLM generates improvement suggestions (always present in output)
    
    Key improvements over basic G-Eval:
    - Forces LLM to provide analysis BEFORE rating (rationale-first approach)
    - Requires suggestions field to always exist (enhances output consistency)
    - Strict JSON validation with RFC 8259 compliance
    - Markdown code block wrapping for robust parsing
    """
    def __init__(self, llm: BaseChatModel, threshold: float = 4.0):
        """
        Args:
            llm: Language model for evaluation
            threshold: Score threshold for improvement suggestions emphasis (default 4.0)
        """
        self.llm = llm
        self.threshold = threshold
    
    def _get_criterion_focus(self, criteria: List[str]) -> str:
        """
        Generate criterion-specific focus guidance to separate evaluation responsibilities.
        """
        focus_map = {
            "Understandable": """
**æœ¬æ¬¡è©•ä¼°ç¶­åº¦ï¼šå¯ç†è§£æ€§ (Understandable)**
- âœ… åƒ…è©•ä¼°ï¼šæƒ…å¢ƒæ˜¯å¦å®Œæ•´ã€è¡“èªæ˜¯å¦åœ¨å­¸ç”Ÿå­¸ç¿’ç¯„åœå…§
- âŒ ä¸è©•ä¼°ï¼šç­”æ¡ˆæ˜¯å¦æ­£ç¢ºã€æ‹¼å¯«éŒ¯èª¤ã€ç”¨èªè¦ç¯„ï¼ˆé€™äº›ç”±å…¶ä»–ç¶­åº¦è² è²¬ï¼‰
- ğŸ¯ ç„¦é»ï¼šã€Œå­¸ç”Ÿæ˜¯å¦èƒ½ç†è§£é¡Œç›®åœ¨å•ä»€éº¼ã€ç‚ºä»€éº¼è¦å•é€™å€‹å•é¡Œã€

**è©•ä¼°æ–¹æ³•ï¼š**
1. æª¢æŸ¥é¡Œç›®æ˜¯å¦æä¾›å……è¶³çš„èƒŒæ™¯æƒ…å¢ƒï¼ˆç‚ºä»€éº¼è¦å•é€™å€‹å•é¡Œï¼Ÿï¼‰
2. å°ç…§**æª¢ç´¢åˆ°çš„åƒè€ƒè³‡æ–™**ï¼ˆè‹¥æœ‰ï¼‰æª¢æŸ¥é¡Œç›®æ˜¯å¦éºæ¼é‡è¦èƒŒæ™¯è³‡è¨Š
3. æª¢æŸ¥å°ˆæ¥­è¡“èªæ˜¯å¦åœ¨æª¢ç´¢åˆ°çš„åƒè€ƒè³‡æ–™ä¸­å‡ºç¾é
4. **ä¸è¦**åƒ…å› ã€Œè¡“èªåœ¨ evidence ä¸­å‡ºç¾ã€å°±çµ¦é«˜åˆ†

**ç¯„ä¾‹ï¼š**
âŒ éŒ¯èª¤è©•ä¼°ï¼šã€Œè¡“èªéƒ½åœ¨ evidence ä¸­ï¼Œçµ¦5åˆ†ã€
âœ… æ­£ç¢ºè©•ä¼°ï¼šã€Œé¡Œç›®ç¼ºå°‘ã€ç‚ºä»€éº¼è–ªæ°´æœ‰ç¼ºå¤±å€¼ã€ã€ã€ç‚ºä»€éº¼é¸ä¸­ä½æ•¸ã€çš„èƒŒæ™¯èªªæ˜ï¼ˆæª¢ç´¢åˆ°çš„åƒè€ƒè³‡æ–™ä¸­æœ‰æåˆ°ï¼‰ï¼Œçµ¦2åˆ†ã€

ğŸ“ å³ä½¿ç™¼ç¾ç­”æ¡ˆçŸ›ç›¾æˆ–æ‹¼å¯«éŒ¯èª¤ï¼Œä¹Ÿæ‡‰å°ˆæ³¨æ–¼æƒ…å¢ƒå®Œæ•´æ€§è©•åˆ†
""",
            "Grammatical": """
**æœ¬æ¬¡è©•ä¼°ç¶­åº¦ï¼šèªæ³•æ­£ç¢ºæ€§ (Grammatical)**
- âœ… åƒ…è©•ä¼°ï¼šæ‹¼å¯«éŒ¯èª¤ã€éŒ¯åˆ¥å­—ã€æ¨™é»ç¬¦è™Ÿä½¿ç”¨ï¼ˆåŠå½¢/å…¨å½¢ã€å•è™Ÿ/å¥è™Ÿï¼‰
- âŒ ä¸è©•ä¼°ï¼šç­”æ¡ˆæ­£ç¢ºæ€§ã€æƒ…å¢ƒå®Œæ•´æ€§ã€é‚è¼¯çŸ›ç›¾ï¼ˆé€™äº›ç”±å…¶ä»–ç¶­åº¦è² è²¬ï¼‰
- ğŸ¯ ç„¦é»ï¼šã€Œæ–‡å­—æœ¬èº«æ˜¯å¦æœ‰éŒ¯èª¤ã€
- ğŸ“ ç‰¹åˆ¥æ³¨æ„ï¼šå°ˆæ¥­è¡“èªæ‹¼å¯«ï¼ˆå¦‚ã€ŒPæ–½ã€æ‡‰ç‚ºã€ŒPCAã€ï¼‰ã€å…¨å½¢/åŠå½¢æ¨™é»æ··ç”¨
- ğŸ“ å³ä½¿å…§å®¹é‚è¼¯æœ‰å•é¡Œï¼Œåªè¦æ–‡å­—æ‹¼å¯«æ­£ç¢ºï¼Œä»æ‡‰çµ¦é«˜åˆ†
""",
            "Logical_Consistency": """
**æœ¬æ¬¡è©•ä¼°ç¶­åº¦ï¼šé‚è¼¯ä¸€è‡´æ€§ (Logical_Consistency)**
- âœ… åƒ…è©•ä¼°ï¼šç­”æ¡ˆèˆ‡åƒè€ƒè³‡æ–™æ˜¯å¦ä¸€è‡´ã€é¸é …é‚è¼¯æ˜¯å¦åˆç†
- âŒ ä¸è©•ä¼°ï¼šæ‹¼å¯«éŒ¯èª¤ã€æƒ…å¢ƒå®Œæ•´æ€§ã€ç”¨èªè¦ç¯„ï¼ˆé€™äº›ç”±å…¶ä»–ç¶­åº¦è² è²¬ï¼‰
- ğŸ¯ ç„¦é»ï¼šã€Œæ­£ç¢ºç­”æ¡ˆæ˜¯å¦èˆ‡ evidence/æª¢ç´¢åˆ°çš„åƒè€ƒè³‡æ–™ çŸ›ç›¾ã€
- ğŸ“ å³ä½¿é¡Œç›®æƒ…å¢ƒä¸å®Œæ•´ï¼Œåªè¦ç­”æ¡ˆèˆ‡è³‡æ–™ä¸€è‡´ï¼Œä»æ‡‰çµ¦é«˜åˆ†
""",
            "Phrasing": """
**æœ¬æ¬¡è©•ä¼°ç¶­åº¦ï¼šæªè¾­æ­£ç•¶æ€§ (Phrasing)**
- âœ… åƒ…è©•ä¼°ï¼šæ˜¯å¦ä½¿ç”¨å¤§é™¸ç”¨èª/ç°¡é«”è©å½™ã€é€£æ¥è©æ˜¯å¦é‡è¤‡ã€å¥å­æ˜¯å¦é€šé †
- âŒ ä¸è©•ä¼°ï¼šç­”æ¡ˆæ­£ç¢ºæ€§ã€æ‹¼å¯«éŒ¯èª¤ã€æƒ…å¢ƒå®Œæ•´æ€§ï¼ˆé€™äº›ç”±å…¶ä»–ç¶­åº¦è² è²¬ï¼‰
- ğŸ¯ ç„¦é»ï¼šã€Œç”¨è©æ˜¯å¦ç¬¦åˆå°ç£ç¹é«”ä¸­æ–‡è¦ç¯„ã€å¥å­æ˜¯å¦æµæš¢ã€
- ğŸ“ ç‰¹åˆ¥æ³¨æ„ï¼šã€Œæœºå™¨å­¦ä¹ ã€â†’ã€Œæ©Ÿå™¨å­¸ç¿’ã€ã€ã€Œæ•°æ®ã€â†’ã€Œè³‡æ–™ã€ç­‰ç°¡é«”è©å½™
- ğŸ“ è‹¥åƒè€ƒè³‡æ–™æœ¬èº«å«å¤§é™¸ç”¨èªï¼Œå‰‡å¯å¯¬é¬†çœ‹å¾…ï¼ˆåœ¨åˆ†æä¸­èªªæ˜ï¼‰
"""
        }
        
        focus_texts = [focus_map.get(c, "") for c in criteria if c in focus_map]
        return "\n".join(focus_texts)
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type(Exception)
    )
    async def evaluate(self, content: Dict[str, Any], criteria: List[str] = None) -> Dict[str, Any]:
        """
        Evaluates a single content item using Analyze-rate strategy (based on G-Eval).
        
        Args:
            content: Content to evaluate (dict format, will be serialized to JSON)
            criteria: List of criteria names to evaluate. If None, evaluates all rubrics.
        
        Returns:
            Dict with structure:
            {
                "evaluations": [
                    {
                        "criteria": str,
                        "analysis": str (Traditional Chinese),
                        "rating": int (1-5),
                        "suggestions": List[str] (always present, empty if no issues)
                    }
                ]
            }
        """
        if criteria is None:
            criteria = list(RUBRICS.keys())
            
        # Prepare content string
        content_str = json.dumps(content, ensure_ascii=False, indent=2)
        
        # Construct rubric text with evaluation steps
        rubric_sections = []
        for key in criteria:
            if key in RUBRICS:
                r = RUBRICS[key]
                section = f"### {r['description']}\n"
                section += "**è©•åˆ†æ¨™æº–ï¼š**\n"
                for score in ["1", "2", "3", "4", "5"]:
                    section += f"- {score} åˆ†ï¼š{r[score]}\n"
                rubric_sections.append(section)
        
        rubric_text = "\n".join(rubric_sections)
        
        # Criterion-specific focus guidance
        criterion_focus = self._get_criterion_focus(criteria)
        
        # Improved prompt following Analyze-rate strategy with enhanced robustness
        prompt = f"""ä½ æ˜¯ä¸€ä½å°æ ¼å¼è¦æ±‚æ¥µåº¦ç²¾ç¢ºçš„å°ˆæ¥­æ•™è‚²å…§å®¹è©•ä¼°å°ˆå®¶ã€‚

**ä½ çš„è§’è‰²èˆ‡è²¬ä»»ï¼š**
- ä½¿ç”¨åš´è¬¹çš„æ¨™æº–è©•ä¼°æ•™è‚²å…§å®¹
- æä¾›è©³ç´°çš„åˆ†æä½œç‚ºè©•åˆ†çš„å”¯ä¸€ä¾æ“š
- è¼¸å‡ºæ ¼å¼å¿…é ˆ 100% ç¬¦åˆ JSON è¦ç¯„

---

**ğŸš¨ é‡è¦ï¼šé—œæ–¼åƒè€ƒè³‡æ–™çš„ä¾†æº**

å¾…è©•ä¼°å…§å®¹ä¸­åŒ…å«å…©ç¨®åƒè€ƒè³‡æ–™ï¼š
1. **`source.evidence`**ï¼šç”± LLM Generator è‡ªå‹•ç²¾ç°¡ç”Ÿæˆçš„æ‘˜è¦ï¼Œ**å¯èƒ½ä¸å®Œæ•´æˆ–æœ‰èª¤**
2. **æª¢ç´¢åˆ°çš„åƒè€ƒè³‡æ–™**ï¼šçœŸæ­£çš„æ•™æåŸæ–‡å…§å®¹ï¼ˆè‹¥æœ‰æä¾›ï¼‰

**è©•ä¼°å„ªå…ˆç´šï¼š**
- âœ… **å„ªå…ˆåƒè€ƒã€Œæª¢ç´¢åˆ°çš„åƒè€ƒè³‡æ–™ã€**ï¼ˆçœŸå¯¦æ•™æï¼‰ä¾†åˆ¤æ–·è¡“èªç¯„åœã€æƒ…å¢ƒå®Œæ•´æ€§
- âš ï¸ **è¬¹æ…ä½¿ç”¨ `source.evidence`**ï¼ˆLLMç”Ÿæˆçš„æ‘˜è¦ï¼Œå¯èƒ½å‡ºéŒ¯ï¼‰
- ğŸ“ è‹¥å…©è€…å…§å®¹ä¸ä¸€è‡´ï¼Œä»¥æª¢ç´¢åˆ°çš„åƒè€ƒè³‡æ–™ç‚ºæº–

**ç¯„ä¾‹èªªæ˜ï¼š**
```json
{{
  "source": {{
    "evidence": "è–ªæ°´åˆ—å¡«è£œç‚ºä¸­ä½æ•¸"  // â† LLM ç²¾ç°¡çš„ï¼Œå¯èƒ½ç¼ºæ¼æƒ…å¢ƒ
  }},
  "æª¢ç´¢åˆ°çš„åƒè€ƒè³‡æ–™": "...è–ªæ°´æ¬„ä½æœ‰20%ç¼ºå¤±å€¼ï¼Œç”±æ–¼è–ªæ°´åˆ†å¸ƒæœ‰æ¥µç«¯å€¼..."  // â† çœŸå¯¦æ•™æ
}}
```
â†’ è©•ä¼°æƒ…å¢ƒå®Œæ•´æ€§æ™‚ï¼Œæ‡‰æª¢æŸ¥é¡Œç›®æ˜¯å¦åŒ…å«æª¢ç´¢åˆ°çš„åƒè€ƒè³‡æ–™ä¸­çš„èƒŒæ™¯èªªæ˜ï¼ˆç¼ºå¤±å€¼ã€æ¥µç«¯å€¼ï¼‰ï¼Œè€Œéåªçœ‹ evidence

---

**é‡è¦ï¼šè©•ä¼°è·è²¬åˆ†é›¢**

{criterion_focus}

**è«‹åš´æ ¼éµå®ˆä¸Šè¿°è·è²¬ç¯„åœï¼Œä¸è¦è·¨è¶Šåˆ°å…¶ä»–è©•ä¼°ç¶­åº¦ã€‚**

---

**è©•ä¼°ç­–ç•¥ (Analyze-Rate Strategy):**

æœ¬è©•ä¼°æ¡ç”¨**åŸºæ–¼é‡‹ç¾©çš„ LLM è©•ä¼°æ¡†æ¶ (Rationale-Based Evaluation)**ï¼š

1. **åˆ†æ (Analyze)**
   - ä»”ç´°é–±è®€å¾…è©•ä¼°å…§å®¹
   - **åƒ…é‡å°ç•¶å‰è©•ä¼°ç¶­åº¦**é€²è¡Œæ·±å…¥ã€å…·é«”çš„åˆ†æ
   - åˆ†æå¿…é ˆ**å¼•ç”¨å…§å®¹ä¸­çš„å…·é«”ä¾‹å­**
   - åˆ†æå¿…é ˆ**å…ˆæ–¼è©•åˆ†**å®Œæˆï¼Œä½œç‚ºè©•åˆ†çš„å”¯ä¸€ä¾æ“š

2. **è©•åˆ† (Rate)**
   - åŸºæ–¼åˆ†æçµæœï¼Œ**åƒ…é‡å°ç•¶å‰ç¶­åº¦**çµ¦äºˆ 1-5 åˆ†
   - è©•åˆ†å¿…é ˆèˆ‡åˆ†æé‚è¼¯ä¸€è‡´
   - **å¿½ç•¥å…¶ä»–ç¶­åº¦çš„å•é¡Œ**ï¼Œå³ä½¿ç™¼ç¾äº†ä¹Ÿä¸æ‡‰å½±éŸ¿ç•¶å‰ç¶­åº¦çš„è©•åˆ†

3. **å»ºè­° (Suggest)**
   - **ç„¡è«–åˆ†æ•¸é«˜ä½**ï¼Œéƒ½è¦ç¸½çµæ”¹é€²ç©ºé–“
   - è‹¥è©•åˆ† >= {self.threshold}ï¼šæä¾›å¯é¸çš„å„ªåŒ–å»ºè­°ï¼ˆå¯ç‚ºç©ºï¼‰
   - è‹¥è©•åˆ† < {self.threshold}ï¼šå¿…é ˆæä¾›å…·é«”ã€å¯æ“ä½œçš„æ”¹é€²å»ºè­°

**é‡è¦ï¼šåš´æ ¼è©•åˆ†æ ¡æº–**
è«‹åƒè€ƒä»¥ä¸‹ç¯„ä¾‹ä¾†æ ¡æº–æ‚¨çš„è©•åˆ†æ¨™æº–ã€‚

**ç‰¹åˆ¥æ³¨æ„ï¼šæœ¬è©•ä¼°çš„å‰ææ˜¯ã€Œå­¸ç”Ÿå·²çœ‹éå®Œæ•´æ•™æã€ï¼Œå› æ­¤ï¼š**
1. **ä¸éœ€è¦**åœ¨é¡Œç›®ä¸­é‡è¤‡æä¾›å®Œæ•´æƒ…å¢ƒæ•…äº‹
2. **é‡é»æª¢æŸ¥**ï¼šé¡Œç›®ä¸­çš„å°ˆæ¥­è¡“èªæ˜¯å¦åœ¨ `source.evidence` ä¸­æœ‰å‡ºç¾
3. **åš´æ ¼ç¦æ­¢**ï¼šä½¿ç”¨æ•™æä¸­å®Œå…¨æœªæåŠçš„è¡“èª
4. **å„ªå…ˆæª¢æŸ¥**ï¼šæ­£ç¢ºç­”æ¡ˆæ˜¯å¦èˆ‡ evidence çŸ›ç›¾ï¼ˆæœ€åš´é‡éŒ¯èª¤ï¼‰

**ã€1 åˆ†ç¯„ä¾‹ Aã€‘æ­£ç¢ºç­”æ¡ˆèˆ‡ evidence çŸ›ç›¾:**
å•é¡Œï¼šã€Œå¡«è£œç¼ºå¤±å€¼çš„æ–¹å¼ä¹‹ä¸€æ˜¯ä½¿ç”¨ä»€éº¼ä¾†å¡«è£œå¹´é½¡ï¼Ÿã€
æ­£ç¢ºç­”æ¡ˆ: A (ä¸­ä½æ•¸)
evidence: "å¹´é½¡ (Age) åˆ—å¡«è£œç‚ºå¹³å‡å€¼ã€‚"
â†’ **åš´é‡çŸ›ç›¾**ï¼šæ­£ç¢ºç­”æ¡ˆæ˜¯ã€Œä¸­ä½æ•¸ã€ï¼Œä½† evidence æ˜ç¢ºèªªã€Œå¹³å‡å€¼ã€
â†’ ç­”æ¡ˆèˆ‡è­‰æ“šå®Œå…¨ç›¸åï¼Œå­¸ç”Ÿæœƒå®Œå…¨æ··æ·†ï¼Œé€™æ˜¯é‚è¼¯éŒ¯èª¤
â†’ **å„ªå…ˆåµæ¸¬æ­¤é¡å•é¡Œ**
â†’ è©•åˆ†ï¼š**1 åˆ†**

**ã€1 åˆ†ç¯„ä¾‹ Bã€‘è¡“èªå®Œå…¨æœªåœ¨æ•™æå‡ºç¾:**
å•é¡Œï¼šã€Œåœ¨æ·±åº¦å­¸ç¿’ä¸­ï¼Œä½¿ç”¨ Adam optimizer çš„ä¸»è¦å„ªå‹¢ç‚ºä½•ï¼Ÿã€
evidence: "æ©Ÿå™¨å­¸ç¿’æœ‰å¤šç¨®æ–¹æ³•ã€‚"
â†’ **å•é¡Œ**ï¼šé¡Œç›®æåˆ°ã€Œæ·±åº¦å­¸ç¿’ã€ã€ã€ŒAdam optimizerã€éƒ½æœªåœ¨ evidence ä¸­å‡ºç¾
â†’ å­¸ç”Ÿå³ä½¿çœ‹éæ•™æä¹Ÿç„¡å¾å¾—çŸ¥é€™äº›è¡“èª
â†’ è©•åˆ†ï¼š**1 åˆ†**

**ã€2 åˆ†ç¯„ä¾‹ã€‘å¤šå€‹è¡“èªæœªåœ¨æ•™æå‡ºç¾:**
å•é¡Œï¼šã€Œä½¿ç”¨ KNN ç®—æ³•é€²è¡Œç‰¹å¾µé¸æ“‡çš„ç›®çš„ç‚ºä½•ï¼Ÿã€
evidence: "ä½¿ç”¨ KNN ç®—æ³•é æ¸¬ç¼ºå¤±å€¼ã€‚"
â†’ **å•é¡Œ**ï¼šã€Œç‰¹å¾µé¸æ“‡ã€æœªåœ¨ evidence ä¸­æåŠï¼ˆevidence åªèªªã€Œé æ¸¬ç¼ºå¤±å€¼ã€ï¼‰
â†’ å­¸ç”Ÿæœƒæ··æ·† KNN çš„ç”¨é€”
â†’ è©•åˆ†ï¼š**2 åˆ†**

**ã€3 åˆ†ç¯„ä¾‹ã€‘å¤§éƒ¨åˆ†è¡“èªæœ‰å‡ºç¾:**
å•é¡Œï¼šã€Œå¡«è£œç¼ºå¤±å€¼å¯ä»¥ä½¿ç”¨ä¸­ä½æ•¸æˆ–å¹³å‡æ•¸ï¼Œä½•è€…è¼ƒä¸å—æ¥µç«¯å€¼å½±éŸ¿ï¼Ÿã€
evidence: "å¡«è£œç¼ºå¤±å€¼å¯ä½¿ç”¨å¹³å‡æ•¸ã€‚"
â†’ **ç‹€æ³**ï¼ševidence æåˆ°ã€Œå¹³å‡æ•¸ã€ï¼Œä½†æœªæåŠã€Œä¸­ä½æ•¸ã€å’Œã€Œæ¥µç«¯å€¼ã€
â†’ å­¸ç”Ÿçœ‹éæ•™æçš„å…¶ä»–éƒ¨åˆ†å¯èƒ½çŸ¥é“ä¸­ä½æ•¸æ¦‚å¿µ
â†’ è©•åˆ†ï¼š**3 åˆ†**

**ã€4 åˆ†ç¯„ä¾‹ã€‘æ‰€æœ‰è¡“èªéƒ½åœ¨æ•™æä¸­:**
å•é¡Œï¼šã€Œä½¿ç”¨ KNN ç®—æ³•çš„ç›®çš„ç‚ºä½•ï¼Ÿã€
evidence: "ä½¿ç”¨å¦‚ KNNï¼ˆK-Nearest Neighborsï¼‰ç­‰ç®—æ³•ï¼Œæ ¹æ“šç›¸ä¼¼è¨˜éŒ„ä¾†é æ¸¬ç¼ºå¤±å€¼ã€‚"
â†’ **å„ªé»**ï¼šKNNã€é æ¸¬ç¼ºå¤±å€¼éƒ½åœ¨ evidence ä¸­æ˜ç¢ºæåˆ°
â†’ å­¸ç”Ÿçœ‹éæ•™æå¾Œèƒ½ç›´æ¥ç†è§£
â†’ è©•åˆ†ï¼š**4 åˆ†**

**è©•ä¼°æ­¥é©Ÿï¼ˆé‡å° Understandableï¼‰ï¼š**
1. **å„ªå…ˆï¼šæª¢æŸ¥çŸ›ç›¾**
   - æ­£ç¢ºç­”æ¡ˆæ˜¯å¦èˆ‡ source.evidence å…§å®¹ç›¸åæˆ–çŸ›ç›¾ï¼Ÿ
   - è‹¥çŸ›ç›¾ â†’ **ç›´æ¥è©• 1 åˆ†**ï¼Œè¨˜éŒ„çŸ›ç›¾é»ï¼Œç„¡éœ€ç¹¼çºŒå¾ŒçºŒæª¢æŸ¥
   
2. **æå–é¡Œç›®ä¸­çš„å°ˆæ¥­è¡“èª**ï¼ˆåŒ…æ‹¬ question_text å’Œ optionsï¼‰

3. **æª¢æŸ¥ source.evidence**ï¼šé€™äº›è¡“èªæ˜¯å¦åœ¨ evidence ä¸­å‡ºç¾ï¼Ÿ

4. **è¨ˆæ•¸æœªå‡ºç¾çš„è¡“èª**ï¼š
   - 4+ å€‹æœªå‡ºç¾ â†’ 1 åˆ†
   - 3 å€‹æœªå‡ºç¾ â†’ 2 åˆ†  
   - 1-2 å€‹æœªå‡ºç¾ â†’ 3 åˆ†
   - å…¨éƒ¨å‡ºç¾ â†’ 4-5 åˆ†

---

**è©•åˆ†æ¨™æº– (Rubrics):**

{rubric_text}

---

**å¾…è©•ä¼°å…§å®¹:**

{content_str}

---

**è¼¸å‡ºæ ¼å¼è¦æ±‚ (CRITICAL - å¿…é ˆåš´æ ¼éµå®ˆ):**

1. **JSON æ¨™æº–**ï¼š
   - è¼¸å‡ºå¿…é ˆåš´æ ¼ç¬¦åˆ RFC 8259 æ¨™æº–
   - æ‰€æœ‰éµå’Œå­—ä¸²å€¼å¿…é ˆä½¿ç”¨é›™å¼•è™Ÿ `"`
   - ä¸å¾—ä½¿ç”¨å–®å¼•è™Ÿæˆ–å…¶ä»–éæ¨™æº–å­—ç¬¦
   - è¼¸å‡ºä¸­ä¸å¾—åŒ…å«ä»»ä½•é¡å¤–çš„è§£é‡‹æ€§æ–‡å­—

2. **çµæ§‹è¦æ±‚**ï¼š
   - ç‚º**ä¸Šè¿° Rubrics ä¸­çš„æ¯ä¸€å€‹è©•åˆ†æ¨™æº–**éƒ½ç”¢ç”Ÿä¸€å€‹è©•ä¼°ç‰©ä»¶
   - `suggestions` æ¬„ä½å¿…é ˆå§‹çµ‚å­˜åœ¨
   - è‹¥ç„¡å»ºè­°ï¼Œ`suggestions` å¿…é ˆç‚ºç©ºé™£åˆ— `[]`ï¼ˆä¸å¯çœç•¥æ­¤æ¬„ä½ï¼‰

3. **Markdown åŒ…è£**ï¼š
   - è«‹å°‡ JSON è¼¸å‡ºåŒ…è£åœ¨ Markdown ç¨‹å¼ç¢¼å€å¡Šä¸­ï¼š
   ```json
   {{
     "evaluations": [...]
   }}
   ```

**JSON çµæ§‹ç¯„ä¾‹ï¼š**

```json
{{
  "evaluations": [
    {{
      "criteria": "å¯ç†è§£æ€§ (Understandable)",
      "analysis": "ã€å¿…å¡«ã€‘è©³ç´°åˆ†æï¼Œå¿…é ˆå¼•ç”¨å…§å®¹ä¸­çš„å…·é«”ä¾‹å­ï¼Œèªªæ˜ç‚ºä½•çµ¦äºˆæ­¤è©•åˆ†ã€‚åˆ†ææ‡‰å…ˆæ–¼è©•åˆ†å®Œæˆã€‚",
      "rating": 4,
      "suggestions": ["å»ºè­°1ï¼šå…·é«”å¯æ“ä½œçš„æ”¹é€²æ–¹å‘", "å»ºè­°2ï¼š..."]
    }},
    {{
      "criteria": "èªæ³•æ­£ç¢ºæ€§ (Grammatical)",
      "analysis": "ã€å¿…å¡«ã€‘èªæ³•å„ªç§€ï¼Œç„¡æ˜é¡¯éŒ¯èª¤ã€‚",
      "rating": 5,
      "suggestions": []
    }}
  ]
}}
```

**é‡è¦æç¤ºï¼š**
- æ‰€æœ‰æ–‡å­—ï¼ˆanalysis, suggestionsï¼‰å¿…é ˆä½¿ç”¨ç¹é«”ä¸­æ–‡
- `analysis` å¿…é ˆå…ˆæ–¼ `rating` æ€è€ƒå®Œæˆï¼Œä½œç‚ºè©•åˆ†çš„**å”¯ä¸€ä¾æ“š**
- æ¯å€‹è©•ä¼°ç‰©ä»¶çš„ `suggestions` æ¬„ä½å¿…é ˆå­˜åœ¨ï¼ˆå³ä½¿ç‚ºç©ºé™£åˆ—ï¼‰
- è¼¸å‡ºçš„ JSON å¿…é ˆèƒ½è¢« Python çš„ `json.loads()` ç›´æ¥è§£æï¼Œä¸å¾—æœ‰ä»»ä½•èªæ³•éŒ¯èª¤

ç¾åœ¨è«‹é–‹å§‹è©•ä¼°ï¼Œä¸¦ä»¥ä¸Šè¿° JSON æ ¼å¼è¼¸å‡ºçµæœã€‚
"""
        
        messages = [HumanMessage(content=prompt)]
        
        try:
            # Call LLM with temperature=0 for consistency
            response = await self.llm.ainvoke(messages)
            output = response.content.strip()
            
            # Parse JSON from response
            parsed = self._parse_json_response(output)
            
            # Validate structure
            if "evaluations" not in parsed:
                raise ValueError("Response missing 'evaluations' key")
            
            # Strict validation: ensure all evaluations have required fields
            for i, eval_item in enumerate(parsed["evaluations"]):
                if "criteria" not in eval_item:
                    raise ValueError(f"Evaluation {i} missing 'criteria' field")
                if "analysis" not in eval_item:
                    raise ValueError(f"Evaluation {i} missing 'analysis' field")
                if "rating" not in eval_item:
                    raise ValueError(f"Evaluation {i} missing 'rating' field")
                
                # CRITICAL: suggestions must always exist (even if empty)
                if "suggestions" not in eval_item:
                    logger.warning(f"Evaluation {i} missing 'suggestions', adding empty array")
                    eval_item["suggestions"] = []
                
                # Validate rating range
                rating = eval_item.get("rating", 0)
                if not isinstance(rating, int) or rating < 1 or rating > 5:
                    raise ValueError(f"Invalid rating {rating} in evaluation {i}, must be 1-5")
            
            return parsed
            
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse LLM JSON output: {e}\nRaw output: {output[:500]}")
            return {
                "error": "JSON parsing failed",
                "raw_output": output,
                "evaluations": []
            }
        except Exception as e:
            logger.error(f"Error in evaluation: {e}")
            return {
                "error": str(e),
                "evaluations": []
            }

    def _parse_json_response(self, output: str) -> Dict[str, Any]:
        """
        Parse JSON from LLM response, handling code blocks.
        """
        # Remove markdown code blocks if present
        if "```json" in output:
            output = output.split("```json")[1].split("```")[0].strip()
        elif "```" in output:
            # Try to extract content between first pair of ```
            parts = output.split("```")
            if len(parts) >= 3:
                output = parts[1].strip()
        
        # Parse JSON
        return json.loads(output)

    async def batch_evaluate(self, content_list: List[Dict[str, Any]], criteria: List[str] = None) -> List[Dict[str, Any]]:
        """
        Evaluate multiple content items.
        
        Args:
            content_list: List of content items to evaluate
            criteria: Criteria to use for all evaluations
        
        Returns:
            List of evaluation results, one per content item
        """
        results = []
        for content in content_list:
            result = await self.evaluate(content, criteria)
            results.append(result)
        return results

    async def evaluate_exam(
        self, 
        exam: Dict[str, Any], 
        rag_content: str = None,
        criteria: List[str] = None,
        mode: str = "quick"
    ) -> Dict[str, Any]:
        """
        Evaluate an entire exam with different evaluation modes.
        
        Args:
            exam: Exam content with structure:
                {
                    "type": "multiple_choice" or "exam",
                    "questions": [
                        {"question_number": 1, "question_text": "...", ...},
                        {"question_number": 2, ...},
                        ...
                    ]
                }
            rag_content: Optional RAG context (retrieved educational material)
            criteria: List of criteria names to evaluate. If None, evaluates all rubrics.
            mode: Evaluation mode:
                - "quick" (default): Only overall evaluation, cost-effective
                - "comprehensive": Overall + per-question + statistics
        
        Returns:
            Dict with structure:
            {
                "mode": str,                # Evaluation mode used
                "overall": {...},           # Overall exam assessment
                "per_question": [...],      # Individual assessments (comprehensive only)
                "statistics": {...}         # Summary statistics (comprehensive only)
            }
        
        Example:
            # Quick mode (default)
            result = await critic.evaluate_exam(exam, rag_content="...", mode="quick")
            
            # Comprehensive mode
            result = await critic.evaluate_exam(exam, rag_content="...", mode="comprehensive")
        """
        # Add rag_content to exam if provided
        if rag_content:
            exam["rag_content"] = rag_content
        
        all_questions = exam.get("questions", [])
        results = {"mode": mode}
        
        # 1. Overall exam evaluation (always performed)
        logger.info(f"[{mode.upper()} MODE] Evaluating exam with {len(all_questions)} questions at exam-level")
        results["overall"] = await self.evaluate(exam, criteria)
        
        # 2. Per-question evaluation (comprehensive mode only)
        if mode == "comprehensive":
            logger.info(f"[COMPREHENSIVE MODE] Evaluating all {len(all_questions)} questions individually")
            
            # Create evaluation tasks for all questions (concurrent)
            eval_tasks = []
            for q in all_questions:
                single_q = {
                    "type": "multiple_choice",
                    "questions": [q]
                }
                # Pass rag_content to individual questions too
                if rag_content:
                    single_q["rag_content"] = rag_content
                eval_tasks.append(self.evaluate(single_q, criteria))
            
            # Execute all evaluations concurrently
            question_results = await asyncio.gather(*eval_tasks, return_exceptions=True)
            
            # Format results
            results["per_question"] = []
            for i, (q, q_result) in enumerate(zip(all_questions, question_results)):
                if isinstance(q_result, Exception):
                    logger.error(f"Error evaluating question {q.get('question_number', i+1)}: {q_result}")
                    results["per_question"].append({
                        "question_type": q.get("question_type", "unknown"),
                        "question_number": q.get("question_number", i + 1),
                        "error": str(q_result),
                        "evaluations": []
                    })
                else:
                    results["per_question"].append({
                        "question_type": q.get("question_type", "unknown"),
                        "question_number": q.get("question_number", i + 1),
                        "evaluations": q_result.get("evaluations", [])
                    })
            
            # Compute statistics
            results["statistics"] = self._compute_exam_statistics(results["per_question"])
        else:
            logger.info(f"[QUICK MODE] Skipping per-question evaluation")
            results["per_question"] = []
            results["statistics"] = {
                "note": f"Per-question evaluation skipped in {mode} mode"
            }
        
        return results
    
    async def evaluate_single_question(
        self,
        question: Dict[str, Any],
        rag_content: str = None,
        criteria: List[str] = None
    ) -> Dict[str, Any]:
        """
        Evaluate a single question.
        
        This is a simplified API for unit testing individual questions.
        
        Args:
            question: Single question dict with structure:
                {
                    "question_number": 1,
                    "question_text": "...",
                    "options": {"A": "...", "B": "..."},
                    "correct_answer": "A",
                    "source": {"page_number": "...", "evidence": "..."}
                }
            rag_content: Optional RAG context (retrieved educational material)
            criteria: List of criteria names to evaluate. If None, evaluates all rubrics.
        
        Returns:
            Dict with evaluation results for the single question
        
        Example:
            result = await critic.evaluate_single_question(question, rag_content="...")
        """
        # Wrap question in expected format
        content = {
            "type": "multiple_choice",
            "questions": [question]
        }
        
        # Add rag_content if provided
        if rag_content:
            content["rag_content"] = rag_content
        
        # Evaluate using the core evaluate method
        return await self.evaluate(content, criteria)
    
    def _compute_exam_statistics(self, per_question_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Compute summary statistics from per-question evaluation results.
        
        Args:
            per_question_results: List of per-question evaluation results
        
        Returns:
            Dict containing statistics:
            {
                "total_questions": int,
                "avg_scores_by_criteria": {"Understandable": 3.5, ...},
                "min_scores_by_criteria": {"Understandable": 2, ...},
                "max_scores_by_criteria": {"Understandable": 5, ...},
                "questions_below_threshold": [1, 3, 5]  # Question numbers
            }
        """
        if not per_question_results:
            return {}
        
        # Aggregate scores by criteria
        criteria_scores = {}
        questions_below_threshold = []
        
        for q_result in per_question_results:
            if "error" in q_result or not q_result.get("evaluations"):
                continue
            
            question_num = q_result.get("question_number")
            has_low_score = False
            
            for eval_item in q_result["evaluations"]:
                criteria = eval_item.get("criteria")
                rating = eval_item.get("rating", 0)
                
                if criteria:
                    if criteria not in criteria_scores:
                        criteria_scores[criteria] = []
                    criteria_scores[criteria].append(rating)
                
                # Check if any score is below threshold
                if rating < self.threshold:
                    has_low_score = True
            
            if has_low_score and question_num:
                questions_below_threshold.append(question_num)
        
        # Compute statistics
        stats = {
            "total_questions": len(per_question_results),
            "avg_scores_by_criteria": {},
            "min_scores_by_criteria": {},
            "max_scores_by_criteria": {},
            "questions_below_threshold": questions_below_threshold
        }
        
        for criteria, scores in criteria_scores.items():
            if scores:
                stats["avg_scores_by_criteria"][criteria] = round(sum(scores) / len(scores), 2)
                stats["min_scores_by_criteria"][criteria] = min(scores)
                stats["max_scores_by_criteria"][criteria] = max(scores)
        
        return stats
