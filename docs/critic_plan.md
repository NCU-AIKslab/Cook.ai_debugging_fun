# Graph å…§å»º Critic Loop - å®Œæ•´å¯¦ç¾è¦åŠƒ

> **æ—¥æœŸ**: 2025-12-09  
> **ç‹€æ…‹**: Ready for Implementation

---

## ç›®éŒ„

1. [ç›®æ¨™èˆ‡æ ¸å¿ƒæµç¨‹](#ç›®æ¨™èˆ‡æ ¸å¿ƒæµç¨‹)
2. [State ç®¡ç†](#state-ç®¡ç†)
3. [Multi-Critic æ¶æ§‹](#multi-critic-æ¶æ§‹)
4. [Skill å¯æ“´å……æ€§è¨­è¨ˆ](#skill-å¯æ“´å……æ€§è¨­è¨ˆ)
5. [çµ±ä¸€çš„ Refinement æ©Ÿåˆ¶](#çµ±ä¸€çš„-refinement-æ©Ÿåˆ¶)
6. [æ•¸æ“šè¨˜éŒ„å„ªåŒ–](#æ•¸æ“šè¨˜éŒ„å„ªåŒ–)
7. [API è¼¸å‡ºè¨­è¨ˆ](#api-è¼¸å‡ºè¨­è¨ˆ)
8. [å‰ç«¯å¯è¦–åŒ–æ”¯æŒ](#å‰ç«¯å¯è¦–åŒ–æ”¯æŒ)
9. [æ€§èƒ½å„ªåŒ–](#æ€§èƒ½å„ªåŒ–)
10. [å¯¦ç¾æ­¥é©Ÿ](#å¯¦ç¾æ­¥é©Ÿ)
11. [é…ç½®åƒæ•¸](#é…ç½®åƒæ•¸)

---

## ç›®æ¨™èˆ‡æ ¸å¿ƒæµç¨‹

### ç›®æ¨™

å°‡ `test_critic_workflow` å‡ç´šç‚ºå®Œæ•´çš„ Graph å…§å»º Critic Loopï¼Œå¯¦ç¾ï¼š
- âœ… è‡ªå‹•å“è³ªæª¢æŸ¥èˆ‡è¿­ä»£æ”¹é€²
- âœ… **å¤š Critic æ”¯æŒ**ï¼ˆFact + Qualityï¼Œå¯é¸ï¼‰
- âœ… å®Œæ•´çš„å¯¦é©—æ•¸æ“šè¨˜éŒ„èˆ‡å‰ç«¯å¯è¦–åŒ–æ”¯æŒ
- âœ… æ™ºèƒ½ RAG å¿«å–èˆ‡éƒ¨åˆ†é‡æ–°ç”Ÿæˆå„ªåŒ–
- âœ… **å¯æ“´å……çš„ Skill æ¶æ§‹**ï¼ˆæ”¯æŒæœªä¾†æ–°å¢ skillsï¼‰

### æ ¸å¿ƒæµç¨‹è¨­è¨ˆ

```
æ•™å¸«è¼¸å…¥æŒ‡ä»¤ 
  â†“
Router (è·¯ç”±åˆ°æŠ€èƒ½)
  â†“
Skill (exam/summary ç”Ÿæˆ) - Iteration 1
  â†“
Fact Critic (äº‹å¯¦æ€§æª¢æŸ¥) â† ç”¨æˆ¶å¯é¸ âš ï¸ å…ˆåŸ·è¡Œ
  â†“
Quality Critic (å“è³ªè©•ä¼°) â† ç”¨æˆ¶å¯é¸
  â†“
Decision Point (should_continue_from_critic)
  â”œâ”€ é€šé (is_passed=True) â†’ Aggregate Output
  â”‚   â””â”€ è¿”å›: æœ€çµ‚çµæœ + å®Œæ•´è©•åˆ†æ­·å² + æ”¹é€²æ‘˜è¦
  â”œâ”€ å¤±æ•—ä½†æœªé”ä¸Šé™ â†’ å›åˆ° Skill (å¸¶ feedback)
  â”‚   â””â”€ Skill æ¥æ”¶ feedbackï¼Œé€²è¡Œæ™ºèƒ½æ”¹é€² â†’ Iteration 2 â†’ Critic
  â””â”€ å¤±æ•—ä¸”é”ä¸Šé™ â†’ Aggregate Output
      â””â”€ è¿”å›: æœ€çµ‚çµæœ + å®Œæ•´è©•åˆ† + å¤±æ•—åˆ†æèˆ‡å»ºè­°
```

#### é—œéµè¨­è¨ˆæ±ºç­–

1. **Critic åŸ·è¡Œé †åº**: Fact â†’ Quality  
   - **åŸå› **: äº‹å¯¦æ­£ç¢ºæ€§æ˜¯åŸºç¤ï¼Œå…ˆç¢ºä¿å…§å®¹äº‹å¯¦ç„¡èª¤ï¼Œå†è©•ä¼°å“è³ª

2. **å¤±æ•—æ™‚å®Œæ•´è¼¸å‡º**  
   - ç„¡è«–é€šéæˆ–å¤±æ•—ï¼Œéƒ½è¿”å›å®Œæ•´çµæœ + åˆ†æ•¸ + å»ºè­°
   - é”ä¸Šé™æ™‚æ¨™è¨˜ç‚º `partial_success`ï¼Œä½†ä»æä¾›æœ€çµ‚å…§å®¹

3. **çµ±ä¸€çš„ Refinement è¨­è¨ˆ**  
   - ä¸æ–°å¢ç¨ç«‹çš„ refinement nodes
   - åœ¨ç¾æœ‰ generation nodes ä¸­åŠ å…¥ refinement é‚è¼¯
   - æ ¹æ“š `is_refinement` æ¨™èªŒæ±ºå®šè¡Œç‚º

---

## State ç®¡ç†

### TeacherAgentState æ–°å¢/ä¿®æ”¹æ¬„ä½

```python
# graph.py - TeacherAgentState
class TeacherAgentState(TypedDict):
    # ... ç¾æœ‰æ¬„ä½ ...
    
    # === Critic é…ç½® ===
    enabled_critics: List[str]  # ["fact", "quality"] - ç”¨æˆ¶å¯é¸æ“‡å•Ÿç”¨å“ªäº› critic
    critic_mode: str  # "quick" or "comprehensive"
    
    # === è¿­ä»£ç®¡ç† ===
    iteration_count: int  # ç•¶å‰è¿­ä»£æ¬¡æ•¸ (é è¨­ 1)
    max_iterations: int  # æœ€å¤§è¿­ä»£æ¬¡æ•¸ (é è¨­ 3)
    
    # === Critic Feedback (çµ±ä¸€æ ¼å¼) ===
    critic_feedback: List[Dict]  # æ¯æ¬¡è¿­ä»£çš„è©•ä¼°çµæœ
    # æ ¼å¼ç¤ºä¾‹:
    # [
    #   {
    #     "iteration": 1,
    #     "critics": {
    #       "fact": {"is_passed": True, "scores": {...}, "feedback": {...}},
    #       "quality": {"is_passed": False, "scores": {...}, "feedback": {...}}
    #     },
    #     "overall_passed": False,  # éœ€å…¨éƒ¨é€šéæ‰ç‚º True
    #     "timestamp": "2025-12-09T15:00:00+08:00"
    #   }
    # ]
    
    critic_passed: bool  # æœ€æ–°ä¸€æ¬¡æ˜¯å¦å…¨éƒ¨é€šé
    critic_metrics: Dict  # æœ€æ–°ä¸€æ¬¡çš„ç¶œåˆæŒ‡æ¨™
    
    # === ç‰ˆæœ¬è¿½è¹¤ ===
    generation_history: List[Dict]  # æ¯å€‹ç‰ˆæœ¬çš„ç”Ÿæˆå…§å®¹
    # æ ¼å¼ç¤ºä¾‹:
    # [
    #   {
    #     "iteration": 1,
    #     "content": [...],
    #     "task_id": 1234,
    #     "timestamp": "2025-12-09T15:00:00+08:00"
    #   }
    # ]
    
    # === RAG å¿«å– âœ… ===
    rag_cache: Dict  # å¿«å– RAG æª¢ç´¢çµæœï¼Œé¿å…é‡è¤‡æª¢ç´¢
    # æ ¼å¼:
    # {
    #   "text_chunks": [...],
    #   "page_content": [...],
    #   "cached_at": "2025-12-09T15:00:00+08:00"
    # }
```

### Sub-graph States (ExamGenerationState / SummarizationState)

```python
# é€™äº› sub-graph states ä¹Ÿéœ€è¦æ”¯æŒ refinement
class ExamGenerationState(TypedDict):
    # ... ç¾æœ‰æ¬„ä½ ...
    
    # === Refinement æ”¯æŒ ===
    is_refinement: bool  # æ˜¯å¦ç‚º refinement iteration
    refinement_feedback: Dict  # Critic æä¾›çš„å…·é«”æ”¹é€²å»ºè­°
    previous_content: List[Dict]  # ä¸Šä¸€ç‰ˆæœ¬çš„å…§å®¹ï¼ˆç”¨æ–¼å°æ¯”ï¼‰
    
    # === RAG å¿«å– ===
    use_cached_rag: bool  # æ˜¯å¦ä½¿ç”¨å¿«å–çš„ RAG çµæœ
    cached_rag_data: Dict  # å¾ parent state å‚³å…¥çš„å¿«å–æ•¸æ“š
```

---

## Multi-Critic æ¶æ§‹

### Critic åŸ·è¡Œæµç¨‹

```python
# graph.py - run_critics_node

@log_task(
    agent_name="critics_evaluation",
    task_description="Run enabled critics sequentially (fact â†’ quality)",
    input_extractor=lambda state: {
        "enabled_critics": state.get("enabled_critics", []),
        "iteration": state.get("iteration_count", 1)
    }
)
async def run_critics_node(state: TeacherAgentState) -> dict:
    """
    æ ¹æ“š enabled_critics ä¾åºåŸ·è¡Œå°æ‡‰çš„ critic
    
    åŸ·è¡Œé †åº: Fact Critic â†’ Quality Critic âš ï¸
    åŸå› : äº‹å¯¦æ­£ç¢ºæ€§æ˜¯åŸºç¤ï¼Œå…ˆç¢ºä¿å…§å®¹äº‹å¯¦ç„¡èª¤
    
    åŸ·è¡Œé‚è¼¯:
    1. ä¾åºåŸ·è¡Œå•Ÿç”¨çš„ critics
    2. æ”¶é›†æ‰€æœ‰è©•ä¼°çµæœ
    3. ç¶œåˆåˆ¤æ–·æ˜¯å¦é€šéï¼ˆéœ€å…¨éƒ¨é€šéæ‰ç®—é€šéï¼‰
    """
    enabled_critics = state.get("enabled_critics", ["quality"])
    iteration = state.get("iteration_count", 1)
    
    critics_results = {}
    overall_passed = True
    
    # 1. åŸ·è¡Œ Fact Critic (å„ªå…ˆ) âš ï¸
    if "fact" in enabled_critics:
        logger.info("ğŸ” Running Fact Critic...")
        fact_result = await run_fact_critic(state)
        critics_results["fact"] = fact_result
        if not fact_result.get("is_passed"):
            overall_passed = False
            logger.warning("âŒ Fact Critic failed")
    
    # 2. åŸ·è¡Œ Quality Critic
    if "quality" in enabled_critics:
        logger.info("âœ¨ Running Quality Critic...")
        quality_result = await run_quality_critic(state)
        critics_results["quality"] = quality_result
        if not quality_result.get("is_passed"):
            overall_passed = False
            logger.warning("âŒ Quality Critic failed")
    
    # 3. æ§‹å»ºç¶œåˆ feedback
    combined_feedback = {
        "iteration": iteration,
        "critics": critics_results,
        "overall_passed": overall_passed,
        "timestamp": datetime.now(TAIPEI_TZ).isoformat()
    }
    
    # 4. æ›´æ–° feedback history
    feedback_history = state.get("critic_feedback", [])
    feedback_history.append(combined_feedback)
    
    logger.info(f"ğŸ“Š Iteration {iteration} evaluation complete: {'âœ… Passed' if overall_passed else 'âŒ Failed'}")
    
    return {
        "critic_passed": overall_passed,
        "critic_feedback": feedback_history,
        "critic_metrics": _aggregate_metrics(critics_results)
    }
```

### Critic Helper Functions

```python
async def run_quality_critic(state: TeacherAgentState) -> dict:
    """
    åŸ·è¡Œ Quality Critic - å¾ç¾æœ‰é‚è¼¯æŠ½å–
    
    Returns:
        {
            "is_passed": bool,
            "scores": {...},
            "feedback": {...},
            "failed_criteria": [...]
        }
    """
    # å¾ç¾æœ‰çš„ quality_critic_node æŠ½å–é‚è¼¯
    pass

async def run_fact_critic(state: TeacherAgentState) -> dict:
    """
    åŸ·è¡Œ Fact Critic - æœªä¾†å¯¦ç¾
    
    æª¢æŸ¥å…§å®¹çš„äº‹å¯¦æ€§:
    - ç­”æ¡ˆèˆ‡è­‰æ“šæ˜¯å¦ä¸€è‡´
    - å¼•ç”¨ä¾†æºæ˜¯å¦æ­£ç¢º
    - æ•¸æ“šæ˜¯å¦æº–ç¢º
    
    Returns:
        {
            "is_passed": bool,
            "scores": {...},
            "feedback": {...},
            "factual_errors": [...]
        }
    """
    # TODO: æœªä¾†å¯¦ç¾
    # æš«æ™‚è¿”å› pass
    return {"is_passed": True, "scores": {}, "feedback": {}}

def _aggregate_metrics(critics_results: Dict) -> dict:
    """
    ç¶œåˆå¤šå€‹ critic çš„æŒ‡æ¨™
    
    Returns:
        {
            "is_passed": bool,
            "failed_critics": ["quality"],  # å¤±æ•—çš„ critics
            "failed_criteria": ["factual_accuracy", "clarity"],  # æ‰€æœ‰å¤±æ•—çš„æ¨™æº–
            "overall_scores": {...},  # ç¶œåˆåˆ†æ•¸
            "improvement_suggestions": "..."  # ç¶œåˆå»ºè­°
        }
    """
    is_passed = all(r.get("is_passed", False) for r in critics_results.values())
    
    failed_critics = [
        name for name, result in critics_results.items()
        if not result.get("is_passed", False)
    ]
    
    failed_criteria = []
    for result in critics_results.values():
        failed_criteria.extend(result.get("failed_criteria", []))
    
    # å»é‡
    failed_criteria = list(set(failed_criteria))
    
    return {
        "is_passed": is_passed,
        "failed_critics": failed_critics,
        "failed_criteria": failed_criteria,
        "overall_scores": {
            name: result.get("scores", {})
            for name, result in critics_results.items()
        },
        "improvement_suggestions": _combine_suggestions(critics_results)
    }

def _combine_suggestions(critics_results: Dict) -> str:
    """åˆä½µæ‰€æœ‰ critics çš„å»ºè­°"""
    suggestions = []
    for name, result in critics_results.items():
        if not result.get("is_passed"):
            suggestions.append(f"[{name.upper()}] {result.get('feedback', {}).get('overall_feedback', '')}")
    return "\n".join(suggestions)
```

---

## Skill å¯æ“´å……æ€§è¨­è¨ˆ

### å•é¡Œ

ç›®å‰çš„è¨­è¨ˆåªè€ƒæ…®äº† exam å’Œ summaryï¼Œæ²’æœ‰è€ƒæ…®ï¼š
- `general_chat_skill` ä¸éœ€è¦ critic è©•ä¼°
- æœªä¾†å¯èƒ½æ–°å¢çš„ skills

### è§£æ±ºæ–¹æ¡ˆï¼šSkill Capability ç³»çµ±

#### A. Skill é…ç½®å®šç¾©

```python
# skills/base.py (æ–°å¢æª”æ¡ˆ)

from pydantic import BaseModel
from typing import Literal

class SkillCapability(BaseModel):
    """å®šç¾© skill çš„èƒ½åŠ›èˆ‡ç‰¹æ€§"""
    name: str
    supports_refinement: bool  # æ˜¯å¦æ”¯æŒ refinement
    supports_critic: bool  # æ˜¯å¦éœ€è¦ critic è©•ä¼°
    refinement_strategy: Literal["partial", "full", "none"]
    # - "partial": å¯ä»¥åªæ”¹éƒ¨åˆ†å…§å®¹ï¼ˆå¦‚åªæ”¹å¤±æ•—çš„é¡Œç›®ï¼‰
    # - "full": å¿…é ˆå®Œæ•´é‡æ–°ç”Ÿæˆï¼ˆå¦‚ summaryï¼‰
    # - "none": ä¸æ”¯æŒ refinement

# æ‰€æœ‰ skills çš„é…ç½®
SKILL_CONFIGS = {
    "exam_generation_skill": SkillCapability(
        name="exam_generation",
        supports_refinement=True,
        supports_critic=True,
        refinement_strategy="partial"  # å¯ä»¥åªæ”¹å¤±æ•—çš„é¡Œç›®
    ),
    "summarization_skill": SkillCapability(
        name="summarization",
        supports_refinement=True,
        supports_critic=True,
        refinement_strategy="full"  # å¿…é ˆå®Œæ•´é‡æ–°ç”Ÿæˆ
    ),
    "general_chat_skill": SkillCapability(
        name="general_chat",
        supports_refinement=False,  # å°è©±ä¸æ”¯æŒæ”¹é€²
        supports_critic=False,  # ä¸éœ€è¦è©•ä¼°
        refinement_strategy="none"
    )
    # æœªä¾†æ–°å¢ skill æ™‚ï¼Œåœ¨é€™è£¡æ·»åŠ é…ç½®å³å¯
}
```

#### B. å‹•æ…‹çš„ Graph Edges

```python
# graph.py

def build_skill_to_critic_edges(builder: StateGraph, skill_configs: Dict):
    """
    æ ¹æ“š skill é…ç½®å‹•æ…‹å»ºç«‹ edges
    
    å„ªé»:
    - æ–°å¢ skill æ™‚ä¸éœ€è¦ä¿®æ”¹ graph å»ºæ§‹é‚è¼¯
    - é…ç½®é›†ä¸­ç®¡ç†
    - æ˜“æ–¼ç¶­è­·
    """
    for skill_name, config in skill_configs.items():
        if config.supports_critic:
            # éœ€è¦ critic çš„ skill â†’ critics
            builder.add_edge(skill_name, "critics")
            logger.info(f"âœ“ {skill_name} â†’ critics")
        else:
            # ä¸éœ€è¦ critic çš„ skill â†’ aggregate_output
            builder.add_edge(skill_name, "aggregate_output")
            logger.info(f"âœ“ {skill_name} â†’ aggregate_output (bypass critic)")

# ä½¿ç”¨
from backend.app.agents.teacher_agent.skills.base import SKILL_CONFIGS
build_skill_to_critic_edges(builder, SKILL_CONFIGS)
```

#### C. å‹•æ…‹çš„ Conditional Edge

```python
def should_continue_from_critic(state: TeacherAgentState) -> str:
    """
    æ±ºå®š critic ä¹‹å¾Œçš„æµå‘
    
    å‹•æ…‹æª¢æŸ¥ skill æ˜¯å¦æ”¯æŒ refinement
    """
    # 1. æª¢æŸ¥æ˜¯å¦é€šé
    if state.get("critic_passed", False):
        logger.info("âœ… All critics passed, proceeding to output")
        return "aggregate_output"
    
    # 2. æª¢æŸ¥è¿­ä»£æ¬¡æ•¸
    iteration = state.get("iteration_count", 1)
    max_iter = state.get("max_iterations", 3)
    
    if iteration >= max_iter:
        logger.warning(f"âš ï¸ Max iterations ({max_iter}) reached")
        logger.info("Proceeding to output with partial success status")
        return "aggregate_output"
    
    # 3. å¢åŠ è¿­ä»£è¨ˆæ•¸
    state["iteration_count"] = iteration + 1
    
    # 4. å‹•æ…‹æª¢æŸ¥ skill æ˜¯å¦æ”¯æŒ refinement âœ…
    last_skill = state.get("next_node")
    skill_config = SKILL_CONFIGS.get(last_skill)
    
    if skill_config and skill_config.supports_refinement:
        logger.info(f"ğŸ”„ Iteration {iteration + 1}: Returning to {last_skill}")
        logger.info(f"   Strategy: {skill_config.refinement_strategy}")
        return last_skill
    else:
        logger.warning(f"âš ï¸ Skill {last_skill} doesn't support refinement")
        logger.info("Ending loop and proceeding to output")
        return "aggregate_output"
```

#### D. æ¢ä»¶å¼ Critic Edgesï¼ˆæœªä¾†å„ªåŒ–ï¼‰

```python
# æœªä¾†å¯ä»¥é€²ä¸€æ­¥å‹•æ…‹åŒ–
builder.add_conditional_edges(
    "critics",
    should_continue_from_critic,
    {
        "aggregate_output": "aggregate_output",
        **{
            skill_name: skill_name
            for skill_name, config in SKILL_CONFIGS.items()
            if config.supports_refinement
        }
    }
)
```

---

## çµ±ä¸€çš„ Refinement æ©Ÿåˆ¶

### è¨­è¨ˆç†å¿µ

- âœ… **ä¸æ–°å¢å°ˆé–€çš„ refinement nodes**
- âœ… **åœ¨ç¾æœ‰çš„ generation nodes ä¸­åŠ å…¥ refinement é‚è¼¯**
- âœ… æ ¹æ“š `is_refinement` æ¨™èªŒæ±ºå®šåŸ·è¡Œåˆæ¬¡ç”Ÿæˆæˆ–æ”¹é€²
- âœ… æœªä¾†å¯è®“æ•™å¸«ç›´æ¥è¼¸å…¥å»ºè­°é€²å…¥ refinement æ¨¡å¼

### A. Exam Generation Skill æ”¹é€²

#### 1. RAG æª¢ç´¢ with å¿«å–

```python
# exam_nodes.py - retrieve_chunks_node

@log_task(...)
def retrieve_chunks_node(state: ExamGenerationState) -> dict:
    """
    RAG æª¢ç´¢ with å¿«å–æ”¯æŒ âœ…
    """
    # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨å¿«å–
    if state.get("use_cached_rag") and state.get("cached_rag_data"):
        logger.info("ğŸ“¦ Using cached RAG results (saved tokens & time)")
        cached_data = state["cached_rag_data"]
        
        return {
            "retrieved_text_chunks": cached_data["text_chunks"],
            "retrieved_page_content": cached_data["page_content"],
            "generation_plan": [],
            "final_generated_content": [],
            "generation_errors": [],
            "parent_task_id": state["current_task_id"]
        }
    
    # å¦å‰‡æ­£å¸¸æª¢ç´¢
    try:
        logger.info("ğŸ” Retrieving RAG context from database...")
        rag_results = rag_agent.search(
            user_prompt=state["query"],
            unique_content_id=state["unique_content_id"]
        )
        log_task_sources(state["current_task_id"], rag_results["text_chunks"])
        
        return {
            "retrieved_text_chunks": rag_results["text_chunks"],
            "retrieved_page_content": rag_results["page_content"],
            "generation_plan": [],
            "final_generated_content": [],
            "generation_errors": [],
            "parent_task_id": state["current_task_id"]
        }
    except Exception as e:
        return {"error": f"Failed to retrieve context: {str(e)}"}
```

#### 2. Plan Generation with Refinement

```python
@log_task(
    agent_name="plan_or_refine_exam",
    task_description="Create generation plan or refinement plan",
    input_extractor=lambda state: {
        "query": state.get("query"),
        "is_refinement": state.get("is_refinement", False),
        "iteration": state.get("iteration_count", 1)
    }
)
def plan_generation_tasks_node(state: ExamGenerationState) -> dict:
    """
    çµ±ä¸€è™•ç†åˆæ¬¡ç”Ÿæˆå’Œ refinement
    """
    # === Refinement Mode === âœ…
    if state.get("is_refinement"):
        feedback = state.get("refinement_feedback", {})
        previous_content = state.get("previous_content", [])
        
        iteration = state.get("iteration_count", 1)
        logger.info(f"ğŸ”§ Refinement mode: Iteration {iteration}")
        logger.info(f"   Previous content: {len(previous_content)} sections")
        
        # è§£æ feedbackï¼Œæ‰¾å‡ºéœ€è¦æ”¹é€²çš„é¡Œç›®
        failed_questions = _extract_failed_questions(feedback, previous_content)
        
        if not failed_questions:
            # æ²’æœ‰å…·é«”å¤±æ•—é¡Œç›®ï¼Œå…¨éƒ¨é‡æ–°ç”Ÿæˆ
            logger.info("âš ï¸ No specific failed questions identified, regenerating all")
            return _create_initial_plan(state["query"], state)
        
        # åªé‡æ–°ç”Ÿæˆå¤±æ•—çš„é¡Œç›® âœ…
        logger.info(f"ğŸ“‹ Creating refinement plan for {len(failed_questions)} questions")
        refinement_plan = _create_refinement_plan(
            failed_questions=failed_questions,
            feedback=feedback
        )
        
        llm = get_llm()
        return {
            "generation_plan": refinement_plan,
            "parent_task_id": state["current_task_id"],
            "model_name": llm.model_name
        }
    
    # === Initial Generation Mode ===
    else:
        logger.info("âœ¨ Initial generation mode")
        return _create_initial_plan(state["query"], state)

def _create_initial_plan(query: str, state: ExamGenerationState) -> dict:
    """åˆæ¬¡ç”Ÿæˆçš„è¨ˆåŠƒå‰µå»ºé‚è¼¯ï¼ˆç¾æœ‰é‚è¼¯ï¼‰"""
    # ... ç¾æœ‰çš„ plan generation é‚è¼¯ ...
    pass
```

#### 3. Helper Functions

```python
def _extract_failed_questions(feedback: Dict, previous_content: List) -> List[Dict]:
    """
    å¾ feedback ä¸­æå–éœ€è¦æ”¹é€²çš„é¡Œç›®
    
    Args:
        feedback: Critic è¿”å›çš„ feedback
        previous_content: ä¸Šä¸€ç‰ˆæœ¬çš„å…§å®¹
    
    Returns:
        [
            {
                "question_index": 1,
                "question_type": "multiple_choice",
                "original_question": {...},
                "issues": ["factual_accuracy", "clarity"],
                "suggestions": "é¡Œç›®1çš„ç­”æ¡ˆèˆ‡è­‰æ“šè¡çª..."
            }
        ]
    """
    failed_questions = []
    
    # è§£æ per-question feedback
    feedback_items = feedback.get("critics", {}).get("quality", {}).get("feedback", {}).get("per_question", [])
    
    for item in feedback_items:
        if item.get("status") == "fail":
            question_index = item["question_index"]
            failed_questions.append({
                "question_index": question_index,
                "question_type": item.get("question_type"),
                "original_question": _find_question_by_index(
                    question_index, previous_content
                ),
                "issues": item.get("failed_criteria", []),
                "suggestions": item.get("improvement_suggestions", "")
            })
    
    return failed_questions

def _find_question_by_index(index: int, content: List) -> Dict:
    """å¾å…§å®¹ä¸­æ‰¾å‡ºæŒ‡å®šç´¢å¼•çš„é¡Œç›®"""
    for section in content:
        if section.get("type") in ["multiple_choice", "short_answer", "true_false"]:
            for q in section.get("questions", []):
                if q.get("question_number") == index:
                    return q
    return {}

def _create_refinement_plan(failed_questions: List, feedback: Dict) -> List[Dict]:
    """
    ç‚ºå¤±æ•—çš„é¡Œç›®å‰µå»ºæ”¹é€²è¨ˆåŠƒ
    
    ç­–ç•¥: æŒ‰é¡Œç›®é¡å‹åˆ†çµ„ï¼Œç‚ºæ¯ç¨®é¡å‹å‰µå»ºä¸€å€‹ refinement task
    """
    refinement_tasks = []
    
    # æŒ‰é¡Œç›®é¡å‹åˆ†çµ„
    by_type = {}
    for q in failed_questions:
        q_type = q["question_type"]
        if q_type not in by_type:
            by_type[q_type] = []
        by_type[q_type].append(q)
    
    # ç‚ºæ¯ç¨®é¡å‹å‰µå»ºä¸€å€‹ task
    for q_type, questions in by_type.items():
        refinement_tasks.append({
            "type": f"refine_{q_type}",  # "refine_multiple_choice"
            "count": len(questions),
            "questions_to_refine": questions,
            "feedback_summary": feedback.get("improvement_suggestions", "")
        })
    
    return refinement_tasks
```

#### 4. Generation with Refinement

```python
def _generic_generate_question(state: ExamGenerationState, task_type_name: str) -> dict:
    """
    çµ±ä¸€çš„é¡Œç›®ç”Ÿæˆé‚è¼¯ï¼Œæ”¯æŒ refinement
    """
    current_task = state.get("current_task", {})
    
    # === Refinement Mode === âœ…
    if current_task.get("type", "").startswith("refine_"):
        logger.info(f"ğŸ”§ Refining {task_type_name} questions")
        
        questions_to_refine = current_task.get("questions_to_refine", [])
        feedback_summary = current_task.get("feedback_summary", "")
        
        # æ§‹å»º refinement prompt
        system_prompt = """You are refining exam questions based on critic feedback.
Your goal is to address all identified issues while maintaining the question structure."""

        issues_list = []
        for q in questions_to_refine:
            issues_list.append(f"- Question {q['question_index']}: {', '.join(q['issues'])}")
        
        human_prompt = f"""
**REFINEMENT TASK**

Previous questions had the following issues:
{chr(10).join(issues_list)}

**Overall Feedback:**
{feedback_summary}

**Questions to improve:**
{json.dumps([q["original_question"] for q in questions_to_refine], ensure_ascii=False, indent=2)}

**Instructions:**
Please regenerate these questions, addressing ALL the issues mentioned above.
Maintain the same format (multiple choice/short answer/true-false) but improve:
{', '.join(set(issue for q in questions_to_refine for issue in q["issues"]))}

Ensure:
1. Factual accuracy with proper evidence
2. Clear and unambiguous wording
3. Proper citation of sources
4. All output in Traditional Chinese
"""
        
        # å‘¼å« LLM æ”¹é€²ï¼ˆé¡ä¼¼ç¾æœ‰é‚è¼¯ï¼‰
        llm = get_llm()
        # ... æ§‹å»º messages, å‘¼å« tool_llm.invoke, è§£æçµæœ ...
        # ... è¨ˆç®— tokens å’Œ cost ...
        
        return {
            "final_generated_content": refined_content,
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "estimated_cost_usd": estimated_cost,
            "model_name": model_name
        }
    
    # === Initial Generation Mode ===
    else:
        # ç¾æœ‰é‚è¼¯ä¿æŒä¸è®Š
        # ... (ç¾æœ‰çš„ç”Ÿæˆé‚è¼¯) ...
        pass
```

### B. Summarization Skill æ”¹é€²

Summary æ²’æœ‰ã€Œéƒ¨åˆ†æ”¹é€²ã€çš„æ¦‚å¿µï¼Œ**æ¯æ¬¡éƒ½å®Œæ•´é‡æ–°ç”Ÿæˆ**ã€‚

```python
@log_task(...)
def summarize_node(state: SummarizationState) -> dict:
    """
    ç”Ÿæˆæ‘˜è¦ with refinement æ”¯æŒ
    """
    # === Refinement Mode === âœ…
    if state.get("is_refinement"):
        feedback = state.get("refinement_feedback", {})
        previous_summary = state.get("previous_content")
        
        logger.info("ğŸ”§ Refinement mode: Regenerating entire summary")
        logger.info("   (Summary is a whole, cannot partially refine)")
        
        # æ§‹å»º refinement promptï¼ˆåŒ…å«ä¹‹å‰çš„æ‘˜è¦å’Œ feedbackï¼‰
        system_prompt = """You are refining a summary based on critic feedback.
Your goal is to improve the summary while addressing all identified issues."""
        
        human_prompt = f"""
**REFINEMENT TASK**

**Previous summary:**
{json.dumps(previous_summary, ensure_ascii=False, indent=2)}

**Feedback from critics:**
{json.dumps(feedback.get("improvement_suggestions", ""), ensure_ascii=False, indent=2)}

**Issues to fix:**
{', '.join(feedback.get("failed_criteria", []))}

**Instructions:**
Please regenerate the ENTIRE summary, addressing all the issues mentioned.
Ensure:
1. All feedback points are addressed
2. Structure is clear and logical
3. Key points are comprehensive
4. Output in Traditional Chinese
5. Use the SummaryReport tool to format your response
"""
        
        # å‘¼å« LLMï¼ˆé‚è¼¯é¡ä¼¼åˆæ¬¡ç”Ÿæˆï¼Œä½† prompt ä¸åŒï¼‰
        llm = get_llm()
        # ... æ§‹å»º messages, å‘¼å« tool_llm.invoke, è§£æçµæœ ...
        # ... è¨ˆç®— tokens å’Œ cost ...
        
        return {
            "final_generated_content": refined_summary,
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "estimated_cost_usd": estimated_cost,
            "model_name": model_name,
            "parent_task_id": state.get("current_task_id")
        }
    
    # === Initial Generation Mode ===
    else:
        # ç¾æœ‰é‚è¼¯ä¿æŒä¸è®Š
        # ... (ç¾æœ‰çš„ç”Ÿæˆé‚è¼¯) ...
        pass
```

### C. General Chat Skill

**ä¸æ”¯æŒ refinement**ï¼Œå› ç‚ºæ˜¯å°è©±æ€§è³ªã€‚

```python
# general_chat_node ä¿æŒä¸è®Š
# åœ¨ graph ä¸­ç›´æ¥ bypass critics
builder.add_edge("general_chat_skill", "aggregate_output")
```

---

## Skill Wrapper æ”¹é€²ï¼ˆå‚³é Refinement Contextï¼‰

```python
# graph.py

@log_task(...)
def exam_skill_node(state: TeacherAgentState) -> dict:
    """
    Exam skill wrapper - å‚³é refinement context å’Œ RAG å¿«å–
    """
    try:
        iteration = state.get("iteration_count", 1)
        is_refinement = iteration > 1
        
        logger.info(f"{'ğŸ”§ Refinement' if is_refinement else 'âœ¨ Initial'} call to exam generation skill")
        
        skill_input = {
            "job_id": state["job_id"],
            "query": state["user_query"],
            "unique_content_id": state["unique_content_id"],
            "parent_task_id": state.get("current_task_id"),
            
            # Refinement æ”¯æŒ
            "is_refinement": is_refinement,
            "iteration_count": iteration,
        }
        
        # å¦‚æœæ˜¯ refinementï¼Œå‚³é feedback å’Œ previous content
        if is_refinement:
            logger.info("   Passing feedback and previous content to skill")
            latest_feedback = state.get("critic_feedback", [])[-1]
            skill_input["refinement_feedback"] = latest_feedback
            
            # å¾ generation_history å–å¾—ä¸Šä¸€ç‰ˆæœ¬
            history = state.get("generation_history", [])
            if history:
                skill_input["previous_content"] = history[-1]["content"]
        
        # RAG å¿«å– âœ…
        if iteration > 1 and state.get("rag_cache"):
            logger.info("   Using cached RAG results")
            skill_input["use_cached_rag"] = True
            skill_input["cached_rag_data"] = state["rag_cache"]
        
        final_skill_state = exam_generator_app.invoke(skill_input)
        
        if final_skill_state.get("error"):
            raise Exception(f"Exam generator failed: {final_skill_state['error']}")
        
        # è¨˜éŒ„åˆ° generation_history
        generated_content = final_skill_state.get("final_generated_content")
        history = state.get("generation_history", [])
        history.append({
            "iteration": iteration,
            "content": generated_content,
            "task_id": state.get("current_task_id"),
            "timestamp": datetime.now(TAIPEI_TZ).isoformat()
        })
        
        # å»ºç«‹ RAG å¿«å–ï¼ˆç¬¬ä¸€æ¬¡è¿­ä»£ï¼‰
        rag_cache = state.get("rag_cache")
        if not rag_cache and final_skill_state.get("retrieved_text_chunks"):
            logger.info("ğŸ’¾ Caching RAG results for future iterations")
            rag_cache = {
                "text_chunks": final_skill_state["retrieved_text_chunks"],
                "page_content": final_skill_state["retrieved_page_content"],
                "cached_at": datetime.now(TAIPEI_TZ).isoformat()
            }
        
        return {
            "final_result": final_skill_state,
            "final_generated_content": generated_content,
            "generation_history": history,
            "rag_cache": rag_cache,
            "parent_task_id": state.get("current_task_id")
        }
    
    except Exception as e:
        logger.error(f"Exam skill node failed: {e}")
        return {"error": str(e)}
```

åŒæ¨£çš„é‚è¼¯æ‡‰ç”¨æ–¼ `summarization_skill_node`ã€‚

---

## æ•¸æ“šè¨˜éŒ„å„ªåŒ–

### A. è¿­ä»£æ¬¡æ•¸è¨˜éŒ„ âœ…

åœ¨ `create_task` èª¿ç”¨æ™‚å‚³é `iteration_number`ï¼š

```python
# db_logger.py - log_task decorator

# Async wrapper
task_id = create_task(
    job_id=state["job_id"],
    agent_name=agent_name,
    task_description=task_description,
    task_input=extracted_input,
    parent_task_id=parent_task_id,
    iteration_number=state.get("iteration_count", 1),  # âœ… åŠ é€™è¡Œ
    model_name=None  # å°‡åœ¨ update_task æ™‚æ›´æ–°
)

# Sync wrapper åŒæ¨£ä¿®æ”¹
```

### B. Evaluation é—œè¯

**ç•¶å‰ç‹€æ…‹**: 
- `task_evaluations.task_id` â†’ critic task çš„ ID
- critic task çš„ `parent_task_id` â†’ generator task çš„ ID

**é—œè¯æŸ¥è©¢**:
```sql
-- æŸ¥è©¢æŸ job çš„æ‰€æœ‰ evaluations
SELECT 
    at_gen.id as generator_task_id,
    at_gen.agent_name as generator,
    at_gen.iteration_number,
    at_critic.id as critic_task_id,
    te.is_passed,
    te.metric_details
FROM agent_tasks at_gen
JOIN agent_tasks at_critic ON at_critic.parent_task_id = at_gen.id
JOIN task_evaluations te ON te.task_id = at_critic.id
WHERE at_gen.job_id = 280
ORDER BY at_gen.iteration_number;
```

**çµè«–**: ç¾æœ‰çµæ§‹å·²è¶³å¤ ï¼Œä¸éœ€è¦é¡å¤–æ¬„ä½ã€‚

---

## API è¼¸å‡ºè¨­è¨ˆ

### `test_critic_workflow` æœ€çµ‚è¼¸å‡º

```python
{
    "job_id": 280,
    "status": "completed",  # or "partial_success" if failed after max iterations
    
    # === 1. æœ€çµ‚çµæœï¼ˆç„¡è«–passæˆ–failéƒ½è¿”å›ï¼‰===
    "final_result": {
        "content": [...],  # æœ€çµ‚ç‰ˆæœ¬çš„å…§å®¹
        "title": "æ©Ÿå™¨å­¸ç¿’åŸºç¤æ¸¬é©—",
        "display_type": "exam_questions",
        "iteration": 2  # æœ€çµ‚ç‰ˆæœ¬çš„è¿­ä»£æ¬¡æ•¸
    },
    
    # === 2. Critic æ‘˜è¦ï¼ˆå‰ç«¯å¯è¦–åŒ–ç”¨ï¼‰===
    "critic_summary": {
        "total_iterations": 2,
        "final_passed": True,  # or False
        "enabled_critics": ["fact", "quality"],
        
        # å®Œæ•´çš„åˆ†æ•¸æ­·å²ï¼ˆæ‰€æœ‰è¿­ä»£ï¼‰âœ…
        "scores_history": [
            {
                "iteration": 1,
                "critics": {
                    "fact": {
                        "overall_score": 4.0,
                        "dimension_scores": {
                            "source_citation": 4.5,
                            "evidence_match": 3.5
                        }
                    },
                    "quality": {
                        "overall_score": 3.2,
                        "dimension_scores": {
                            "factual_accuracy": 2.5,
                            "clarity": 3.8,
                            "difficulty": 3.5
                        }
                    }
                },
                "overall_passed": False
            },
            {
                "iteration": 2,
                "critics": {
                    "fact": {
                        "overall_score": 4.8,
                        "dimension_scores": {
                            "source_citation": 4.9,
                            "evidence_match": 4.7
                        }
                    },
                    "quality": {
                        "overall_score": 4.5,
                        "dimension_scores": {
                            "factual_accuracy": 4.8,
                            "clarity": 4.2,
                            "difficulty": 4.5
                        }
                    }
                },
                "overall_passed": True
            }
        ],
        
        # å¤±æ•—çš„æ”¹é€²å»ºè­°ï¼ˆåƒ…è¨˜éŒ„å¤±æ•—çš„ï¼‰âœ…
        "improvement_history": [
            {
                "iteration": 1,
                "failed": True,
                "failed_critics": ["quality"],
                "failed_criteria": ["factual_accuracy", "clarity"],
                "suggestions": "é¡Œç›®1çš„ç­”æ¡ˆèˆ‡è­‰æ“šä¸ç¬¦ï¼Œå»ºè­°ä¿®æ­£...",
                "detailed_feedback": {
                    "per_question": [
                        {
                            "question_index": 1,
                            "issues": ["factual_accuracy"],
                            "suggestions": "ç­”æ¡ˆèªªã€Œæ©Ÿå™¨å­¸ç¿’æ˜¯...ã€ä½†è­‰æ“šé¡¯ç¤º..."
                        },
                        {
                            "question_index": 3,
                            "issues": ["clarity"],
                            "suggestions": "é¸é …Bå’ŒDçš„æè¿°éæ–¼ç›¸ä¼¼..."
                        }
                    ]
                }
            }
            // iteration 2 é€šéäº†ï¼Œä¸è¨˜éŒ„
        ]
    },
    
    # === 3. å‰ç«¯å¯è¦–åŒ–æ•¸æ“š âœ… ===
    "visualization_data": {
        // A. è¿­ä»£åˆ—è¡¨ï¼ˆç”¨æ–¼é€²åº¦æ¢/Timelineï¼‰
        "iterations": [
            {
                "number": 1,
                "status": "failed",
                "timestamp": "2025-12-09T15:00:00+08:00",
                "duration_ms": 5000,
                "content_summary": "ç”Ÿæˆäº†5é¡Œé¸æ“‡é¡Œ",
                "improvements_made": null
            },
            {
                "number": 2,
                "status": "passed",
                "timestamp": "2025-12-09T15:01:30+08:00",
                "duration_ms": 3000,
                "content_summary": "æ”¹é€²äº†2é¡Œé¸æ“‡é¡Œï¼ˆé¡Œç›®1, 3ï¼‰",
                "improvements_made": [
                    "ä¿®æ­£é¡Œç›®1çš„äº‹å¯¦æ€§éŒ¯èª¤",
                    "æ”¹é€²é¡Œç›®3çš„é¸é …æ¸…æ™°åº¦"
                ]
            }
        ],
        
        // B. åˆ†æ•¸è¶¨å‹¢ï¼ˆç”¨æ–¼æŠ˜ç·šåœ–ï¼‰
        "score_trends": {
            "overall": [3.2, 4.5],
            "fact": [4.0, 4.8],
            "quality": [3.2, 4.5],
            "factual_accuracy": [2.5, 4.8],
            "clarity": [3.8, 4.2],
            "difficulty": [3.5, 4.5]
        },
        
        // C. æ”¹é€²çš„é¡Œç›®é«˜äº®ï¼ˆç”¨æ–¼UIæ¨™ç¤ºï¼‰
        "modified_questions": [1, 3],  // é¡Œç›®ç´¢å¼•
        
        // D. é¡Œç›®ç´šåˆ¥çš„ä¿®æ”¹è©³æƒ…
        "modifications": {
            "1": {
                "before": {
                    "question_text": "...",
                    "options": {...}
                },
                "after": {
                    "question_text": "...",
                    "options": {...}
                },
                "reason": "ä¿®æ­£äº‹å¯¦æ€§éŒ¯èª¤ï¼šç­”æ¡ˆèˆ‡è­‰æ“šè¡çª",
                "improved_criteria": ["factual_accuracy"]
            },
            "3": {
                "before": {...},
                "after": {...},
                "reason": "æ”¹é€²æ¸…æ™°åº¦ï¼šé¸é …æè¿°éæ–¼ç›¸ä¼¼",
                "improved_criteria": ["clarity"]
            }
        }
    },
    
    # === 4. èª¿è©¦ä¿¡æ¯ï¼ˆå¯é¸ï¼‰===
    "debug_info": {
        "generation_history": [...],  // æ¯å€‹ç‰ˆæœ¬çš„å®Œæ•´å…§å®¹
        "rag_cache_used": true,
        "total_tokens": 12345,
        "total_cost_usd": 0.05,
        "skill_used": "exam_generation_skill",
        "refinement_strategy": "partial"
    }
}
```

---

## å‰ç«¯å¯è¦–åŒ–æ”¯æŒ

### å•é¡Œï¼šé€™äº›æ•¸æ“šéƒ½æœƒåŒ…å«åœ¨ API returns ä¸­å—ï¼Ÿ

**ç­”æ¡ˆï¼šæ˜¯çš„ï¼** âœ… æ‰€æœ‰ `visualization_data` éƒ½æœƒåŒ…å«åœ¨ API response çš„ `visualization_data` æ¬„ä½ä¸­ã€‚

### å‰ç«¯å¦‚ä½•ä½¿ç”¨é€™äº›æ•¸æ“š

#### A. è¿­ä»£é€²åº¦å¯è¦–åŒ–

**ä½¿ç”¨æ•¸æ“š**: `visualization_data.iterations`

**å‰ç«¯å¯¦ç¾ç¯„ä¾‹** (React):
```jsx
// Timeline çµ„ä»¶
<Timeline>
  {iterations.map(iter => (
    <Timeline.Item
      key={iter.number}
      color={iter.status === 'passed' ? 'green' : 'red'}
      label={new Date(iter.timestamp).toLocaleTimeString()}
    >
      <h4>ç¬¬ {iter.number} æ¬¡è¿­ä»£ {iter.status === 'passed' ? 'âœ…' : 'âŒ'}</h4>
      <p>{iter.content_summary}</p>
      {iter.improvements_made && (
        <ul>
          {iter.improvements_made.map((imp, i) => (
            <li key={i}>{imp}</li>
          ))}
        </ul>
      )}
      <small>è€—æ™‚: {iter.duration_ms}ms</small>
    </Timeline.Item>
  ))}
</Timeline>
```

**UI æ•ˆæœ**:
```
ç¬¬ 1 æ¬¡è¿­ä»£ âŒ
ç”Ÿæˆäº†5é¡Œé¸æ“‡é¡Œ
è€—æ™‚: 5000ms
15:00:00

ç¬¬ 2 æ¬¡è¿­ä»£ âœ…
æ”¹é€²äº†2é¡Œé¸æ“‡é¡Œï¼ˆé¡Œç›®1, 3ï¼‰
â€¢ ä¿®æ­£é¡Œç›®1çš„äº‹å¯¦æ€§éŒ¯èª¤
â€¢ æ”¹é€²é¡Œç›®3çš„é¸é …æ¸…æ™°åº¦
è€—æ™‚: 3000ms
15:01:30
```

#### B. åˆ†æ•¸è¶¨å‹¢åœ–è¡¨

**ä½¿ç”¨æ•¸æ“š**: `visualization_data.score_trends`

**å‰ç«¯å¯¦ç¾ç¯„ä¾‹** (ä½¿ç”¨ Chart.js):
```javascript
const chartData = {
  labels: iterations.map(i => `ç¬¬${i.number}æ¬¡`),  // ['ç¬¬1æ¬¡', 'ç¬¬2æ¬¡']
  datasets: [
    {
      label: 'ç¸½åˆ†',
      data: score_trends.overall,  // [3.2, 4.5]
      borderColor: 'rgb(75, 192, 192)',
    },
    {
      label: 'Fact Critic',
      data: score_trends.fact,  // [4.0, 4.8]
      borderColor: 'rgb(255, 99, 132)',
    },
    {
      label: 'Quality Critic',
      data: score_trends.quality,  // [3.2, 4.5]
      borderColor: 'rgb(54, 162, 235)',
    }
  ]
};

<Line data={chartData} options={{ ... }} />
```

**UI æ•ˆæœ**: æŠ˜ç·šåœ–é¡¯ç¤ºåˆ†æ•¸é€æ¬¡æå‡

#### C. é¡Œç›®é«˜äº®é¡¯ç¤º

**ä½¿ç”¨æ•¸æ“š**: `visualization_data.modified_questions` å’Œ `visualization_data.modifications`

**å‰ç«¯å¯¦ç¾ç¯„ä¾‹**:
```jsx
{questions.map((q, index) => {
  const isModified = modified_questions.includes(index);
  const modification = modifications[index];
  
  return (
    <QuestionCard
      key={index}
      className={isModified ? 'modified' : ''}
      highlight={isModified}
    >
      {isModified && (
        <Badge color="orange">å·²æ”¹é€²</Badge>
      )}
      
      <QuestionText>{q.question_text}</QuestionText>
      
      {modification && (
        <ImprovementNote>
          <Icon type="info-circle" />
          æ”¹é€²åŸå› : {modification.reason}
          <br />
          æå‡ç¶­åº¦: {modification.improved_criteria.join(', ')}
        </ImprovementNote>
      )}
    </QuestionCard>
  );
})}
```

**UI æ•ˆæœ**:
- æ”¹é€²çš„é¡Œç›®æœ‰æ©˜è‰² badge
- é¡¯ç¤ºæ”¹é€²åŸå› å’Œæå‡çš„è©•åˆ†ç¶­åº¦

#### D. æ”¹é€²æ‘˜è¦å±•ç¤º

**ä½¿ç”¨æ•¸æ“š**: `critic_summary.improvement_history`

**å‰ç«¯å¯¦ç¾ç¯„ä¾‹**:
```jsx
<Collapse>
  {improvement_history.map((hist, i) => (
    <Panel
      key={i}
      header={`ç¬¬ ${hist.iteration} æ¬¡è¿­ä»£å¤±æ•—`}
      extra={<Tag color="red">æœªé€šé</Tag>}
    >
      <Descriptions bordered size="small">
        <Descriptions.Item label="å¤±æ•—çš„ Critics">
          {hist.failed_critics.join(', ')}
        </Descriptions.Item>
        <Descriptions.Item label="å¤±æ•—çš„è©•åˆ†ç¶­åº¦">
          {hist.failed_criteria.join(', ')}
        </Descriptions.Item>
        <Descriptions.Item label="æ”¹é€²å»ºè­°" span={3}>
          {hist.suggestions}
        </Descriptions.Item>
      </Descriptions>
      
      <h4>å…·é«”å•é¡Œ:</h4>
      <List
        dataSource={hist.detailed_feedback.per_question}
        renderItem={item => (
          <List.Item>
            <strong>é¡Œç›® {item.question_index}:</strong> {item.suggestions}
          </List.Item>
        )}
      />
    </Panel>
  ))}
</Collapse>
```

---

## æ€§èƒ½å„ªåŒ–

### A. RAG å¿«å–å¯¦ç¾ âœ…

**å¯¦ç¾ä½ç½®**:
1. `retrieve_chunks_node` - æª¢æŸ¥å¿«å–
2. `exam_skill_node` / `summarization_skill_node` - å»ºç«‹å’Œå‚³éå¿«å–

**æ•ˆæœ**:
- ç¬¬ä¸€æ¬¡è¿­ä»£: æ­£å¸¸æª¢ç´¢ RAGï¼ˆè€—æ™‚ ~500msï¼‰
- å¾ŒçºŒè¿­ä»£: ä½¿ç”¨å¿«å–ï¼ˆè€—æ™‚ ~0msï¼‰
- **ç¯€çœæ™‚é–“**: (n-1) Ã— 500msï¼Œå…¶ä¸­ n = è¿­ä»£æ¬¡æ•¸
- **ç¯€çœæˆæœ¬**: é¿å…é‡è¤‡çš„ vector search

### B. éƒ¨åˆ†é‡æ–°ç”Ÿæˆ âœ…

**é©ç”¨ç¯„åœ**: Exam Generation

**å¯¦ç¾**: `_extract_failed_questions` + `_create_refinement_plan`

**æ•ˆæœ**:
- å‡è¨­5é¡Œï¼Œ2é¡Œå¤±æ•—
- å…¨éƒ¨é‡æ–°ç”Ÿæˆ: 5é¡Œ Ã— tokens
- éƒ¨åˆ†é‡æ–°ç”Ÿæˆ: 2é¡Œ Ã— tokens
- **ç¯€çœæˆæœ¬**: ~60%

**ä¸é©ç”¨**: Summaryï¼ˆå› ç‚ºæ˜¯æ•´é«”å…§å®¹ï¼‰

### C. ä¸¦è¡Œ Criticsï¼ˆæœªä¾†å„ªåŒ–ï¼‰

å¦‚æœå•Ÿç”¨å¤šå€‹ criticsï¼Œå¯ä»¥ä¸¦è¡ŒåŸ·è¡Œï¼š

```python
import asyncio

async def run_critics_node(state):
    tasks = []
    
    if "fact" in enabled_critics:
        tasks.append(run_fact_critic(state))
    if "quality" in enabled_critics:
        tasks.append(run_quality_critic(state))
    
    results = await asyncio.gather(*tasks)
    
    # åˆä½µçµæœ
    critics_results = {
        "fact": results[0] if "fact" in enabled_critics else None,
        "quality": results[1] if "quality" in enabled_critics else None
    }
    # ...
```

**æ•ˆæœ**: ç¯€çœæ™‚é–“ ~50%ï¼ˆå…©å€‹ critic ä¸¦è¡Œè€Œéä¸²è¡Œï¼‰

---

## Graph ä¿®æ”¹

```python
# graph.py

# 1. Import skill configs
from backend.app.agents.teacher_agent.skills.base import SKILL_CONFIGS

# 2. æ›´æ–° node åç¨±
builder.add_node("critics", run_critics_node)  # å¾ "quality_critic" æ”¹å

# 3. å‹•æ…‹å»ºç«‹ skill â†’ critics edges
build_skill_to_critic_edges(builder, SKILL_CONFIGS)

# 4. å•Ÿç”¨ conditional edge
builder.add_conditional_edges(
    "critics",
    should_continue_from_critic,
    {
        "aggregate_output": "aggregate_output",
        **{
            skill_name: skill_name
            for skill_name, config in SKILL_CONFIGS.items()
            if config.supports_refinement
        }
    }
)
```

---

## å¯¦ç¾æ­¥é©Ÿ

### Phase 1: åŸºç¤ Loop + Skill æ¶æ§‹ï¼ˆå„ªå…ˆï¼‰

**ç›®æ¨™**: å»ºç«‹åŸºæœ¬çš„è¿­ä»£å¾ªç’°å’Œå¯æ“´å……çš„ skill æ¶æ§‹

1. âœ… å‰µå»º `skills/base.py`ï¼Œå®šç¾© `SkillCapability` å’Œ `SKILL_CONFIGS`
2. âœ… å¯¦ç¾ `build_skill_to_critic_edges` å‡½æ•¸
3. âœ… ä¿®æ”¹ `should_continue_from_critic` æ”¯æŒå‹•æ…‹ skill æª¢æŸ¥
4. âœ… æ›´æ–° Graph edges (conditional)
5. âœ… åœ¨ State ä¸­åŠ å…¥æ‰€æœ‰å¿…è¦æ¬„ä½
6. âœ… é‡æ§‹ `quality_critic_node` â†’ `run_critics_node`
7. âœ… åœ¨ `create_task` èª¿ç”¨æ™‚å‚³é `iteration_number`
8. âœ… æ¸¬è©¦ç°¡å–®çš„ loop (å¤±æ•— â†’ é‡æ–°ç”Ÿæˆ â†’ æˆåŠŸ)

**é è¨ˆæ™‚é–“**: 2-3å°æ™‚

### Phase 2: RAG å¿«å–

**ç›®æ¨™**: é¿å…é‡è¤‡çš„ RAG æª¢ç´¢ï¼Œæå‡æ•ˆç‡

1. âœ… åœ¨ `retrieve_chunks_node` åŠ å…¥å¿«å–æª¢æŸ¥é‚è¼¯
2. âœ… åœ¨ skill wrappers ä¸­å»ºç«‹å’Œå‚³éå¿«å–
3. âœ… æ¸¬è©¦å¿«å–æ•ˆæœï¼ˆè§€å¯Ÿ log ç¢ºèªç¬¬2æ¬¡è¿­ä»£ä½¿ç”¨å¿«å–ï¼‰

**é è¨ˆæ™‚é–“**: 1å°æ™‚

### Phase 3: çµ±ä¸€ Refinement æ©Ÿåˆ¶

**ç›®æ¨™**: æ™ºèƒ½æ”¹é€²ï¼Œåªé‡æ–°ç”Ÿæˆå¤±æ•—çš„éƒ¨åˆ†

1. âœ… åœ¨ `plan_generation_tasks_node` åŠ å…¥ refinement åˆ¤æ–·
2. âœ… å¯¦ç¾ helper functions:
   - `_extract_failed_questions`
   - `_find_question_by_index`
   - `_create_refinement_plan`
3. âœ… åœ¨ `_generic_generate_question` åŠ å…¥ refinement é‚è¼¯
4. âœ… åœ¨ `summarize_node` åŠ å…¥ refinement é‚è¼¯
5. âœ… æ¸¬è©¦éƒ¨åˆ†é‡æ–°ç”Ÿæˆï¼ˆexamï¼‰å’Œå®Œæ•´é‡æ–°ç”Ÿæˆï¼ˆsummaryï¼‰

**é è¨ˆæ™‚é–“**: 2-3å°æ™‚

### Phase 4: API è¼¸å‡ºå„ªåŒ–

**ç›®æ¨™**: å®Œæ•´çš„ API responseï¼ŒåŒ…å«å‰ç«¯å¯è¦–åŒ–æ•¸æ“š

1. âœ… åœ¨ `aggregate_output_node` æ§‹å»ºå®Œæ•´çš„ API response
2. âœ… å¯¦ç¾ `visualization_data` çµæ§‹:
   - `iterations` åˆ—è¡¨
   - `score_trends` è¶¨å‹¢æ•¸æ“š
   - `modified_questions` é«˜äº®ä¿¡æ¯
   - `modifications` è©³ç´°ä¿®æ”¹
3. âœ… å¯¦ç¾åˆ†æ•¸æ­·å²å’Œæ”¹é€²æ­·å²çš„æ ¼å¼åŒ–
4. âœ… æ¸¬è©¦ API è¼¸å‡ºæ ¼å¼

**é è¨ˆæ™‚é–“**: 1-2å°æ™‚

### Phase 5: Multi-Critic æ¡†æ¶ï¼ˆæœªä¾†ï¼‰

**ç›®æ¨™**: æ”¯æŒ Fact Critic

1. â³ å¯¦ç¾ `run_fact_critic` å‡½æ•¸
2. â³ æ›´æ–° `_aggregate_metrics` æ”¯æŒå¤š critic
3. â³ æ¸¬è©¦ Fact + Quality é›™ critic æµç¨‹

**é è¨ˆæ™‚é–“**: 2-3å°æ™‚ï¼ˆFact Critic æœ¬èº«çš„å¯¦ç¾å¦è¨ˆï¼‰

### Phase 6: æ¸¬è©¦èˆ‡å„ªåŒ–

**ç›®æ¨™**: E2E æ¸¬è©¦ï¼Œç¢ºä¿åŠŸèƒ½å®Œæ•´æ€§

1. âœ… E2E æ¸¬è©¦å ´æ™¯:
   - å–®æ¬¡é€šéï¼ˆç”Ÿæˆå„ªè³ªå…§å®¹ï¼‰
   - è¿­ä»£æ”¹é€²ï¼ˆ2-3æ¬¡è¿­ä»£å¾Œé€šéï¼‰
   - é”åˆ°ä¸Šé™ï¼ˆå¤±æ•—ä½†ä»è¿”å›çµæœï¼‰
2. âœ… æ€§èƒ½æ¸¬è©¦:
   - RAG å¿«å–æ•ˆæœ
   - éƒ¨åˆ†é‡æ–°ç”Ÿæˆæ•ˆç‡
3. âœ… æ•¸æ“šå®Œæ•´æ€§æ¸¬è©¦:
   - æª¢æŸ¥æ‰€æœ‰ iteration æ­£ç¢ºè¨˜éŒ„åˆ°è³‡æ–™åº«
   - é©—è­‰ parent_task_id éˆæ¢å®Œæ•´

**é è¨ˆæ™‚é–“**: 1-2å°æ™‚

---

## é…ç½®åƒæ•¸

### ç’°å¢ƒè®Šæ•¸

```bash
# .env
MAX_CRITIC_ITERATIONS=3
CRITIC_MODE=quick  # or comprehensive
ENABLE_QUALITY_CRITIC=true
ENABLE_FACT_CRITIC=false  # æœªä¾†å•Ÿç”¨
ENABLE_RAG_CACHE=true
ENABLE_PARTIAL_REFINEMENT=true  # åªæ”¹é€²å¤±æ•—çš„é¡Œç›®ï¼ˆåƒ… examï¼‰
```

### API Request åƒæ•¸

```python
# teacher_testing_router.py

class TestCriticWorkflowRequest(BaseModel):
    unique_content_id: int
    prompt: str
    user_id: int = 1
    
    # Critic é…ç½®
    enabled_critics: List[str] = ["quality"]  # æœªä¾†å¯é¸ ["fact", "quality"]
    critic_mode: str = "quick"  # "quick" or "comprehensive"
    max_iterations: int = 3
    
    # èª¿è©¦é¸é …
    debug_mode: bool = False  # æ˜¯å¦è¿”å› debug_info
```

---

## é™„éŒ„ï¼šé—œéµæª”æ¡ˆæ¸…å–®

### æ–°å¢æª”æ¡ˆ
- `backend/app/agents/teacher_agent/skills/base.py` - Skill é…ç½®ç³»çµ±

### ä¿®æ”¹æª”æ¡ˆ
- `backend/app/agents/teacher_agent/graph.py` - Multi-critic, conditional edges
- `backend/app/agents/teacher_agent/skills/exam_generator/exam_nodes.py` - Refinement é‚è¼¯
- `backend/app/agents/teacher_agent/skills/summarization/nodes.py` - Refinement é‚è¼¯
- `backend/app/routers/teacher_testing_router.py` - API request/response
- `backend/app/utils/db_logger.py` - iteration_number å‚³é

---


---

**æœ€å¾Œæ›´æ–°**: 2025-12-10  
**æº–å‚™ç‹€æ…‹**: âœ… Phase 1 å¯¦ä½œä¸­

---

## Phase 1 å¯¦ç¾ç´°ç¯€ (2025-12-10)

### ç›®æ¨™

å¯¦ç¾æ”¯æŒ 4 ç¨®å¯¦é©— workflow çš„ Multi-Critic æ¶æ§‹ã€‚

### å¯¦é©—çµ„åˆ¥è¨­è¨ˆ

| å¯¦é©—çµ„åˆ¥ | ç„¡ Fact Critic | æœ‰ Fact Critic |
|---------|---------------|---------------|
| **ç„¡ Quality Critic** | Workflow 1: Only Generator | Workflow 2: è¿­ä»£è‡³ Ragas æŒ‡æ¨™é”æ¨™ |
| **æœ‰ Quality Critic** | Workflow 3: è¿­ä»£è‡³ G-eval æŒ‡æ¨™é”æ¨™ | Workflow 4: è¿­ä»£è‡³æ‰€æœ‰æŒ‡æ¨™é”æ¨™ |

### è³‡æ–™åº«å„²å­˜ç­–ç•¥

- **ä¸æ–°å¢**é¡å¤–çš„è³‡æ–™åº«æ¬„ä½
- **å­˜å…¥** `ORCHESTRATION_JOBS.experiment_config` (JSONB æ¬„ä½)
  ```json
  {
    "enabled_critics": ["fact", "quality"],
    "critic_mode": "quick",
    "max_iterations": 3
  }
  ```
- `workflow_type` è‡ªå‹•åˆ¤æ–·ï¼š`1_no_critic`, `2_fact_only`, `3_qual_only`, `4_all_critics`

### Critic å›å‚³æ ¼å¼çµ±ä¸€

å…©ç¨® critic éƒ½è¿”å›çµ±ä¸€çš„ `evaluations` é™£åˆ—æ ¼å¼ï¼š

```python
{
    "evaluations": [
        {
            "criteria": str,      # "Faithfulness" or "Understandable"
            "analysis": str,      # åˆ†æèªªæ˜
            "rating": float,      # Ragas: 0.0-1.0, G-eval: 1-5
            "suggestions": List[str]  # æ”¹é€²å»ºè­°
        }
    ],
    "is_passed": bool,
    "failed_criteria": List[str]
}
```

### æ ¸å¿ƒå‡½æ•¸

#### 1. `run_fact_critic(state)` - Ragas è©•ä¼°

- ä½¿ç”¨ `CustomFaithfulness` (äº‹å¯¦æ­£ç¢ºæ€§)
- ä½¿ç”¨ `CustomAnswerRelevancy` (ç­”æ¡ˆç›¸é—œæ€§)
- é–¾å€¼: 0.7
- è¿”å›çµ±ä¸€çš„ evaluations æ ¼å¼

#### 2. `run_quality_critic(state)` - G-eval è©•ä¼°

- ä½¿ç”¨ç¾æœ‰ `QualityCritic` é¡
- æ”¯æŒ quick/comprehensive æ¨¡å¼
- 5 å€‹è©•ä¼°æ¨™æº– + Overall Quality

#### 3. `_aggregate_metrics(critics_results)` - ç¶œåˆæŒ‡æ¨™

- åˆä½µæ‰€æœ‰ evaluations ç‚º `all_evaluations`
- åˆ¤æ–·æ•´é«”é€šé/å¤±æ•—
- æå–æ‰€æœ‰ failed_criteria
- åˆä½µæ”¹é€²å»ºè­°

#### 4. `_format_content_for_ragas(content, query, contexts)` - Ragas æ ¼å¼åŒ–

- å°‡æ•™æå…§å®¹è½‰ç‚º Ragas è¼¸å…¥æ ¼å¼
- è™•ç† retrieved_text_chunks

#### 5. `run_critics_node(state)` - ä¸»è¦åŸ·è¡Œç¯€é»

- ä¾åºåŸ·è¡Œ Fact â†’ Quality
- æ§‹å»ºç¶œåˆ feedback
- æ›´æ–° feedback history

### å¯¦ç¾æª”æ¡ˆ

- `backend/app/agents/teacher_agent/graph.py` - Multi-critic helpers + run_critics_node
- `backend/app/routers/teacher_testing_router.py` - API æ›´æ–°
- `backend/app/utils/db_logger.py` - experiment_config å„²å­˜


---

## Phase 1.5 å¯¦ç¾ç´°ç¯€ (2025-12-11)

### ç›®æ¨™

é‡æ§‹ Fact Critic ä»¥å°é½Š Quality Critic çš„æ¶æ§‹æ¨™æº–ï¼Œæå‡ä¸€è‡´æ€§èˆ‡å¯ç¶­è­·æ€§ã€‚

### æ ¸å¿ƒæ”¹é€²

#### 1. **RAG è³‡æ–™å‚³éå„ªåŒ–**
- **å•é¡Œ**ï¼šFact Critic ä½¿ç”¨ state ä¸­çš„ `retrieved_text_chunks`ï¼Œä½†é€™äº›è³‡æ–™æ²’æœ‰å¾ subgraph æ­£ç¢ºå‚³é
- **è§£æ±º**ï¼šåœ¨ `exam_skill_node` å’Œ `summarization_skill_node` ä¸­æ˜ç¢ºè¿”å› `retrieved_text_chunks` åˆ° TeacherAgentState
- **æ•ˆæœ**ï¼šé¿å… critic é‡è¤‡æª¢ç´¢ï¼Œç¢ºä¿ä½¿ç”¨èˆ‡ generator ç›¸åŒçš„åƒè€ƒè³‡æ–™

#### 2. **LLM ç”Ÿæˆ Feedback**
- **å•é¡Œ**ï¼šåŸæœ¬ä½¿ç”¨ hardcoded çš„ä¸­æ–‡æ¨¡æ¿ï¼Œç¼ºä¹é‡å°æ€§
- **è§£æ±º**ï¼š
  - æ–°å¢ `_generate_feedback_with_llm()` æ–¹æ³•ï¼Œåƒè€ƒ QualityCritic çš„ prompt è¨­è¨ˆ
  - ä½¿ç”¨ **Analysis-Rate-Suggest ç­–ç•¥**ï¼šå…ˆåˆ†æ â†’ å†çµåˆ Ragas åˆ†æ•¸ â†’ æä¾›å…·é«”å»ºè­°
  - Prompt åŒ…å« Ragas åŸå§‹åˆ†æ•¸ã€è©•ä¼°å…§å®¹ã€åƒè€ƒè³‡æ–™ã€è©•åˆ†æ¨™æº–
- **æ•ˆæœ**ï¼šæä¾›æ›´å…·é«”ã€æ›´æœ‰é‡å°æ€§çš„æ”¹é€²å»ºè­°

#### 3. **åˆ†æ•¸æ¨™æº–åŒ– (0-1 â†’ 1-5)**
- **æ–¹æ³•**ï¼šç·šæ€§æ˜ å°„ + å››æ¨äº”å…¥
  ```python
  raw_score = 1 + (ragas_score Ã— 4)
  normalized_score = round(raw_score)
  ```
- **æ˜ å°„è¦å‰‡**ï¼š
  - [0.0, 0.125) â†’ 1
  - [0.125, 0.375) â†’ 2
  - [0.375, 0.625) â†’ 3
  - [0.625, 0.875) â†’ 4
  - [0.875, 1.0] â†’ 5
- **é–¾å€¼**ï¼š4 åˆ†ä»¥ä¸‹ä¸é€šéï¼ˆå°æ‡‰ Ragas â‰¥ 0.625ï¼‰
- **ä¿ç•™è³‡æ–™**ï¼šåŒæ™‚ä¿ç•™åŸå§‹ Ragas åˆ†æ•¸å’Œç·šæ€§æ˜ å°„åˆ†æ•¸ï¼Œä¾›å¾ŒçºŒåˆ†ææ˜¯å¦éœ€èª¿æ•´ç‚ºè‡ªå®šç¾©åˆ†æ®µ

#### 4. **è³‡æ–™åº« Logging ä¸€è‡´æ€§**
- ç¢ºä¿ `fact_critic_node` å’Œ `quality_critic_node` çš„è³‡æ–™åº«è¨˜éŒ„æ ¼å¼å®Œå…¨ä¸€è‡´
- çµ±ä¸€ `feedback` å’Œ `metrics_detail` çµæ§‹
- æ‰€æœ‰ LLM å‘¼å«çš„ token ä½¿ç”¨é‡èˆ‡æˆæœ¬è¨˜éŒ„åˆ° `agent_tasks` è¡¨

#### 5. **API Response æ ¼å¼å°é½Š**
- Fact Critic å’Œ Quality Critic è¿”å›ç›¸åŒçš„çµæ§‹ï¼š
  ```python
  {
      "evaluations": [
          {
              "criteria": "Faithfulness",
              "analysis": "...",  # LLM ç”Ÿæˆ
              "rating": 4,        # æ•´æ•¸ 1-5
              "suggestions": [...],  # LLM ç”Ÿæˆ
              "raw_ragas_score": 0.75,  # ä¿ç•™åŸå§‹åˆ†æ•¸
              "raw_linear_score": 4.0   # ä¿ç•™ç·šæ€§åˆ†æ•¸
          }
      ],
      "is_passed": bool,
      "failed_criteria": [...]
  }
  ```

### å¯¦ç¾æ–‡ä»¶

**ä¿®æ”¹æª”æ¡ˆ**ï¼š
- `backend/app/agents/teacher_agent/critics/fact_critic.py` - æ ¸å¿ƒé‡æ§‹
- `backend/app/agents/teacher_agent/graph.py` - RAG å‚³éã€run_fact_critic
- `backend/app/agents/teacher_agent/state.py` - ç¢ºèª retrieved_text_chunks å®šç¾©

**æ–°å¢åŠŸèƒ½**ï¼š
- `normalize_ragas_score()` - åˆ†æ•¸æ¨™æº–åŒ–å‡½æ•¸
- `CustomFaithfulness._generate_feedback_with_llm()` - LLM feedback ç”Ÿæˆ
- `CustomAnswerRelevancy._generate_feedback_with_llm()` - LLM feedback ç”Ÿæˆ

### é©—è­‰è¨ˆåŠƒ

1. **Unit Tests**: æ¸¬è©¦åˆ†æ•¸æ¨™æº–åŒ–ã€LLM feedback ç”Ÿæˆ
2. **Integration Tests**: æ¸¬è©¦ RAG cachingã€è³‡æ–™åº«æ ¼å¼ä¸€è‡´æ€§
3. **E2E Tests**: åŸ·è¡Œ dual-critic workflowï¼Œé©—è­‰å®Œæ•´æµç¨‹

### Token æˆæœ¬ä¼°ç®—

- æ¯æ¬¡è©•ä¼°å¢åŠ  2 æ¬¡ LLM å‘¼å«ï¼ˆFaithfulness + Relevancyï¼‰
- æ¯æ¬¡ç´„ 500-800 tokens
- æˆæœ¬å¢åŠ ï¼š~$0.001 USD per iteration (gpt-4o-mini)

---

**æœ€å¾Œæ›´æ–°**ï¼š2025-12-29  
**å¯¦ç¾ç‹€æ…‹**ï¼šPhase 2 é€²è¡Œä¸­

---

## è®Šæ›´è¨˜éŒ„ï¼š2025-12-29 Fact Critic é‡æ§‹

### ä¸€ã€ç§»é™¤ Answer Relevancy

#### ç§»é™¤åŸå› 

1. **è©•ä¼°æ–¹å¼ä¸é©ç”¨æ–¼ç”Ÿæˆä»»å‹™**
   - Ragas Answer Relevancy çš„è¨ˆç®—æ–¹å¼æ˜¯ï¼šå¾ç­”æ¡ˆæ¨æ–·å‡è¨­å•é¡Œï¼Œå†èˆ‡åŸå§‹å•é¡Œæ¯”å° embedding ç›¸ä¼¼åº¦
   - é€™å°**çŸ¥è­˜å•ç­”**æœ‰æ•ˆï¼ˆå¦‚ã€Œä»€éº¼æ˜¯ç¼ºå¤±å€¼ï¼Ÿã€ï¼‰ï¼Œä½†å°**ä»»å‹™å‹æŒ‡ä»¤**ç„¡æ•ˆï¼ˆå¦‚ã€Œå‡ºå…©é¡Œé¸æ“‡é¡Œã€ï¼‰
   - ç¯„ä¾‹ï¼š
     - åŸå§‹å•é¡Œï¼š`å‡ºå…©é¡Œè³‡æ–™æ¸…ç†çš„é¸æ“‡é¡Œ`
     - ç­”æ¡ˆåŒ…å«å…©é“é¸æ“‡é¡Œ
     - å¾ç­”æ¡ˆæ¨æ–·çš„å‡è¨­å•é¡Œï¼šã€Œè™•ç†ç¼ºå¤±å€¼çš„æ–¹æ³•ï¼Ÿã€
     - å‡è¨­å•é¡Œèˆ‡åŸå§‹å•é¡Œèªç¾©ä¸ç›¸ä¼¼ â†’ åˆ†æ•¸åä½ï¼ˆ0.17-0.56ï¼‰

2. **JSON è§£æç©©å®šæ€§å•é¡Œ**
   - Ragas å…§éƒ¨ LLM å‘¼å«ç¶“å¸¸è¿”å› markdown åŒ…è£¹çš„ JSONï¼ˆ`\`\`\`json...\`\`\``ï¼‰
   - Ragas ç„¡æ³•è§£æé€™ç¨®æ ¼å¼ï¼Œéœ€è¦é¡å¤–çš„ fallback æ©Ÿåˆ¶

3. **èˆ‡ Faithfulness çš„è·è²¬é‡ç–Š**
   - Faithfulness å·²æª¢æŸ¥ç­”æ¡ˆæ˜¯å¦åŸºæ–¼ context
   - Answer Relevancy å°æ–¼ RAG å ´æ™¯çš„é™„åŠ åƒ¹å€¼æœ‰é™

#### ç§»é™¤ç¯„åœ

- `fact_critic.py`: åˆªé™¤ `CustomAnswerRelevancy` class
- `fact_critic.py`: åˆªé™¤ `get_fact_critic_embeddings()` function
- `graph.py` / `critics/graph.py`: ç§»é™¤ Answer Relevancy ç›¸é—œèª¿ç”¨
- `teacher_testing_router.py`: ç§»é™¤ Answer Relevancy ç›¸é—œèª¿ç”¨
- ç›¸é—œæ¸¬è©¦æ–‡ä»¶

---

### äºŒã€æ–°å¢ TaskSatisfactionï¼ˆä»»å‹™å®Œæˆåº¦ï¼‰

#### è¨­è¨ˆç›®çš„

è©•ä¼°ç”Ÿæˆçµæœæ˜¯å¦æ»¿è¶³ä½¿ç”¨è€…çš„**åŸºæœ¬ä»»å‹™è¦æ±‚**ï¼ˆæ ¼å¼ã€æ•¸é‡ç­‰ï¼‰ï¼Œå–ä»£åŸæœ¬çš„ Answer Relevancyã€‚

#### ä¸­æ–‡åç¨±

**ä»»å‹™ç¬¦åˆåº¦**

#### è©•åˆ†æ–¹å¼

æ¡ç”¨**åŠ æ¬Šæª¢æŸ¥é …ç›®**è¨ˆç®— 1-5 åˆ†ï¼Œèˆ‡å…¶ä»–æŒ‡æ¨™ï¼ˆFaithfulnessã€Quality Metricsï¼‰çµ±ä¸€è©•åˆ†æ–¹å¼ã€‚

#### æª¢æŸ¥é …ç›®èˆ‡æ¬Šé‡ï¼ˆexam_generationï¼‰

| æª¢æŸ¥é …ç›® | èªªæ˜ | æ¬Šé‡ | ç†ç”± |
|---------|------|------|------|
| `question_count` | é¡Œç›®æ•¸é‡æ˜¯å¦ç¬¦åˆè¦æ±‚ | 2 | æ•¸é‡éŒ¯èª¤å¾ˆæ˜é¡¯ |
| `question_type` | é¡Œå‹æ˜¯å¦ç¬¦åˆï¼ˆé¸æ“‡é¡Œ/æ˜¯éé¡Œ/å•ç­”é¡Œï¼‰ | 2 | é¡Œå‹éŒ¯èª¤å°è‡´ä¸å¯ç”¨ |
| `has_options` | é¸æ“‡é¡Œæ˜¯å¦æœ‰ ABCD é¸é … | 1 | é¸é …ç¼ºå¤±è¼ƒæ˜“è£œæ•‘ |
| `has_correct_answer` | æ˜¯å¦æœ‰æ­£ç¢ºç­”æ¡ˆ | 2 | æ²’ç­”æ¡ˆç„¡æ³•ä½¿ç”¨ |
| `has_source` | æ˜¯å¦æœ‰ä¾†æºå¼•ç”¨ | 1 | ä¾†æºç‚ºå¯é¸é …ç›® |

**ç¸½æ¬Šé‡ = 8**

#### è¨ˆåˆ†å…¬å¼

```python
weighted_score = sum(check.weight for check in passed_checks)
total_weight = sum(check.weight for check in all_checks)  # = 8

# ç·šæ€§è½‰æ›ç‚º 1-5 åˆ†
raw_score = 1 + (weighted_score / total_weight) * 4
normalized_score = round(raw_score)
```

#### åˆ†æ•¸å°æ‡‰è¡¨

| åŠ æ¬Šå¾—åˆ† | æ¯”ä¾‹ | 1-5 åˆ† |
|---------|------|--------|
| 8/8 | 100% | 5 |
| 7/8 | 87.5% | 5 |
| 6/8 | 75% | 4 |
| 5/8 | 62.5% | 4 |
| 4/8 | 50% | 3 |
| 3/8 | 37.5% | 3 |
| 2/8 | 25% | 2 |
| 1/8 | 12.5% | 2 |
| 0/8 | 0% | 1 |

#### å¯¦ä½œæ–¹å¼

ä½¿ç”¨ **è¦å‰‡æª¢æŸ¥ + LLM è¼”åŠ©** æ··åˆï¼š

1. **è¦å‰‡æª¢æŸ¥**ï¼ˆå¿«é€Ÿã€ç¢ºå®šæ€§ï¼‰ï¼š
   - é¡Œç›®æ•¸é‡ï¼šè§£æç”Ÿæˆå…§å®¹ä¸­çš„é¡Œç›®åˆ—è¡¨
   - é¸é …å­˜åœ¨æ€§ï¼šæª¢æŸ¥æ˜¯å¦æœ‰ A/B/C/D é¸é …
   - ç­”æ¡ˆå­˜åœ¨æ€§ï¼šæª¢æŸ¥æ˜¯å¦æœ‰ `correct_answer` æ¬„ä½

2. **LLM è¼”åŠ©æª¢æŸ¥**ï¼ˆè¤‡é›œã€å½ˆæ€§ï¼‰ï¼š
   - å¾ user_query è§£æè¦æ±‚çš„é¡Œç›®æ•¸é‡å’Œé¡Œå‹
   - å°é¡Œå‹é€²è¡Œèªç¾©åŒ¹é…

#### è¼¸å‡ºæ ¼å¼

```python
{
    "score": 0.875,              # åŸå§‹æ¯”ä¾‹ (0-1)
    "normalized_score": 5,       # æ¨™æº–åŒ–åˆ†æ•¸ (1-5)
    "checks": [
        {"name": "question_count", "weight": 2, "passed": True, "expected": 2, "actual": 2},
        {"name": "question_type", "weight": 2, "passed": True, "expected": "multiple_choice", "actual": "multiple_choice"},
        {"name": "has_options", "weight": 1, "passed": True},
        {"name": "has_correct_answer", "weight": 2, "passed": True},
        {"name": "has_source", "weight": 1, "passed": False}
    ],
    "weighted_score": 7,
    "total_weight": 8,
    "analysis": "ç”Ÿæˆçµæœç¬¦åˆè¦æ±‚ï¼Œåƒ…ç¼ºå°‘ä¾†æºå¼•ç”¨ã€‚",
    "suggestions": ["å»ºè­°ç‚ºæ¯é“é¡Œç›®åŠ å…¥ä¾†æºé ç¢¼å¼•ç”¨ã€‚"]
}
```

#### ä¿®æ”¹æª”æ¡ˆ

- `backend/app/agents/teacher_agent/critics/fact_critic.py`: 
  - ç§»é™¤ `CustomAnswerRelevancy`
  - æ–°å¢ `TaskSatisfaction` class
- `backend/app/agents/teacher_agent/critics/graph.py`: å°‡ `answer_relevancy` æ›¿æ›ç‚º `task_satisfaction`
- `backend/app/routers/teacher_testing_router.py`: æ›´æ–° debug API
- ç›¸é—œæ¸¬è©¦æ–‡ä»¶

---

### ä¸‰ã€æ›´æ–°å¾Œçš„ Fact Critic æ¶æ§‹

```
Fact Critic
â”œâ”€â”€ Faithfulness (Ragas + LLM Feedback, 1-5 åˆ†)
â”‚   â†’ ç­”æ¡ˆæ˜¯å¦åŸºæ–¼ contextï¼Œæ²’æœ‰æé€ 
â”‚
â””â”€â”€ TaskSatisfaction (Rule + LLM, 1-5 åˆ†) â† æ–°å¢
    â†’ ç”Ÿæˆçµæœæ˜¯å¦ç¬¦åˆä»»å‹™è¦æ±‚ï¼ˆé¡Œæ•¸ã€é¡Œå‹ã€æ ¼å¼ï¼‰
```

#### åŸ·è¡Œé †åº

```
1. Faithfulness è©•ä¼°ï¼ˆ1-5 åˆ†ï¼‰
2. TaskSatisfaction è©•ä¼°ï¼ˆ1-5 åˆ†ï¼‰
3. Quality Critic è©•ä¼°ï¼ˆ1-5 åˆ† Ã— N å€‹æŒ‡æ¨™ï¼‰
```

> TaskSatisfaction èˆ‡å…¶ä»–æŒ‡æ¨™åŒç­‰å°å¾…ï¼Œçµ±ä¸€è¼¸å‡º 1-5 åˆ†ã€‚

---

### å››ã€ä¿ç•™é …ç›®ï¼ˆæœªä¾†å¯ç”¨ï¼‰

- **Context Precision**ï¼ˆä¸»é¡Œç›¸é—œæ€§ï¼‰ï¼šè©•ä¼°æª¢ç´¢åˆ°çš„ context æ˜¯å¦èˆ‡ user query ç›¸é—œ
  - ç›®å‰æš«ä¸å¯¦ä½œï¼Œå› ä½¿ç”¨è€… query ä¸ä¸€å®šæ˜ç¢ºæŒ‡å®šå–®å…ƒ
  - é ç•™ä»‹é¢ï¼Œæœªä¾†å¯åŠ å…¥
